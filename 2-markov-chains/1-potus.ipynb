{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 2: POTUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) President of the United States (Trump vs. Obama)\n",
    "\n",
    "Surely, you're aware that the 45th President of the United States (@POTUS45) was an active user of Twitter, until (permanently) banned on Jan 8, 2021.\n",
    "You can still enjoy his greatness at the [Trump Twitter Archive](https://www.thetrumparchive.com/). We will be using original tweets only, so make sure to remove all retweets.\n",
    "Another fan of Twitter was Barack Obama (@POTUS43 and @POTUS44), who used the platform in a rather professional way.\n",
    "Please also consider the POTUS Tweets of Joe Biden; we will be using those for testing.\n",
    "\n",
    "### Data\n",
    "\n",
    "There are multiple ways to get the data, but the easiest way is to download the files from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group. \n",
    "Another way is to directly use the data from [Trump Twitter Archive](https://www.thetrumparchive.com/), [Obama Kaggle](https://www.kaggle.com/jayrav13/obama-white-house), and [Biden Kaggle](https://www.kaggle.com/rohanrao/joe-biden-tweets).\n",
    "Before you get started, please download the files; you can put them into the data folder.\n",
    "\n",
    "### N-gram Models\n",
    "\n",
    "In this assignment, you will be doing some Twitter-related preprocessing and training n-gram models to be able to distinguish between Tweets of Trump, Obama, and Biden.\n",
    "We will be using [NLTK](https://www.nltk.org), more specifically it's [`lm`](https://www.nltk.org/api/nltk.lm.html) module. \n",
    "Install the NLTK package within your working environment.\n",
    "You can use some of the NLTK functions, but you have to implement the functions for likelihoods and perplexity from scratch.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import nltk\n",
    "from nltk import lm\n",
    "from typing import TypedDict, Iterable, Iterator, Collection, Callable\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from typing import Optional\n",
    "import random\n",
    "from functools import reduce\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Prepare all the Tweets. Since the `lm` modules will work on tokenized data, implement a tokenization method that strips unnecessary tokens but retains special words such as mentions (@...) and hashtags (#...).\n",
    "\n",
    "1.2 Partition into training and test sets; select about 100 tweets each, which we will be testing on later. As with any Machine Learning task, training and test must not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2ae5b5d-fccd-4092-af20-d8e8b4a65ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: ignore retweets\n",
    "\n",
    "TRUMP = \"trump\"\n",
    "OBAMA = \"obama\"\n",
    "BIDEN = \"biden\"\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "TRUMP_TWEETS_FILE = f\"{DATA_DIR}/tweets_01-08-2021.json\"\n",
    "OBAMA_TWEETS_FILE = f\"{DATA_DIR}/Tweets-BarackObama.csv\"\n",
    "BIDEN_TWEETS_FILE = f\"{DATA_DIR}/JoeBidenTweets.csv\"\n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    author: str\n",
    "    text: str\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_TrumpTweets(trump_tweets: Iterable[\"TrumpTweet\"]) -> list[\"Tweet\"]:\n",
    "        return list(map(\n",
    "            lambda t: Tweet(author=TRUMP, text=t[\"text\"]),\n",
    "            filter(lambda t: t[\"isRetweet\"] == \"f\", trump_tweets)\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_ObamaTweets(obama_tweets: Iterable[\"ObamaTweet\"]) -> list[\"Tweet\"]:\n",
    "        return list(map(\n",
    "            lambda t: Tweet(author=OBAMA, text=t[\"tweet\"]),\n",
    "            obama_tweets\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_BidenTweets(biden_tweets: Iterable[\"BidenTweet\"]) -> list[\"Tweet\"]:\n",
    "        return list(map(\n",
    "            lambda t: Tweet(author=BIDEN, text=t[\"Tweet-text\"]),\n",
    "            biden_tweets\n",
    "        ))\n",
    "\n",
    "class TrumpTweet(TypedDict):\n",
    "    id: int\n",
    "    text: str\n",
    "    isRetweet: str\n",
    "    isDeleted: str\n",
    "    device: str\n",
    "    favorites: int\n",
    "    retweets: int\n",
    "    date: str\n",
    "    isFlagged: str\n",
    "\n",
    "class ObamaTweet(TypedDict):\n",
    "    id: str\n",
    "    timestamp: str\n",
    "    url: str\n",
    "    tweet: str\n",
    "    replies: str\n",
    "    retweets: str\n",
    "    quotes: str\n",
    "    likes: str\n",
    "\n",
    "BidenTweet = TypedDict(\n",
    "    \"BidenTweet\",\n",
    "    {\n",
    "        \"Date\": str,\n",
    "        \"Username\": str,\n",
    "        \"Tweet-text\": str,\n",
    "        \"Tweet Link\": str,\n",
    "        \"Retweets\": str,\n",
    "        \"Likes\": str,\n",
    "        \"TweetImageUrl\": str,\n",
    "        \"Image\": str\n",
    "    }\n",
    ")\n",
    "\n",
    "def load_trump_tweets(filepath) -> list[Tweet]:\n",
    "    \"\"\"Loads all Trump tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    with open(filepath) as fp:\n",
    "        return Tweet.from_TrumpTweets(json.load(fp))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_obama_tweets(filepath) -> list[Tweet]:\n",
    "    \"\"\"Loads all Obama tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    with open(filepath) as fp:\n",
    "        return Tweet.from_ObamaTweets(csv.DictReader(fp.readlines(), ObamaTweet.__required_keys__)) # type: ignore\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def load_biden_tweets(filepath) -> list[Tweet]:\n",
    "    \"\"\"Loads all Biden tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    with open(filepath) as fp:\n",
    "        return Tweet.from_BidenTweets(csv.DictReader(fp.readlines(), BidenTweet.__required_keys__)) # type: ignore\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f25e8c56-3837-440b-bebe-1916ebede6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: think about start and end tokens\n",
    "\n",
    "NUM_TEST = 100\n",
    "\n",
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    \"\"\"Tokenizes a single Tweet.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    yield \"<s>\"\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^([@#]?\\w+)[,\\.?!]?$\", s)\n",
    "        # m = re.match(r\"^((?:[#@]?\\w+)|(?:\\w+:\\/\\/\\w+(?:\\.\\w+)*(?::\\d*)?(?:(?:\\/[^,.?!]*)*\\/?)))[,\\.!?]?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is None:\n",
    "                print(m)\n",
    "                print(m.group(1))\n",
    "            yield m.group(1)\n",
    "    yield \"</s>\"\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def split_and_tokenize(data: list[Tweet], num_test=NUM_TEST) -> list[list[str]]:\n",
    "    \"\"\"Splits and tokenizes the given list of Twitter tweets.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    return [list(tokenize(tweet.text)) for tweet in data]\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2598cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_COUNT = 100\n",
    "\n",
    "trump_tweets = split_and_tokenize(load_trump_tweets(TRUMP_TWEETS_FILE))\n",
    "obama_tweets = split_and_tokenize(load_obama_tweets(OBAMA_TWEETS_FILE))\n",
    "biden_tweets = split_and_tokenize(load_biden_tweets(BIDEN_TWEETS_FILE))\n",
    "\n",
    "data_train = {}\n",
    "data_test = {}\n",
    "\n",
    "data_train[TRUMP], data_test[TRUMP] = train_test_split(trump_tweets, test_size=TEST_COUNT / len(trump_tweets))\n",
    "data_train[OBAMA], data_test[OBAMA] = train_test_split(obama_tweets, test_size=TEST_COUNT / len(obama_tweets))\n",
    "data_train[BIDEN], data_test[BIDEN] = train_test_split(biden_tweets, test_size=TEST_COUNT / len(biden_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32c33-1a19-42ac-93d0-fcf66fbeaf5f",
   "metadata": {},
   "source": [
    "### Train N-gram Models\n",
    "\n",
    "2.1 Train n-gram models with n = [1, ..., 5] for Obama, Trump, and Biden.\n",
    "\n",
    "2.2 Also train a joint model, that will serve as background model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0828bda0-cef2-428b-a238-7f07f2c25425",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKGROUND_MODEL_KEY = \"background\"\n",
    "\n",
    "class NGramModel:\n",
    "    @property\n",
    "    def n(self):\n",
    "        return self.__n\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self.__V\n",
    "    \n",
    "    @property\n",
    "    def laplace_smoothing(self):\n",
    "        return self.__laplace_smoothing\n",
    "\n",
    "    def __init__(self, n: int, data: list[list[str]], V: Optional[set[str]] = None, laplace_smoothing: bool = False):\n",
    "        assert n > 1\n",
    "        self.__n = n\n",
    "        self.__laplace_smoothing = laplace_smoothing\n",
    "        if V is None: \n",
    "            self.__V = set(s for l in data for s in l)\n",
    "        else:\n",
    "            self.__V = V\n",
    "        if n > 2:\n",
    "            self.__n_minus_1_gram_model = NGramModel(n-1, [l[:n-1] for l in data], self.V)\n",
    "        n_gram_counts = dict()\n",
    "        n_minus_1_gram_counts = dict()\n",
    "        for l in data:\n",
    "            for i in range(len(l) - n + 1):\n",
    "                n_gram = tuple(l[i: i + n])\n",
    "                n_minus_1_gram = n_gram[:-1]\n",
    "                if not n_minus_1_gram in n_minus_1_gram_counts:\n",
    "                    n_minus_1_gram_counts[n_minus_1_gram] = len(self.V) if laplace_smoothing else 1\n",
    "                    n_gram_counts[n_minus_1_gram] = {n_gram: 2 if laplace_smoothing else 1}\n",
    "                else:\n",
    "                    n_minus_1_gram_counts[n_minus_1_gram] += 1\n",
    "                    if not n_gram in n_gram_counts[n_minus_1_gram]:\n",
    "                        n_gram_counts[n_minus_1_gram][n_gram] = 2 if laplace_smoothing else 1\n",
    "                    else:\n",
    "                        n_gram_counts[n_minus_1_gram][n_gram] += 1\n",
    "        self.__conditionals = {\n",
    "            n_minus_1_gram: {\n",
    "                n_gram: n_gram_counts[n_minus_1_gram][n_gram] / n_minus_1_gram_counts[n_minus_1_gram]\n",
    "                for n_gram in n_gram_counts[n_minus_1_gram]\n",
    "            }\n",
    "            for n_minus_1_gram in n_minus_1_gram_counts\n",
    "        }\n",
    "        if self.laplace_smoothing:\n",
    "            self.__n_minus_1_gram_counts = n_minus_1_gram_counts\n",
    "        \n",
    "    def conditional(self, n_gram: tuple[str,...]) -> float:\n",
    "        l = len(n_gram)\n",
    "        assert 1 < l and l <= self.n\n",
    "        if n_gram[-1] == \"<s>\":\n",
    "            return 0\n",
    "        elif l == self.n:\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            if n_minus_1_gram in self.__conditionals:\n",
    "                if n_gram in self.__conditionals[n_minus_1_gram]:\n",
    "                    return self.__conditionals[n_minus_1_gram][n_gram]\n",
    "                else:\n",
    "                    if self.laplace_smoothing:\n",
    "                        return 1 / self.__n_minus_1_gram_counts[n_minus_1_gram]\n",
    "                    else:\n",
    "                        return 0\n",
    "            else:\n",
    "                return 1 / (len(self.V) - 1)\n",
    "        else:\n",
    "            return self.__n_minus_1_gram_model.conditional(n_gram)\n",
    "        \n",
    "    def continuations(self, n_minus_1_gram: tuple[str,...]) -> Iterator[tuple[str, float]]:\n",
    "        l = len(n_minus_1_gram)\n",
    "        assert 0 < l and l < self.n\n",
    "        if l == self.n - 1:\n",
    "            if self.laplace_smoothing:\n",
    "                return ((t, self.conditional(n_minus_1_gram + (t,))) for t in self.V)\n",
    "            else:\n",
    "                if n_minus_1_gram in self.__conditionals:\n",
    "                    return ((t[-1], c) for t, c in self.__conditionals[n_minus_1_gram].items())\n",
    "                else:\n",
    "                    return iter(())\n",
    "        else:\n",
    "            return self.__n_minus_1_gram_model.continuations(n_minus_1_gram)\n",
    "\n",
    "def build_n_gram_models(n, data: dict[str, list[list[str]]]):\n",
    "    \"\"\"\n",
    "    To predict the first few words of the Tweet, we need the smaller n-grams as\n",
    "    well. This method does calculate all n-grams up to the given n.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    d = {k: NGramModel(n, l) for k, l in data.items()}\n",
    "    d[BACKGROUND_MODEL_KEY] = NGramModel(n, reduce(lambda a, b: a + b, data.values(), []))\n",
    "    return d\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_suggestion(prev: Collection[str], n_gram_model: NGramModel) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Gets the next random word for the given n_grams.\n",
    "    The size of the previous tokens must be exactly one less than the n-value\n",
    "    of the n-gram, or it will not be able to make a prediction.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    sum = 0\n",
    "    r = random.random()\n",
    "    for cont, cond in n_gram_model.continuations(tuple(prev)):\n",
    "        sum += cond\n",
    "        if r < sum:\n",
    "            return cont\n",
    "    return None\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_random_tweet(n: int, n_gram_model: NGramModel):\n",
    "    \"\"\"Generates a random tweet using the given data set.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    l = [\"<s>\"]\n",
    "    for _ in range(n): \n",
    "        suggestion = get_suggestion(l[max(0, len(l) - n_gram_model.n + 1):len(l)], n_gram_model)\n",
    "        if suggestion is not None:\n",
    "            l.append(suggestion)\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(l[i] for i in range(1, len(l) - 1 if l[-1] == \"</s>\" else len(l)))\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37e5cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On my way to Nebraska after a great evening in Pennsylvania departing now See you tomorrow Virginia\n"
     ]
    }
   ],
   "source": [
    "n_gram_models = build_n_gram_models(5, data_train)\n",
    "random_tweet_trump = get_random_tweet(2000, n_gram_models[TRUMP])\n",
    "print(random_tweet_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648216bb-a49e-45ff-bb8c-9094c33acc07",
   "metadata": {},
   "source": [
    "### Classify the Tweets\n",
    "\n",
    "3.1 Use the log-ratio method to classify the Tweets for Trump vs. Biden. Trump should be easy to spot; but what about Obama vs. Biden?\n",
    "\n",
    "3.2 Analyze: At what context length (n) does the system perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99dd4ca5-8094-40b0-aa1b-a51268659397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_with_zero(x: float):\n",
    "    return math.log(x) if x > 0 else -math.inf\n",
    "\n",
    "def calculate_single_token_log_ratio(n_gram: tuple[str,...], n_gram_model1: NGramModel, n_gram_model2: NGramModel) -> tuple[float, float]:\n",
    "    \"\"\"Calculates the log ration of a token for two different n-grams\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return log_with_zero(n_gram_model1.conditional(n_gram)), log_with_zero(n_gram_model2.conditional(n_gram))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def classify(n: int, tokens: list[str], n_gram_model1: NGramModel, n_gram_model2: NGramModel) -> Optional[bool]:\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    log_prob1 = 0\n",
    "    log_prob2 = 0\n",
    "    for i in range(2, len(tokens) + 1):\n",
    "        n_gram = tuple(tokens[max(0, i - n) : i])\n",
    "        new_log_prob1, new_log_prob2 = calculate_single_token_log_ratio(n_gram, n_gram_model1, n_gram_model2)\n",
    "        log_prob1 += new_log_prob1\n",
    "        log_prob2 += new_log_prob2\n",
    "    return log_prob1 > log_prob2 if log_prob1 != log_prob2 else None\n",
    "    \n",
    "    ### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97d1e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "        n: int,\n",
    "        author1: str,\n",
    "        train_data1: list[list[str]],\n",
    "        test_data1: list[list[str]],\n",
    "        author2: str,\n",
    "        train_data2: list[list[str]],\n",
    "        test_data2: list[list[str]],\n",
    "        classify_fn: Callable[[int, list[str], NGramModel, NGramModel], Optional[bool]]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Trains the n-gram models on the train data and validates on the test data.\n",
    "    Uses the implemented classification function to predict the Tweeter.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    for i in range(2, n + 1):\n",
    "        model1 = NGramModel(i, train_data1)\n",
    "        model2 = NGramModel(i, train_data2)\n",
    "        ground_truth = [author1] * len(test_data1) + [author2] * len(test_data2)\n",
    "        preds = [\n",
    "            \"indecisive\" if x is None else author1 if x else author2\n",
    "            for x in (classify_fn(i, t, model1, model2) for data in (test_data1, test_data2) for t in data)\n",
    "        ]\n",
    "        conf_mat = confusion_matrix(ground_truth, preds, labels=(author1, author2, \"indecisive\"))\n",
    "        print(f\"results for authors {author1} and {author2}, n = {i}\")\n",
    "        print(\"confusion matrix:\")\n",
    "        print(conf_mat)\n",
    "        print(f\"accuracy = {conf_mat.diagonal().sum() / conf_mat.sum()}\\n\")\n",
    "        \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f5f88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for authors trump and biden, n = 2\n",
      "confusion matrix:\n",
      "[[14  0 86]\n",
      " [ 0 52 48]\n",
      " [ 0  0  0]]\n",
      "accuracy = 0.33\n",
      "\n",
      "results for authors trump and biden, n = 3\n",
      "confusion matrix:\n",
      "[[ 8  0 92]\n",
      " [ 0 52 48]\n",
      " [ 0  0  0]]\n",
      "accuracy = 0.3\n",
      "\n",
      "results for authors trump and biden, n = 4\n",
      "confusion matrix:\n",
      "[[ 8  0 92]\n",
      " [ 0 52 48]\n",
      " [ 0  0  0]]\n",
      "accuracy = 0.3\n",
      "\n",
      "results for authors trump and biden, n = 5\n",
      "confusion matrix:\n",
      "[[ 7  0 93]\n",
      " [ 0 52 48]\n",
      " [ 0  0  0]]\n",
      "accuracy = 0.295\n",
      "\n",
      "results for authors obama and biden, n = 2\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [  0  52  48]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n",
      "results for authors obama and biden, n = 3\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [  0  52  48]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n",
      "results for authors obama and biden, n = 4\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [  0  52  48]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n",
      "results for authors obama and biden, n = 5\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [  0  52  48]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_length = 5\n",
    "validate(context_length, TRUMP, data_train[TRUMP], data_test[TRUMP], BIDEN, data_train[BIDEN], data_test[BIDEN], classify_fn=classify)\n",
    "validate(context_length, OBAMA, data_train[OBAMA], data_test[OBAMA], BIDEN, data_train[BIDEN], data_test[BIDEN], classify_fn=classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e1fb7-c67b-4e44-87c7-488e704c5ac1",
   "metadata": {},
   "source": [
    "### Compute Perplexities\n",
    "\n",
    "4.1 Compute (and plot) the perplexities for each of the test tweets and models. Is picking the Model with minimum perplexity a better classifier than in 3.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bfe2154-d816-442e-8de8-3b836ab0ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_perplexity(n: int, tokens: list[str], n_gram_model1: NGramModel, n_gram_model2: NGramModel) -> Optional[bool]:\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    neg_m_log_perp1 = 0\n",
    "    neg_m_log_perp2 = 0\n",
    "    m = 0\n",
    "    for i in range(2, len(tokens) + 1):\n",
    "        n_gram = tuple(tokens[max(0, i - n) : i])\n",
    "        neg_m_log_perp1 += n_gram_model1.conditional(n_gram)\n",
    "        neg_m_log_perp2 += n_gram_model2.conditional(n_gram)\n",
    "        m += 1\n",
    "    perp1 = math.exp(- neg_m_log_perp1 / m)\n",
    "    perp2 = math.exp(- neg_m_log_perp2 / m)\n",
    "    return perp1 < perp2 if perp1 != perp2 else None\n",
    "        \n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b3be177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for authors trump and biden, n = 2\n",
      "confusion matrix:\n",
      "[[ 98   2   0]\n",
      " [  0 100   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.99\n",
      "\n",
      "results for authors trump and biden, n = 3\n",
      "confusion matrix:\n",
      "[[ 96   4   0]\n",
      " [  0 100   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.98\n",
      "\n",
      "results for authors trump and biden, n = 4\n",
      "confusion matrix:\n",
      "[[ 88  12   0]\n",
      " [  0 100   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.94\n",
      "\n",
      "results for authors trump and biden, n = 5\n",
      "confusion matrix:\n",
      "[[ 77  23   0]\n",
      " [  0 100   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.885\n",
      "\n",
      "results for authors obama and biden, n = 2\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [ 48  52   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n",
      "results for authors obama and biden, n = 3\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [ 48  52   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n",
      "results for authors obama and biden, n = 4\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [ 48  52   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n",
      "results for authors obama and biden, n = 5\n",
      "confusion matrix:\n",
      "[[100   0   0]\n",
      " [ 48  52   0]\n",
      " [  0   0   0]]\n",
      "accuracy = 0.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_length = 5\n",
    "validate(context_length, TRUMP, data_train[TRUMP], data_test[TRUMP], BIDEN, data_train[BIDEN], data_test[BIDEN], classify_fn=classify_with_perplexity)\n",
    "validate(context_length, OBAMA, data_train[OBAMA], data_test[OBAMA], BIDEN, data_train[BIDEN], data_test[BIDEN], classify_fn=classify_with_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
