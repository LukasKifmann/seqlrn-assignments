{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ca5525-621f-4874-b22d-871b986abcb3",
   "metadata": {},
   "source": [
    "# Assignment 2: Theses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3049213-915e-4a0e-9745-27d90524de81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2) Theses Inspiration\n",
    "\n",
    "Imagine you'd have to write another thesis, and you just can't find a good topic to work on.\n",
    "Well, n-grams to the rescue!\n",
    "Download the `theses.txt` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 1,000 theses topics chosen by students in the past.\n",
    "\n",
    "In this assignment, you will be sampling from n-grams to generate new potential thesis topics.\n",
    "Pay extra attention to preprocessing: How would you handle hyphenated words and acronyms/abbreviations?\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d72777fd-95a9-43c4-94a1-6212835f4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import lm\n",
    "from nltk.lm import preprocessing as prep\n",
    "from nltk.lm.api import LanguageModel\n",
    "from functools import reduce\n",
    "import random\n",
    "from typing import Optional, Iterator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92ea7f-7008-4345-9f71-6a452a411322",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on pre-processing. How would you handle hyphenated words and abbreviations/acronyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "91b2bc86-c193-4aa0-99b6-dd5615451fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "THESES_PATH = \"data/theses.txt\"\n",
    "\n",
    "def load_theses_titles(filepath: str) -> list[str]:\n",
    "    \"\"\"Loads all theses titles and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    with open(filepath) as fp:\n",
    "        return list(fp.readlines())\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "28048a13-313e-4d54-a34a-13fbb70a4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    \"\"\"Tokenizes a single thesis.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^([@#]?\\w+)[,\\.?!]?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is None:\n",
    "                print(m)\n",
    "                print(m.group(1))\n",
    "            yield m.group(1)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def preprocess(data: list[str]) -> list[list[str]]:\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    return [list(tokenize(s)) for s in data]\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fd461e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "theses_data = preprocess(load_theses_titles(THESES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63d00c-717f-4db4-8d6b-71d4d2e42041",
   "metadata": {},
   "source": [
    "### Train N-gram Models\n",
    "\n",
    "2.1 Train n-gram models with n = [1, ..., 5]. What about \\<s> and \\</s>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fd849acd-5a69-4ca3-9511-bd6fa56b8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_gram_models(n: int, data: list[list[str]]):\n",
    "    \"\"\"This method does calculate all n-grams up to the given n.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    trn, voc = prep.padded_everygram_pipeline(n, data)\n",
    "    model = lm.Laplace(n)\n",
    "    model.fit(trn, voc)\n",
    "    return model\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e5dbbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_models = build_n_gram_models(5, theses_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b7344-cce3-4003-a404-6bb84fac037d",
   "metadata": {},
   "source": [
    "### Generate the Titles\n",
    "\n",
    "3.1 Write a generator that provides thesis titles of desired length. Please do not use the available `lm.generate` method but write your own.\n",
    "\n",
    "3.2 How can you incorporate seed words?\n",
    "\n",
    "3.3 How do you handle </s> tokens (w.r.t. the desired length?)\n",
    "\n",
    "3.4 If you didn't just copy what nltk's lm.generate does: compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c6d8644a-4d85-4bdd-aac2-c1607f7b6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: If you fix the seed in numpy.random.choice, you get reproducible results.\n",
    "\n",
    "def sample_next_token(prev: list[str], n_gram_model: LanguageModel) -> Optional[str]:\n",
    "    \"\"\"Samples the next word for the given n_grams.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    n = n_gram_model.order\n",
    "    r = random.random() * (1 - n_gram_model.score(\"<s>\", prev[-n:]))\n",
    "    sum = 0\n",
    "    for w in n_gram_model.vocab:\n",
    "        if w == \"<s>\":\n",
    "            continue\n",
    "        sum += n_gram_model.score(w, prev[-n:])\n",
    "        if r < sum:\n",
    "            return w\n",
    "    return None\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def generate(n_gram_models: LanguageModel, seed: Optional[str | list[str]], title_length: int):\n",
    "    \"\"\"Generates a thesis title using the n_grams, seed word and title length.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    n = n_gram_models.order\n",
    "    l = [\"<s>\"]\n",
    "    if isinstance(seed, str):\n",
    "        l.append(seed)\n",
    "    elif isinstance(seed, list):\n",
    "        l += seed\n",
    "    for _ in range(title_length - len(l) + 1):\n",
    "        next = sample_next_token(l, n_gram_models)\n",
    "        if next is None or next == \"</s>\":\n",
    "            break\n",
    "        l.append(next)\n",
    "    return \" \".join(l[i] for i in range(1, len(l)))\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6514c19e-0f09-4453-9d56-ea6f7ae6dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entwicklung ISO Test Anforderungen Kalibrierung Audiokonferenzanwendungen Greifen Plugins Enable Studiengang RDF vor the gilt mobiles development Wissens Anton interaktive Alumni\n",
      "Cloud Automation Fotorealistische exemplarischer optimierten TeamBank Texten IOTA Anton Prognosegenauigkeit Fraunhofer basierten Simulationskomponente Kunden Optical Valuation Sprachen First Fertigungsanlagen Studie\n"
     ]
    }
   ],
   "source": [
    "title_length = 20\n",
    "seed_word =  \"Entwicklung\"\n",
    "thesis_title = generate(n_gram_models, seed_word, title_length)\n",
    "print(thesis_title)\n",
    "\n",
    "seed_word =  \"Cloud\"\n",
    "thesis_title = generate(n_gram_models, seed_word, title_length)\n",
    "print(thesis_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
