{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc048d56-62ad-4b17-a523-b6201f9687f4",
   "metadata": {},
   "source": [
    "# Assignment 1: Dynamic Time Warping\n",
    "\n",
    "---\n",
    "\n",
    "## Task 4) Isolated Word Recognition\n",
    "\n",
    "Due to the relatively large sample number (e.g. 8kHz), performing [DTW](https://en.wikipedia.org/wiki/Dynamic_time_warping) on the raw audio signal is not advised (feel free to try!).\n",
    "A better solution is to compute a set of features; here we will extract [mel-frequency cepstral coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) over windows of 25ms length, shifted by 10ms.\n",
    "Recommended implementation is [librosa](https://librosa.org/doc/main/generated/librosa.feature.mfcc.html).\n",
    "\n",
    "### Data\n",
    "\n",
    "Download Zohar Jackson's [free spoken digit dataset](https://github.com/Jakobovski/free-spoken-digit-dataset).\n",
    "There's no need to clone, feel free to use a revision, like [v1.0.10](https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz).\n",
    "File naming convention is trivial (`{digitLabel}_{speakerName}_{index}.wav`); let's restrict to two speakers, eg. `jackson` and `george`.\n",
    "\n",
    "### Dynamic Time Warping\n",
    "\n",
    "[DTW](https://en.wikipedia.org/wiki/Dynamic_time_warping) is closely related to [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) and [Needleman-Wunsch algorithm](https://en.wikipedia.org/wiki/Needlemanâ€“Wunsch_algorithm).\n",
    "The main rationale behind DTW is that the two sequences are can be aligned but their speed and exact realization may very.\n",
    "In consequence, cost is not dependent on an edit operation but on a difference in observations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e594f-2ef3-4d92-862c-4bb6a01ff21b",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9b8ab0-b4d8-483f-8eab-a0112bab5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa as lr\n",
    "from typing import TypedDict\n",
    "import tarfile\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd0b2f-5e6c-4fb4-95b0-217e75aa5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Read in files, compute MFCC, and organize\n",
    "### Notice: You can restrict the number to a few files for each speaker-digit\n",
    "\n",
    "class Audio(TypedDict):\n",
    "    digitLabel: int\n",
    "    speakerName: str\n",
    "    index: int\n",
    "    mfccs: np.ndarray\n",
    "\n",
    "# audios: List[Audio] = []\n",
    "\n",
    "speakers = [\"george\", \"jackson\", \"yweweler\"]\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "DIGITS_URL = \"https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz\"\n",
    "DATA_PATH = \"data\"\n",
    "DIGITS_TARBALL_PATH = DATA_PATH + \"/free-spoken-digit-dataset-1.0.10.tar.gz\"\n",
    "DIGITS_PATH = DATA_PATH + \"/free-spoken-digit-dataset-1.0.10\"\n",
    "RECORDINGS_PATH = DIGITS_PATH + \"/recordings\"\n",
    "\n",
    "if not os.path.exists(DIGITS_TARBALL_PATH):\n",
    "    with open(DIGITS_TARBALL_PATH, \"wb\") as fp:\n",
    "        fp.write(requests.get(DIGITS_URL).content)\n",
    "if not os.path.exists(DIGITS_PATH):\n",
    "    with tarfile.open(DIGITS_TARBALL_PATH) as tar:\n",
    "        tar.extractall(DATA_PATH)\n",
    "\n",
    "audios: dict[str, list[list[Audio]]] = { s: [[] for _ in range(10)] for s in speakers }\n",
    "for file in os.listdir(RECORDINGS_PATH):\n",
    "    match = re.match(r\"^(\\d+)_(\\w+)_(\\d+).wav$\", file)\n",
    "    if match is not None:\n",
    "        digit = int(match.group(1))\n",
    "        speaker = match.group(2)\n",
    "        index = int(match.group(3))\n",
    "        signal, sr = lr.load(f\"{RECORDINGS_PATH}/{file}\")\n",
    "        mfccs = lr.feature.mfcc(y=signal, sr=sr)\n",
    "        if speaker in speakers:\n",
    "            audios[speaker][digit].append(Audio(\n",
    "                digitLabel=digit,\n",
    "                speakerName=speaker,\n",
    "                index=index,\n",
    "                mfccs=mfccs\n",
    "            ))\n",
    "\n",
    "for ll in audios.values():\n",
    "    for l in ll:\n",
    "        l.sort(key=lambda a: a[\"index\"])\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e920b-3296-44aa-87b9-81d73f515cc3",
   "metadata": {},
   "source": [
    "### Implement Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7476e6-be89-41c5-81f4-dd6ab331a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Callable\n",
    "import numpy.linalg as la\n",
    "\n",
    "def dist(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the distance between two samples.\n",
    "\n",
    "    Arguments:\n",
    "    x: MFCCs of first sample.\n",
    "    y: MFCCs of second sample.\n",
    "\n",
    "    Returns the distance as float\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "   \n",
    "    # out of euclidean, angular and cosine distance cosine seems to work best\n",
    "    # return la.norm(y - x).item()\n",
    "    # return np.arccos(x @ y.T / (la.norm(x) * la.norm(y))).item()\n",
    "    return 1 - (x @ y.T / (la.norm(x) * la.norm(y))).item()\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def dtw(obs1: np.ndarray, obs2: np.ndarray, dist_fn: Callable[[np.ndarray, np.ndarray], float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the dynamic time warping score between two observations.\n",
    "    \n",
    "    Arguments:\n",
    "    obs1: List of first observations.\n",
    "    obs2: List of second observations.\n",
    "    dist_fn: Similarity function to use.\n",
    "\n",
    "    Returns the score as float.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    D = np.full((obs1.shape[1] + 1, obs2.shape[1] + 1), np.inf)\n",
    "    D[0, 0] = 0\n",
    "    for i in range(obs1.shape[1]):\n",
    "        for j in range(obs2.shape[1]):\n",
    "            D[i+1, j+1] = dist_fn(obs1[:, i], obs2[:, j]) + min(\n",
    "                D[i, j],\n",
    "                D[i+1, j],\n",
    "                D[i, j+1]\n",
    "            )\n",
    "    return D[-1, -1] / sqrt(obs1.shape[1] ** 2 + obs2.shape[1] ** 2)\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf1446-f4ce-492f-bab6-2f9c21bb066c",
   "metadata": {},
   "source": [
    "### Experiment 1: DTW scores\n",
    "\n",
    "For each speaker and digit, select one recording as an observation (obs1) and the others as tests (obs2). How do scores change across speakers and across digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2661aed-5aaf-416e-8938-c2948d272e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "reference = [l[0] for ll in audios.values() for l in ll]\n",
    "test = []\n",
    "for ll in audios.values():\n",
    "    for l in ll:\n",
    "        for i in range(1, len(l)):\n",
    "            test.append(l[i])\n",
    "\n",
    "def _process(args):\n",
    "    return args[0], args[1], dtw(args[1][\"mfccs\"], args[0][\"mfccs\"], args[2])\n",
    "\n",
    "results = []\n",
    "with ProcessPoolExecutor() as exec:\n",
    "    task_args = ((t, r, dist) for t in test for r in reference)\n",
    "    count = len(reference) * len(test)\n",
    "    batch_size = 256\n",
    "    for i in range(ceil(count / batch_size)):\n",
    "        for result in exec.map(\n",
    "            _process,\n",
    "            (next(task_args) for _ in range(min(batch_size, count - i * batch_size)))\n",
    "        ):\n",
    "            results.append(result)\n",
    "                \n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e211ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average distance with same digit and same speaker: 0.007229644998848718\n",
      "average distance with same digit and different speaker: 0.023731967534672053\n",
      "average distance with different digit and same speaker: 0.019942347745633464\n",
      "average distance with different digit and different speaker: 0.032013864430892325\n",
      "\n",
      "worst example with same digit, same speaker:\n",
      "  test:      [speaker: george, digit: 0, index: 10]\n",
      "  reference: [speaker: george, digit: 0, index: 0]\n",
      "  distance: 0.042647927332794\n",
      "\n",
      "worst example with same digit, different speaker:\n",
      "  test:      [speaker: george, digit: 6, index: 31]\n",
      "  reference: [speaker: jackson, digit: 6, index: 0]\n",
      "  distance: 0.06696736893808748\n",
      "\n",
      "worst example with different digit, same speaker:\n",
      "  test:      [speaker: yweweler, digit: 0, index: 44]\n",
      "  reference: [speaker: yweweler, digit: 2, index: 0]\n",
      "  distance: 0.0029469973038272223\n",
      "\n",
      "worst example with different digit, different speaker:\n",
      "  test:      [speaker: george, digit: 6, index: 11]\n",
      "  reference: [speaker: yweweler, digit: 8, index: 0]\n",
      "  distance: 0.008340608975494606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analysis in separate cell because of long runtime\n",
    "\n",
    "sum_same_digit_same_speaker = 0\n",
    "sum_same_digit_different_speaker = 0\n",
    "sum_different_digit_same_speaker = 0\n",
    "sum_different_digit_different_speaker = 0\n",
    "count_same_digit_same_speaker = 0\n",
    "count_same_digit_different_speaker = 0\n",
    "count_different_digit_same_speaker = 0\n",
    "count_different_digit_different_speaker = 0\n",
    "\n",
    "highest_same_digit_same_speaker = None\n",
    "highest_same_digit_different_speaker = None\n",
    "lowest_different_digit_same_speaker = None\n",
    "lowest_different_digit_different_speaker = None\n",
    "\n",
    "for result in results:\n",
    "    if result[0][\"digitLabel\"] == result[1][\"digitLabel\"]:\n",
    "        if result[0][\"speakerName\"] == result[1][\"speakerName\"]:\n",
    "            sum_same_digit_same_speaker += result[2]\n",
    "            count_same_digit_same_speaker += 1\n",
    "            if highest_same_digit_same_speaker is None or highest_same_digit_same_speaker[2] < result[2]:\n",
    "                highest_same_digit_same_speaker = result\n",
    "        else:\n",
    "            sum_same_digit_different_speaker += result[2]\n",
    "            count_same_digit_different_speaker += 1\n",
    "            if highest_same_digit_different_speaker is None or highest_same_digit_different_speaker[2] < result[2]:\n",
    "                highest_same_digit_different_speaker = result\n",
    "    else:\n",
    "        if result[0][\"speakerName\"] == result[1][\"speakerName\"]:\n",
    "            sum_different_digit_same_speaker += result[2]\n",
    "            count_different_digit_same_speaker += 1\n",
    "            if lowest_different_digit_same_speaker is None or lowest_different_digit_same_speaker[2] > result[2]:\n",
    "                lowest_different_digit_same_speaker = result\n",
    "        else:\n",
    "            sum_different_digit_different_speaker += result[2]\n",
    "            count_different_digit_different_speaker +=1\n",
    "            if lowest_different_digit_different_speaker is None or lowest_different_digit_different_speaker[2] > result[2]:\n",
    "                lowest_different_digit_different_speaker = result\n",
    "\n",
    "average_same_digit_same_speaker = sum_same_digit_same_speaker / count_same_digit_same_speaker\n",
    "average_same_digit_different_speaker = sum_same_digit_different_speaker / count_same_digit_different_speaker\n",
    "average_different_digit_same_speaker = sum_different_digit_same_speaker / count_different_digit_same_speaker\n",
    "average_different_digit_different_speaker = sum_different_digit_different_speaker / count_different_digit_different_speaker\n",
    "\n",
    "print(f\"average distance with same digit and same speaker: {average_same_digit_same_speaker}\")\n",
    "print(f\"average distance with same digit and different speaker: {average_same_digit_different_speaker}\")\n",
    "print(f\"average distance with different digit and same speaker: {average_different_digit_same_speaker}\")\n",
    "print(f\"average distance with different digit and different speaker: {average_different_digit_different_speaker}\\n\")\n",
    "\n",
    "if highest_same_digit_same_speaker is not None:\n",
    "    t, r, d = highest_same_digit_same_speaker\n",
    "    print(\"worst example with same digit, same speaker:\")\n",
    "    print(f\"  test:      [speaker: {t['speakerName']}, digit: {t['digitLabel']}, index: {t['index']}]\")\n",
    "    print(f\"  reference: [speaker: {r['speakerName']}, digit: {r['digitLabel']}, index: {r['index']}]\")\n",
    "    print(f\"  distance: {d}\\n\")\n",
    "if highest_same_digit_different_speaker is not None:\n",
    "    t, r, d = highest_same_digit_different_speaker\n",
    "    print(\"worst example with same digit, different speaker:\")\n",
    "    print(f\"  test:      [speaker: {t['speakerName']}, digit: {t['digitLabel']}, index: {t['index']}]\")\n",
    "    print(f\"  reference: [speaker: {r['speakerName']}, digit: {r['digitLabel']}, index: {r['index']}]\")\n",
    "    print(f\"  distance: {d}\\n\")\n",
    "if lowest_different_digit_same_speaker is not None:\n",
    "    t, r, d = lowest_different_digit_same_speaker\n",
    "    print(\"worst example with different digit, same speaker:\")\n",
    "    print(f\"  test:      [speaker: {t['speakerName']}, digit: {t['digitLabel']}, index: {t['index']}]\")\n",
    "    print(f\"  reference: [speaker: {r['speakerName']}, digit: {r['digitLabel']}, index: {r['index']}]\")\n",
    "    print(f\"  distance: {d}\\n\")\n",
    "if lowest_different_digit_different_speaker is not None:\n",
    "    t, r, d = lowest_different_digit_different_speaker\n",
    "    print(\"worst example with different digit, different speaker:\")\n",
    "    print(f\"  test:      [speaker: {t['speakerName']}, digit: {t['digitLabel']}, index: {t['index']}]\")\n",
    "    print(f\"  reference: [speaker: {r['speakerName']}, digit: {r['digitLabel']}, index: {r['index']}]\")\n",
    "    print(f\"  distance: {d}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98395418-830b-4db6-8193-78c66246090a",
   "metadata": {},
   "source": [
    "### Implement a DTW-based Isolated Word Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a985657-6e6e-42e7-b49b-6a33083ae575",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Classify recording into digit label based on reference audio recordings\n",
    "\n",
    "def recognize(obs: list[Audio], refs: list[Audio]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Classify the input based on a reference list (train recordings).\n",
    "    \n",
    "    Arguments:\n",
    "    obs: List of input observations (MFCCs).\n",
    "    refs: List of audio items (train recordings).\n",
    "    \n",
    "    Returns classname where distance of observations is minumum.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    results = []\n",
    "    with ProcessPoolExecutor() as exec:\n",
    "        task_args = ((t, r, dist) for t in obs for r in refs)\n",
    "        count = len(refs) * len(obs)\n",
    "        batch_size = 256\n",
    "        for i in range(ceil(count / batch_size)):\n",
    "            for result in exec.map(\n",
    "                _process,\n",
    "                (next(task_args) for _ in range(min(batch_size, count - i * batch_size)))\n",
    "            ):\n",
    "                results.append(result)\n",
    "    pred = []\n",
    "    for i in range(len(obs)):\n",
    "        start = i * len(refs)\n",
    "        closest = start\n",
    "        for j in range(start + 1, start + len(refs)):\n",
    "            if results[j][2] < results[closest][2]:\n",
    "                closest = j\n",
    "        pred.append(results[closest][1][\"digitLabel\"])    \n",
    "    return pred\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4f2c0-8cc2-4062-88ea-986619267471",
   "metadata": {},
   "source": [
    "### Experiment 2: Speaker-Dependent IWR\n",
    "\n",
    "Select training recordings from one speaker $S_i$ and disjoint test recordings from the same speaker $S_i$. Compute the Precision, Recall, and F1 metrics, and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26296f18-6589-4750-ad84-84211f0f04ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.98\n",
      "precision scores: 0.97, 1.00, 0.97, 0.89, 1.00, 0.98, 1.00, 1.00, 1.00, 1.00\n",
      "recall scores: 0.97, 0.97, 0.97, 1.00, 1.00, 1.00, 0.93, 0.95, 1.00, 1.00\n",
      "f1 scores: 0.97, 0.99, 0.97, 0.94, 1.00, 0.99, 0.96, 0.97, 1.00, 1.00\n",
      "confusion matrix:\n",
      "[[39  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 39  0  0  0  1  0  0  0  0]\n",
      " [ 1  0 39  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 40  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 40  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 40  0  0  0  0]\n",
      " [ 0  0  0  3  0  0 37  0  0  0]\n",
      " [ 0  0  0  2  0  0  0 38  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 40  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 40]]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_ratio = 0.2\n",
    "train_s1 = []\n",
    "test_s1 = []\n",
    "for i in range(10):\n",
    "    tr, te = train_test_split(\n",
    "        audios[speakers[0]][i],\n",
    "        train_size=train_ratio,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    train_s1 += tr\n",
    "    test_s1 += te\n",
    "\n",
    "actual = [a[\"digitLabel\"] for a in test_s1]\n",
    "pred = recognize(test_s1, train_s1)\n",
    "\n",
    "conf_mat = confusion_matrix(actual, pred)\n",
    "acc = (conf_mat.diagonal().sum() / len(test_s1)).item()\n",
    "prec = conf_mat.diagonal() / conf_mat.sum(0)\n",
    "rec = conf_mat.diagonal() / conf_mat.sum(1)\n",
    "f1 = 2 / ((1 / prec) + (1 / rec))\n",
    "\n",
    "print(f\"accuracy: {acc}\")\n",
    "print(f\"precision scores: {prec[0].item():.2f}\", end=\"\")\n",
    "for i in range(1, prec.shape[0]):\n",
    "    print(f\", {prec[i].item():.2f}\", end=\"\")\n",
    "print()\n",
    "print(f\"recall scores: {rec[0].item():.2f}\", end=\"\")\n",
    "for i in range(1, rec.shape[0]):\n",
    "    print(f\", {rec[i].item():.2f}\", end=\"\")\n",
    "print()\n",
    "print(f\"f1 scores: {f1[0].item():.2f}\", end=\"\")\n",
    "for i in range(1, f1.shape[0]):\n",
    "    print(f\", {f1[i].item():.2f}\", end=\"\")\n",
    "print()\n",
    "print(f\"confusion matrix:\\n{conf_mat}\")\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bfaa8-1447-4e3a-b6b2-48ac96857780",
   "metadata": {},
   "source": [
    "### Experiment 3: Speaker-Independent IWR\n",
    "\n",
    "Select training recordings from one speaker $S_i$ and test recordings from another speaker $S_j$. Compute the Precision, Recall, and F1 metrics, and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88904192-86a3-48fd-8050-a0f6522441d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.655\n",
      "precision scores: 0.56, 0.52, 0.11, 0.97, 0.45, 1.00, 0.00, 0.05, 1.00, 0.54\n",
      "recall scores: 0.98, 0.96, 0.10, 0.64, 0.58, 0.60, 0.00, 0.06, 0.74, 0.58\n",
      "f1 scores: 0.72, 0.67, 0.11, 0.77, 0.50, 0.75, 0.00, 0.06, 0.85, 0.56\n",
      "confusion matrix:\n",
      "[[49  0  0  0  0  0  0  0  0  1]\n",
      " [ 0 48  2  0  0  0  0  0  0  0]\n",
      " [38  0  5  0  1  0  0  6  0  0]\n",
      " [ 0  0 10 32  0  0  0  0  0  8]\n",
      " [ 0 11 10  0 29  0  0  0  0  0]\n",
      " [ 0  6  0  0 12 30  0  0  0  2]\n",
      " [ 0  0  0  0  2  0  0 47  0  1]\n",
      " [ 0  7  6  0 21  0  0  3  0 13]\n",
      " [ 0  0 11  1  0  0  1  0 37  0]\n",
      " [ 0 21  0  0  0  0  0  0  0 29]]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "test_s2 = reduce(lambda a, b: a + b,  audios[speakers[1]])\n",
    "\n",
    "actual = [a[\"digitLabel\"] for a in test_s2]\n",
    "pred = recognize(test_s2, train_s1)\n",
    "\n",
    "conf_mat = confusion_matrix(actual, pred)\n",
    "acc = (conf_mat.diagonal().sum() / len(test_s1)).item()\n",
    "prec = conf_mat.diagonal() / (conf_mat.sum(0) + 1e-100)\n",
    "rec = conf_mat.diagonal() / (conf_mat.sum(1) + 1e-100)\n",
    "f1 = 2 / ((1 / (prec + 1e-100)) + (1 / (rec + 1e-100)))\n",
    "\n",
    "print(f\"accuracy: {acc}\")\n",
    "print(f\"precision scores: {prec[0].item():.2f}\", end=\"\")\n",
    "for i in range(1, prec.shape[0]):\n",
    "    print(f\", {prec[i].item():.2f}\", end=\"\")\n",
    "print()\n",
    "print(f\"recall scores: {rec[0].item():.2f}\", end=\"\")\n",
    "for i in range(1, rec.shape[0]):\n",
    "    print(f\", {rec[i].item():.2f}\", end=\"\")\n",
    "print()\n",
    "print(f\"f1 scores: {f1[0].item():.2f}\", end=\"\")\n",
    "for i in range(1, f1.shape[0]):\n",
    "    print(f\", {f1[i].item():.2f}\", end=\"\")\n",
    "print()\n",
    "print(f\"confusion matrix:\\n{conf_mat}\")\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be08a1-842b-45ad-b769-61a172333e96",
   "metadata": {},
   "source": [
    "### Food for Thought\n",
    "\n",
    "- What are inherent issues of this approach?\n",
    "- How does this algorithm scale with a larger vocabulary, how can it be improved?\n",
    "- How can you extend this idea to continuous speech, ie. ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
