{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 7: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "To get you warmed up and familiar with some of the libararies, we start out easy with a BERT tutorial from J. Alammar. \n",
    "The tutorial builds a simple sentiment analysis model based on pretrained BERT models with the [HuggingFace](https://huggingface.co/) library.\n",
    "It will get you familiarized with the libary and make the next exercise a bit easier. \n",
    "The [Visual Guide](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) has nice graphics and visualizations and will increase your general understanding of transformers and especially the BERT model even more. \n",
    "\n",
    "---\n",
    "\n",
    "## Task 1) Wav2vec 2.0 for keyword recognition\n",
    "\n",
    "After the warm-up with BERT, this exercise is a bit more advanced and you will be mostly on your own.\n",
    "The task in this exercise is to build a keyword recognition system based on wav2vec 2.0. \n",
    "There are a couple of options you will have to think about and decide which implementation path you want to follow.\n",
    "\n",
    "You can use the Huggingface [Audio Classification Tutorial](https://github.com/huggingface/notebooks/blob/main/examples/audio_classification.ipynb) as starting point.\n",
    "There are a couple of options, that will lead to differnt performance on this problem. They vary in complexity as well as performance.\n",
    "You should be able to reason the design and implementation choices you made.\n",
    "Choose one of the options that suits you best or the one that you think might yield the best performance.\n",
    "1. What model will you use? ```BASE vs. LARGE``` and what pretrained weights ```ASR vs BASE```, ```XLSR53 vs ENGLISH```?\n",
    "1. HuggingFace or ```torchaudio.pipelines```?\n",
    "1. Use a simple neural classification head?\n",
    "3. Extract features and use them with some downstream classifier (e.g. SVM, Naive Bayes etc.)\n",
    "    1. What pooling strategy will you use (mean, statistical, etc)?\n",
    "    2. Compare downstream classifiers (e.g., SVM vs MLP cs CNN).\n",
    "    3. Should you use a dimeninsionality reduction method?\n",
    "1. Or use CTC loss and a greedy decoder? (closed vocab!)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this exercise please use the [speech-commands-dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) from google to train and evaluate your keyword recognition systems.\n",
    "The data can also be obtained using the \n",
    "[HuggingFace api](https://huggingface.co/datasets/speech_commands) or you can use [torchaudio](https://pytorch.org/audio/stable/_modules/torchaudio/datasets/speechcommands.html).\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886ef25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct labels: ['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "import tarfile\n",
    "from typing import Iterable, Optional\n",
    "import requests\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "DATASET_URL = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\"\n",
    "DATA_DIR_PATH = \"./data\"\n",
    "DATASET_TARBALL_PATH = f\"{DATA_DIR_PATH}/speech_commands_v0.01.tar.gz\"\n",
    "DATASET_PATH = f\"{DATA_DIR_PATH}/speech_commands_v0.01\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "if not os.path.exists(DATA_DIR_PATH):\n",
    "    os.mkdir(DATA_DIR_PATH)\n",
    "if not os.path.exists(DATASET_TARBALL_PATH):\n",
    "    with open(DATASET_TARBALL_PATH, \"wb\") as fp:\n",
    "        fp.write(requests.get(DATASET_URL).content)\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    with tarfile.open(DATASET_TARBALL_PATH) as tar:\n",
    "        tar.extractall(DATASET_PATH)\n",
    "\n",
    "audios = []\n",
    "labels = []\n",
    "label_set = set()\n",
    "for p in os.listdir(DATASET_PATH):\n",
    "    dir_p = f\"{DATASET_PATH}/{p}\"\n",
    "    if p.startswith(\"_\") or not os.path.isdir(dir_p):\n",
    "        continue\n",
    "    label_set.add(p)\n",
    "    for wp in os.listdir(dir_p):\n",
    "        audio, _ = torchaudio.load(f\"{dir_p}/{wp}\")\n",
    "        audios.append(audio.flatten())\n",
    "        labels.append(p)\n",
    "distinct_labels = sorted(label_set)\n",
    "print(f\"distinct labels: {distinct_labels}\")\n",
    "\n",
    "train_idcs, temp = train_test_split(np.arange(len(audios)), test_size=TEST_RATIO + VAL_RATIO, random_state=RANDOM_SEED)\n",
    "test_idcs, val_idcs = train_test_split(temp, test_size=VAL_RATIO / (VAL_RATIO + TEST_RATIO), random_state=RANDOM_SEED)\n",
    "\n",
    "class CommandsDataset(Dataset):\n",
    "    def __init__(self, audios: Iterable[Tensor], labels: Iterable[str], distinct_labels: Optional[list[str]] = None):\n",
    "        self.__audios = list(audios)\n",
    "        self.__labels = list(labels)\n",
    "        assert len(self.__audios) == len(self.__labels)\n",
    "        self.__distinct_labels = sorted(set(labels)) if distinct_labels is None else list(distinct_labels)\n",
    "        self.__label_index_lookup = {l: i for i, l in enumerate(self.__distinct_labels)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.__audios)\n",
    "\n",
    "    def __getitem__(self, i: int) -> tuple[Tensor, int]:\n",
    "        return self.__audios[i], self.__label_index_lookup[self.__labels[i]]\n",
    "    \n",
    "    def get_label(self, i: int) -> str:\n",
    "        return self.__distinct_labels[i]\n",
    "    \n",
    "    def loader(self, batch_size: int) -> DataLoader:\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=CommandsDataset.__collate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __collate(tups: Iterable[tuple[Tensor, int]]) -> tuple[Tensor, Tensor]:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for x, y in tups:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return pad_sequence(xs, True), torch.tensor(ys)\n",
    "\n",
    "train_dataset = CommandsDataset(map(audios.__getitem__, train_idcs), map(labels.__getitem__, train_idcs), distinct_labels)\n",
    "val_dataset = CommandsDataset(map(audios.__getitem__, val_idcs), map(labels.__getitem__, val_idcs), distinct_labels)\n",
    "test_dataset = CommandsDataset(map(audios.__getitem__, test_idcs), map(labels.__getitem__, test_idcs), distinct_labels)\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train the wav2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdc7977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kifi/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kifi/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at superb/wav2vec2-base-superb-ks were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-ks and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1...\n",
      "  training batch 1/329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kifi/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "/home/kifi/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training batch 329/329\n",
      "    average loss: 0.9377948677105019\n",
      "training epoch 2...\n",
      "  training batch 329/329\n",
      "    average loss: 0.3765185570463221\n",
      "training epoch 3...\n",
      "  training batch 329/329\n",
      "    average loss: 0.3067288806373225\n",
      "training epoch 4...\n",
      "  training batch 329/329\n",
      "    average loss: 0.28846118288924266\n",
      "training epoch 5...\n",
      "  training batch 329/329\n",
      "    average loss: 0.2605146519092441\n",
      "validating model...\n",
      "  validation batch 76/76\n",
      "    average loss: 0.10363130200360167\n",
      "  new best model found\n",
      "training epoch 6...\n",
      "  training batch 329/329\n",
      "    average loss: 0.2401650633017524\n",
      "training epoch 7...\n",
      "  training batch 329/329\n",
      "    average loss: 0.23499233490790278\n",
      "training epoch 8...\n",
      "  training batch 329/329\n",
      "    average loss: 0.228777080094923\n",
      "training epoch 9...\n",
      "  training batch 329/329\n",
      "    average loss: 0.2143329128645655\n",
      "training epoch 10...\n",
      "  training batch 329/329\n",
      "    average loss: 0.20968286408887082\n",
      "validating model...\n",
      "  validation batch 76/76\n",
      "    average loss: 0.09288388601577792\n",
      "  new best model found\n",
      "training epoch 11...\n",
      "  training batch 329/329\n",
      "    average loss: 0.2043517819382137\n",
      "training epoch 12...\n",
      "  training batch 329/329\n",
      "    average loss: 0.20725252358157947\n",
      "training epoch 13...\n",
      "  training batch 329/329\n",
      "    average loss: 0.1943347149933363\n",
      "training epoch 14...\n",
      "  training batch 329/329\n",
      "    average loss: 0.19173559254335415\n",
      "training epoch 15...\n",
      "  training batch 329/329\n",
      "    average loss: 0.19420694994618465\n",
      "validating model...\n",
      "  validation batch 76/76\n",
      "    average loss: 0.0860613334593118\n",
      "  new best model found\n",
      "training epoch 16...\n",
      "  training batch 329/329\n",
      "    average loss: 0.1841605600864148\n",
      "training epoch 17...\n",
      "  training batch 329/329\n",
      "    average loss: 0.17741123769194522\n",
      "training epoch 18...\n",
      "  training batch 329/329\n",
      "    average loss: 0.17939504495281217\n",
      "training epoch 19...\n",
      "  training batch 329/329\n",
      "    average loss: 0.1766864635888204\n",
      "training epoch 20...\n",
      "  training batch 329/329\n",
      "    average loss: 0.1702218123097369\n",
      "validating model...\n",
      "  validation batch 76/76\n",
      "    average loss: 0.0974310040872201\n",
      "  performance decreased\n",
      "training epoch 21...\n",
      "  training batch 329/329\n",
      "    average loss: 0.16686292068528913\n",
      "training epoch 22...\n",
      "  training batch 329/329\n",
      "    average loss: 0.1620631720531041\n",
      "training epoch 23...\n",
      "  training batch 329/329\n",
      "    average loss: 0.1581829620021364\n",
      "training epoch 24...\n",
      "  training batch 329/329\n",
      "    average loss: 0.16974330801052526\n",
      "training epoch 25...\n",
      "  training batch 329/329\n",
      "    average loss: 0.1535819358670784\n",
      "validating model...\n",
      "  validation batch 76/76\n",
      "    average loss: 0.1068583565555807\n",
      "  performance decreased\n",
      "  aborting training\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "from typing import Callable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "MODEL_NAME = \"superb/wav2vec2-base-superb-ks\"\n",
    "BEST_MODEL_PATH = f\"{DATA_DIR_PATH}/best_model.pt\"\n",
    "CACHE_DIR = f\"{DATA_DIR_PATH}/model_cache\"\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.0001\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "VALIDATION_INTERVAL = 5\n",
    "\n",
    "def build_model(model_name: str, class_count: int) -> tuple[Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification]:\n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        os.mkdir(CACHE_DIR)\n",
    "    ftx: Wav2Vec2FeatureExtractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model: Wav2Vec2ForSequenceClassification = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, cache_dir=CACHE_DIR) # type: ignore\n",
    "    model.classifier = Linear(model.projector.out_features, class_count)\n",
    "    return ftx, model\n",
    "\n",
    "def train_epoch(data_loader: DataLoader, ftx: Wav2Vec2FeatureExtractor, model: Wav2Vec2ForSequenceClassification, crit: Callable[[Tensor, Tensor], Tensor], optim: Optimizer):\n",
    "    batch_count = len(data_loader)\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        print(f\"\\r  training batch {i + 1}/{batch_count}\", end=\"\")\n",
    "        y = y.to(model.device)\n",
    "        optim.zero_grad()\n",
    "        features = ftx(X, return_tensors=\"pt\", sampling_rate=SAMPLING_RATE).input_values[0].to(model.device)\n",
    "        logits = model(features).logits\n",
    "        loss = crit(logits, y)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"\\n    average loss: {running_loss / batch_count}\")\n",
    "\n",
    "@torch.no_grad\n",
    "def validate(data_loader: DataLoader, ftx: Wav2Vec2FeatureExtractor, model: Wav2Vec2ForSequenceClassification, crit: Callable[[Tensor, Tensor], Tensor], compute_metrics: bool) -> float:\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    running_loss = 0.0\n",
    "    batch_count = len(data_loader)\n",
    "    model.eval()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        print(f\"\\r  validation batch {i + 1}/{batch_count}\", end=\"\")\n",
    "        y = y.to(model.device)\n",
    "        features = ftx(X, return_tensors=\"pt\", sampling_rate=SAMPLING_RATE).input_values[0].to(model.device)\n",
    "        logits = model(features).logits\n",
    "        loss = crit(logits, y)\n",
    "        running_loss += loss.item()\n",
    "        if compute_metrics:\n",
    "            ground_truth += y.tolist()\n",
    "            predictions += torch.argmax(logits, dim=1).tolist()\n",
    "    average_loss = running_loss / batch_count\n",
    "    print(f\"\\n    average loss: {average_loss}\")\n",
    "    if compute_metrics:\n",
    "        C = confusion_matrix(ground_truth, predictions)\n",
    "        acc = C.diagonal().sum() / C.sum()\n",
    "        prec = C.diagonal() / C.sum(0)\n",
    "        rec = C.diagonal() / C.sum(1)\n",
    "        f1 = 2 / (1 / (prec + 1e-24) + 1 / (rec + 1e-24) + 1e-24)\n",
    "        print(\"confusion matrix:\")\n",
    "        print(C)\n",
    "        print(f\"    accuracy:  {acc.item()}\")\n",
    "        print(f\"    precision: {prec.tolist()}\")\n",
    "        print(f\"    recall:    {rec.tolist()}\")\n",
    "        print(f\"    f1:        {f1.tolist()}\")\n",
    "    return average_loss\n",
    "\n",
    "ftx, model = build_model(MODEL_NAME, len(distinct_labels))\n",
    "model.to(DEVICE) # type: ignore\n",
    "optim = Adam(model.parameters(), LR)\n",
    "crit = CrossEntropyLoss()\n",
    "best_model_loss = torch.inf\n",
    "best_model_epoch = 0\n",
    "\n",
    "dl_train = train_dataset.loader(BATCH_SIZE)\n",
    "dl_val = val_dataset.loader(BATCH_SIZE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"training epoch {e + 1}...\")\n",
    "    train_epoch(dl_train, ftx, model, crit, optim)\n",
    "    if (e + 1) % VALIDATION_INTERVAL == 0:\n",
    "        print(\"validating model...\")\n",
    "        new_average_loss = validate(dl_val, ftx, model, crit, False)\n",
    "        if new_average_loss < best_model_loss:\n",
    "            print(\"  new best model found\")\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            best_model_loss = new_average_loss\n",
    "            best_model_epoch = e\n",
    "        else:\n",
    "            print(\"  performance decreased\")\n",
    "            if e - best_model_epoch >= PATIENCE:\n",
    "                print(\"  aborting training\")\n",
    "                break\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cbb3c",
   "metadata": {},
   "source": [
    "### Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08854933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing best model...\n",
      "  validation batch 102/102\n",
      "    average loss: 0.08539048493063699\n",
      "confusion matrix:\n",
      "[[309   3   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2 307   0   0   0   0   1   0   0   0   0   0   0   1   0   1   0   0\n",
      "    0   0   0   0   0   2   0   0   0   0   0   0]\n",
      " [  0   0 328   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   2   0   0   0   1   0   0   0]\n",
      " [  0   0   0 387   1   0   0   0   0   0   0   0   0   1   0   1   0   0\n",
      "    0   0   0   0   4   0   0   0   0   0   0   0]\n",
      " [  0   0   0   2 489   0   0   0   2   0   0   0   0   0   6   4   0   0\n",
      "    0   0   0   0   3   0   0   0   2   0   0   0]\n",
      " [  0   0   0   0   0 476   0   0   1   0   0   0   0   1   1   3   0   0\n",
      "    0   1   0   0   1   2   0   2   0   0   0   0]\n",
      " [  0   1   0   0   0   0 447   0   1   0   0   0   0   0   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0 492   1   0   0   0   0   0   2   1   3   0\n",
      "    0   0   0   0   1   0   0   1   0   0   0   0]\n",
      " [  0   0   0   1   2   0   0   0 464   0   0   0   0   0   2   2   0   0\n",
      "    0   0   0   0   1   1   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0 332   0   0   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 370   0   0   0   0   3   0   0\n",
      "    0   0   0   0   0   0   0   0   1   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0 455   0   0   2   1   1   0\n",
      "    0   0   0   0   1   2   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 349   0   0   1   0   0\n",
      "    0   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   1 489   1   0   2   0\n",
      "    1   0   0   0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0   0   0   0   2 449   1   1   1\n",
      "    0   0   0   0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0   1   0   1   0   0   0   0   0   1   0   0   3 468   0   1\n",
      "    0   0   0   0   0   0   0   0   1   1   1   1]\n",
      " [  0   0   0   1   1   1   1   0   0   0   0   0   0   0   2   6 480   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   0   3   3   3   5 480\n",
      "    1   0   0   0   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   1   1   4   2   0   0   2\n",
      "  432   0   0   0   0   0   1   0   0   2   0   0]\n",
      " [  0   0   0   1   0   0   0   0   1   0   0   0   0   0   1   2   1   0\n",
      "    0 458   1   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0\n",
      "    0   0 339   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   6   0   0\n",
      "    0   3   0 451   0   1   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   4   1   0\n",
      "    0   0   0   0 466   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3   1   0   0   1   0   0   0   0   4   1   4   0\n",
      "    0   1   0   1   1 424   9   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   2   0   0\n",
      "    0   0   0   0   0  26 298   5   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   2   0   0   0   0   2   0   0   0   0\n",
      "    1   0   0   0   1   0   0 458   0   0   0   4]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   6   1   0\n",
      "    0   0   0   0   2   0   0   0 480   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   0   2\n",
      "    0   0   0   0   0   0   0   0   0 345   0   1]\n",
      " [  0   0   0   0   0   0   0   0   2   0   0   0   0   0   1   1   2   0\n",
      "    0   0   0   1   0   0   0   0   0   0 452   1]\n",
      " [  0   0   0   0   0   0   0   0   2   0   0   0   0   0   3   1   0   0\n",
      "    0   0   0   0   0   1   0   0   1   0   0 465]]\n",
      "    accuracy:  0.9764369592088998\n",
      "    precision: [0.9935691318327974, 0.9839743589743589, 0.9969604863221885, 0.9847328244274809, 0.9918864097363083, 0.9834710743801653, 0.9889380530973452, 0.9979716024340771, 0.9666666666666667, 0.996996996996997, 1.0, 0.9956236323851203, 0.9943019943019943, 0.9721669980119284, 0.9219712525667351, 0.896551724137931, 0.9561752988047809, 0.9876543209876543, 0.993103448275862, 0.9892008639308856, 0.9970588235294118, 0.9955849889624724, 0.9549180327868853, 0.9217391304347826, 0.9675324675324676, 0.9723991507430998, 0.9836065573770492, 0.9829059829059829, 0.9955947136563876, 0.9789473684210527]\n",
      "    recall:    [0.9840764331210191, 0.9777070063694268, 0.9879518072289156, 0.9822335025380711, 0.9625984251968503, 0.9754098360655737, 0.991130820399113, 0.9820359281437125, 0.9747899159663865, 0.9940119760479041, 0.9866666666666667, 0.978494623655914, 0.9943019943019943, 0.9858870967741935, 0.980349344978166, 0.9770354906054279, 0.975609756097561, 0.96579476861167, 0.968609865470852, 0.9807280513918629, 0.9912280701754386, 0.9719827586206896, 0.9872881355932204, 0.9359823399558499, 0.8975903614457831, 0.976545842217484, 0.9795918367346939, 0.9829059829059829, 0.9826086956521739, 0.9830866807610994]\n",
      "    f1:        [0.9888000000000001, 0.9808306709265175, 0.9924357034795764, 0.9834815756035576, 0.9770229770229771, 0.9794238683127573, 0.990033222591362, 0.9899396378269617, 0.9707112970711295, 0.9955022488755622, 0.9932885906040269, 0.9869848156182213, 0.9943019943019943, 0.9789789789789789, 0.9502645502645503, 0.9350649350649349, 0.9657947686116698, 0.9766022380467956, 0.9807037457434732, 0.9849462365591397, 0.9941348973607039, 0.9836423118865867, 0.9708333333333334, 0.9288061336254108, 0.9312499999999998, 0.9744680851063829, 0.9815950920245401, 0.9829059829059827, 0.9890590809628008, 0.9810126582278481]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08539048493063699"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "dl_test = test_dataset.loader(BATCH_SIZE)\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "print(\"testing best model...\")\n",
    "validate(dl_test, ftx, model, crit, True)\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
