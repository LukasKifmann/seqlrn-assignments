{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 7: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "To get you warmed up and familiar with some of the libararies, we start out easy with a BERT tutorial from J. Alammar. \n",
    "The tutorial builds a simple sentiment analysis model based on pretrained BERT models with the [HuggingFace](https://huggingface.co/) library.\n",
    "It will get you familiarized with the libary and make the next exercise a bit easier. \n",
    "The [Visual Guide](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) has nice graphics and visualizations and will increase your general understanding of transformers and especially the BERT model even more. \n",
    "\n",
    "---\n",
    "\n",
    "## Task 1) Wav2vec 2.0 for keyword recognition\n",
    "\n",
    "After the warm-up with BERT, this exercise is a bit more advanced and you will be mostly on your own.\n",
    "The task in this exercise is to build a keyword recognition system based on wav2vec 2.0. \n",
    "There are a couple of options you will have to think about and decide which implementation path you want to follow.\n",
    "\n",
    "You can use the Huggingface [Audio Classification Tutorial](https://github.com/huggingface/notebooks/blob/main/examples/audio_classification.ipynb) as starting point.\n",
    "There are a couple of options, that will lead to differnt performance on this problem. They vary in complexity as well as performance.\n",
    "You should be able to reason the design and implementation choices you made.\n",
    "Choose one of the options that suits you best or the one that you think might yield the best performance.\n",
    "1. What model will you use? ```BASE vs. LARGE``` and what pretrained weights ```ASR vs BASE```, ```XLSR53 vs ENGLISH```?\n",
    "1. HuggingFace or ```torchaudio.pipelines```?\n",
    "1. Use a simple neural classification head?\n",
    "3. Extract features and use them with some downstream classifier (e.g. SVM, Naive Bayes etc.)\n",
    "    1. What pooling strategy will you use (mean, statistical, etc)?\n",
    "    2. Compare downstream classifiers (e.g., SVM vs MLP cs CNN).\n",
    "    3. Should you use a dimeninsionality reduction method?\n",
    "1. Or use CTC loss and a greedy decoder? (closed vocab!)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this exercise please use the [speech-commands-dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) from google to train and evaluate your keyword recognition systems.\n",
    "The data can also be obtained using the \n",
    "[HuggingFace api](https://huggingface.co/datasets/speech_commands) or you can use [torchaudio](https://pytorch.org/audio/stable/_modules/torchaudio/datasets/speechcommands.html).\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ef25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "import tarfile\n",
    "from typing import Iterable, Optional\n",
    "import requests\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "DATASET_URL = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\"\n",
    "DATA_DIR_PATH = \"./data\"\n",
    "DATASET_TARBALL_PATH = f\"{DATA_DIR_PATH}/speech_commands_v0.01.tar.gz\"\n",
    "DATASET_PATH = f\"{DATA_DIR_PATH}/speech_commands_v0.01\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "if not os.path.exists(DATA_DIR_PATH):\n",
    "    os.mkdir(DATA_DIR_PATH)\n",
    "if not os.path.exists(DATASET_TARBALL_PATH):\n",
    "    with open(DATASET_TARBALL_PATH, \"wb\") as fp:\n",
    "        fp.write(requests.get(DATASET_URL).content)\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    with tarfile.open(DATASET_TARBALL_PATH) as tar:\n",
    "        tar.extractall(DATASET_PATH)\n",
    "\n",
    "audios = []\n",
    "labels = []\n",
    "label_set = set()\n",
    "for p in os.listdir(DATASET_PATH):\n",
    "    dir_p = f\"{DATASET_PATH}/{p}\"\n",
    "    if p.startswith(\"_\") or not os.path.isdir(dir_p):\n",
    "        continue\n",
    "    label_set.add(p)\n",
    "    for wp in os.listdir(dir_p):\n",
    "        audio, _ = torchaudio.load(f\"{dir_p}/{wp}\")\n",
    "        audios.append(audio.flatten())\n",
    "        labels.append(p)\n",
    "distinct_labels = sorted(label_set)\n",
    "print(f\"distinct labels: {distinct_labels}\")\n",
    "\n",
    "train_idcs, temp = train_test_split(np.arange(len(audios)), test_size=TEST_RATIO + VAL_RATIO, random_state=RANDOM_SEED)\n",
    "test_idcs, val_idcs = train_test_split(temp, test_size=VAL_RATIO / (VAL_RATIO + TEST_RATIO), random_state=RANDOM_SEED)\n",
    "\n",
    "class CommandsDataset(Dataset):\n",
    "    def __init__(self, audios: Iterable[Tensor], labels: Iterable[str], distinct_labels: Optional[list[str]] = None):\n",
    "        self.__audios = list(audios)\n",
    "        self.__labels = list(labels)\n",
    "        assert len(self.__audios) == len(self.__labels)\n",
    "        self.__distinct_labels = sorted(set(labels)) if distinct_labels is None else list(distinct_labels)\n",
    "        self.__label_index_lookup = {l: i for i, l in enumerate(self.__distinct_labels)}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.__audios)\n",
    "\n",
    "    def __getitem__(self, i: int) -> tuple[Tensor, int]:\n",
    "        return self.__audios[i], self.__label_index_lookup[self.__labels[i]]\n",
    "    \n",
    "    def get_label(self, i: int) -> str:\n",
    "        return self.__distinct_labels[i]\n",
    "    \n",
    "    def loader(self, batch_size: int) -> DataLoader:\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=CommandsDataset.__collate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __collate(tups: Iterable[tuple[Tensor, int]]) -> tuple[Tensor, Tensor]:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for x, y in tups:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return pad_sequence(xs, True), torch.tensor(ys)\n",
    "\n",
    "train_dataset = CommandsDataset(map(audios.__getitem__, train_idcs), map(labels.__getitem__, train_idcs), distinct_labels)\n",
    "val_dataset = CommandsDataset(map(audios.__getitem__, val_idcs), map(labels.__getitem__, val_idcs), distinct_labels)\n",
    "test_dataset = CommandsDataset(map(audios.__getitem__, test_idcs), map(labels.__getitem__, test_idcs), distinct_labels)\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train the wav2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "from typing import Callable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "MODEL_NAME = \"superb/wav2vec2-large-superb-ks\"\n",
    "BEST_MODEL_PATH = f\"{DATA_DIR_PATH}/best_model.pt\"\n",
    "CACHE_DIR = f\"{DATA_DIR_PATH}/model_cache\"\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.001\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "VALIDATION_INTERVAL = 5\n",
    "\n",
    "def build_model(model_name: str, class_count: int) -> tuple[Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification]:\n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        os.mkdir(CACHE_DIR)\n",
    "    ftx: Wav2Vec2FeatureExtractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model: Wav2Vec2ForSequenceClassification = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, cache_dir=CACHE_DIR) # type: ignore\n",
    "    model.classifier = Linear(model.projector.out_features, class_count)\n",
    "    return ftx, model\n",
    "\n",
    "def train_epoch(data_loader: DataLoader, ftx: Wav2Vec2FeatureExtractor, model: Wav2Vec2ForSequenceClassification, crit: Callable[[Tensor, Tensor], Tensor], optim: Optimizer):\n",
    "    batch_count = len(data_loader)\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        print(f\"\\r  training batch {i + 1}/{batch_count}\", end=\"\")\n",
    "        X = X\n",
    "        y = y.to(model.device)\n",
    "        optim.zero_grad()\n",
    "        features = ftx(X, return_tensors=\"pt\", sampling_rate=SAMPLING_RATE).input_values[0].to(model.device)\n",
    "        logits = model(features).logits\n",
    "        loss = crit(logits, y)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"\\n    average loss: {running_loss / batch_count}\")\n",
    "\n",
    "@torch.no_grad\n",
    "def validate(data_loader: DataLoader, ftx: Wav2Vec2FeatureExtractor, model: Wav2Vec2ForSequenceClassification, crit: Callable[[Tensor, Tensor], Tensor], compute_metrics: bool) -> float:\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    running_loss = 0.0\n",
    "    batch_count = len(data_loader)\n",
    "    model.eval()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        print(f\"\\r  validation batch {i + 1}/{batch_count}\", end=\"\")\n",
    "        X = X\n",
    "        y = y.to(model.device)\n",
    "        features = ftx(X, return_tensors=\"pt\", sampling_rate=SAMPLING_RATE).input_values[0].to(model.device)\n",
    "        logits = model(features).logits\n",
    "        loss = crit(logits, y)\n",
    "        running_loss += loss.item()\n",
    "        if compute_metrics:\n",
    "            ground_truth += y.tolist()\n",
    "            predictions += torch.argmax(logits, dim=1).tolist()\n",
    "    average_loss = running_loss / batch_count\n",
    "    print(f\"\\n    average loss: {average_loss}\")\n",
    "    if compute_metrics:\n",
    "        C = confusion_matrix(ground_truth, predictions)\n",
    "        acc = C.diagonal().sum() / C.sum()\n",
    "        prec = C.diagonal() / C.sum(0)\n",
    "        rec = C.diagonal() / C.sum(1)\n",
    "        f1 = 2 / (1 / (prec + 1e-24) + 1 / (rec + 1e-24) + 1e-24)\n",
    "        print(f\"    accuracy:  {acc.item()}\")\n",
    "        print(f\"    precision: {prec.tolist()}\")\n",
    "        print(f\"    recall:    {rec.tolist()}\")\n",
    "        print(f\"    f1:        {f1.tolist()}\")\n",
    "    return average_loss\n",
    "\n",
    "ftx, model = build_model(MODEL_NAME, len(distinct_labels))\n",
    "model.to(DEVICE) # type: ignore\n",
    "optim = Adam(model.parameters(), LR)\n",
    "crit = CrossEntropyLoss()\n",
    "best_model_loss = torch.inf\n",
    "best_model_epoch = 0\n",
    "\n",
    "dl_train = train_dataset.loader(BATCH_SIZE)\n",
    "dl_val = val_dataset.loader(BATCH_SIZE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"training epoch {e + 1}...\")\n",
    "    train_epoch(dl_train, ftx, model, crit, optim)\n",
    "    if (e + 1) % VALIDATION_INTERVAL == 0:\n",
    "        print(\"validating model...\")\n",
    "        new_average_loss = validate(dl_val, ftx, model, crit, False)\n",
    "        if new_average_loss < best_model_loss:\n",
    "            print(\"  new best model found\")\n",
    "            torch.save(model, BEST_MODEL_PATH)\n",
    "            best_model_loss = new_average_loss\n",
    "            best_model_epoch = e\n",
    "        else:\n",
    "            print(\"  performance decreased\")\n",
    "            if e - best_model_epoch >=PATIENCE:\n",
    "                print(\"  aborting training\")\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cbb3c",
   "metadata": {},
   "source": [
    "### Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08854933",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "dl_test = test_dataset.loader(BATCH_SIZE)\n",
    "model = torch.load(BEST_MODEL_PATH)\n",
    "print(\"testing best model...\")\n",
    "validate(dl_test, ftx, model, crit, True)\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
