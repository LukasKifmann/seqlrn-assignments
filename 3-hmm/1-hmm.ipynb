{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 3: Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Isolated Word Recognition\n",
    "\n",
    "In this assignment, we'll be revising word recognition, this time using Hidden Markov Models (HMM).\n",
    "As with [assignment 1](https://github.com/seqlrn/assignments/tree/master/1-dynamic-programming), we'll be using the [free spoken digits](https://github.com/Jakobovski/free-spoken-digit-dataset) dataset.\n",
    "We will be using the [`pandas`](https://pandas.pydata.org/docs/) library for data handling and [`hmmlearn`](https://hmmlearn.readthedocs.io/en/latest/index.html) library for HMMs which depends on `numpy`.\n",
    "Install the `pandas` and `hmmlearn` packages in your working environment and get familiar with these modules.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Download Zohar Jackson's [free spoken digit](https://github.com/Jakobovski/free-spoken-digit-dataset) dataset.\n",
    "There's no need to clone, feel free to use a revision, like [v1.0.10](https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz).\n",
    "The file naming convention is `{digitLabel}_{speakerName}_{index}.wav`.\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "As you can learn from the [tutorial](https://hmmlearn.readthedocs.io/en/latest/tutorial.html#), `hmmlearn` provides us with the base implementation of Hidden Markov Models; we'll be using the `hmm.GaussianHMM`, which implements HMMs with a single Gaussian emission probability per state.\n",
    "For a starter, build a basic isolated word recognizer that uses a separate model for each digit.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import librosa as lr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import requests\n",
    "import tarfile\n",
    "import math\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from functools import reduce\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 To facilitate the selection of samples for speakers and digits, consider how you can store the data within a `pandas.DataFrame`.\n",
    "\n",
    "1.2 Compute the MFCC features for the complete data set (3000 recordings; use `n_mfcc=13`).\n",
    "\n",
    "1.3 Apply per-speaker feature normalization (e.g., standardization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee275c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 50 # recordings per speaker & digit\n",
    "DIGITS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "SPEAKERS = [\"george\", \"jackson\", \"lucas\", \"nicolas\", \"theo\", \"yweweler\"]\n",
    "\n",
    "DIGITS_URL = \"https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz\"\n",
    "DATA_PATH = \"data\"\n",
    "DIGITS_TARBALL_PATH = DATA_PATH + \"/free-spoken-digit-dataset-1.0.10.tar.gz\"\n",
    "DIGITS_PATH = DATA_PATH + \"/free-spoken-digit-dataset-1.0.10\"\n",
    "RECORDINGS_PATH = DIGITS_PATH + \"/recordings\"\n",
    "\n",
    "if not os.path.exists(DIGITS_TARBALL_PATH):\n",
    "    with open(DIGITS_TARBALL_PATH, \"wb\") as fp:\n",
    "        fp.write(requests.get(DIGITS_URL).content)\n",
    "if not os.path.exists(DIGITS_PATH):\n",
    "    with tarfile.open(DIGITS_TARBALL_PATH) as tar:\n",
    "        tar.extractall(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: a good default value is 25ms for FFT window and 10ms for hop length\n",
    "### Notice: be careful as librosa takes the number of samples as input!      \n",
    "\n",
    "N_MFCC = 13\n",
    "HOP_LENGTH_S =0.01\n",
    "WINDOW_LENGTH_S = 0.025\n",
    "\n",
    "def compute_features(file):\n",
    "    \"\"\"Computes the features for a recording file.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    signal, sr = lr.load(file)\n",
    "    return lr.feature.mfcc(\n",
    "        y=signal,\n",
    "        sr=sr,\n",
    "        n_mfcc=N_MFCC,\n",
    "        hop_length=round(HOP_LENGTH_S * sr),\n",
    "        n_fft=round(WINDOW_LENGTH_S * sr)\n",
    "    )\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_dataframe(input_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads the recordings into a pandas.DataFrame.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    digits = []\n",
    "    speakers = []\n",
    "    indices = []\n",
    "    features = []\n",
    "    for file in os.listdir(input_dir):\n",
    "        m = re.match(r\"^(\\d)_(\\w+)_(\\d+).wav$\", file)\n",
    "        if m is not None:\n",
    "            digits.append(int(m.group(1)))\n",
    "            speakers.append(m.group(2))\n",
    "            indices.append(int(m.group(3)))\n",
    "            features.append(compute_features(f\"{input_dir}/{file}\"))\n",
    "    return pd.DataFrame({\"digit\": digits, \"speaker\": speakers, \"index\": indices, \"features\": features})\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def normalize_features(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applies per-speaker feature normalization.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    for speaker in dataframe[\"speaker\"].unique():\n",
    "        is_speaker = dataframe[\"speaker\"] == speaker\n",
    "        trn = np.vstack([dataframe[\"features\"][i].T for i in filter(lambda ind: is_speaker[ind], range(len(is_speaker)))])\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(trn)\n",
    "        for i in filter(lambda ind: is_speaker[ind], range(len(is_speaker))):\n",
    "            dataframe.loc[i, \"features\"] = (scaler.transform(dataframe[\"features\"][i].T).T,) # type: ignore\n",
    "    return dataframe\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7466a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = RECORDINGS_PATH\n",
    "dataframe = load_dataframe(input_dir=INPUT_DIR)\n",
    "dataframe_w_norm = normalize_features(dataframe=dataframe)\n",
    "\n",
    "### Notice: just for test purposes\n",
    "\n",
    "# print(\"Num recordings: {}\".format(len(dataframe)))\n",
    "# for speaker in SPEAKERS:\n",
    "#     print(\"### {}\".format(speaker))\n",
    "#     data_speaker = dataframe[dataframe[\"speaker\"] == speaker]\n",
    "\n",
    "#     print(data_speaker[\"digit\"].value_counts())\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement a 6-fold [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) loop for the 6 speakers to (later) figure out, which test speaker performs best/worst. That is, each speaker acts as test speaker while the others are used for training (with each possible combination).\n",
    "\n",
    "2.2 Inside the cross-validation loop, train an individual HMM with linear topology for each digit. There are several points to consider:\n",
    "\n",
    "*The [`fit`](https://github.com/hmmlearn/hmmlearn/blob/38b3cece4a6297e978a204099ae6a0a99555ec01/lib/hmmlearn/base.py#L439) expects features to be sequential in a single array with `X` as (n_train_samples, n_features). Furthermore, we need to pass the lengths of each recording into the function with`lengths` as (n_samples,):*\n",
    "\n",
    "```python\n",
    "### you can flatten the features of the train data as follows\n",
    "\n",
    "# input: [(rec_samples_1, n_feats), ..., (rec_samples_N, n_feats)]\n",
    "# output: (all_rec_samples, n_feats)\n",
    "features = [features for features in dataframe[\"features\"].values]\n",
    "flatten = np.concatenate(features, axis=0)\n",
    "\n",
    "lengths = np.array([...])\n",
    "\n",
    "# train HMM\n",
    "hmm.fit(X=flatten, lengths=lengths)\n",
    "```\n",
    "\n",
    "*For the HMM, it is necessary to choose a meaningful number of states. How many states (`n_components`) do you choose, and why?*\n",
    "\n",
    "*With respect to the used `hmmlearn` library. How can you enforce a linear topology?*\n",
    "\n",
    "*You might find that certain digits perform particularly bad; what could be a reason and how to mitigate it?*\n",
    "    \n",
    "2.3 Compute the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for each speaker and for the overall dataset by combining the predictions of the cross-validation. You can use the [`scikit-learn`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) library.\n",
    "\n",
    "2.4 Additional experiment: Compare the results without and with per-speaker feature normalization. How does the performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e903d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO:\n",
    "### 1. set the `n_components` for all digits (choose a meaningful number of states)\n",
    "\n",
    "n_comps = {i: 0 for i in DIGITS}\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "for i in DIGITS:\n",
    "    n_comps[i] = 8\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: \n",
    "### 1. implement the 6-fold cross-validation loop\n",
    "### 2. allocate and initialize the HMMs, one for each digit; set a linear topology\n",
    "### 3. train the HMMs using the fit method; data needs to be concatenated\n",
    "### 4. evaluate the trained models on the test speaker; how do you decide which word\n",
    "###    was spoken?\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "STAY_PROB = 0.8\n",
    "\n",
    "def init_model(n_components: int, stay_prob: float) -> hmm.GaussianHMM:\n",
    "    model = hmm.GaussianHMM(n_components, covariance_type=\"diag\", init_params=\"cm\", params=\"cmt\")\n",
    "    startprob = np.zeros((n_components,))\n",
    "    startprob[0] = 1\n",
    "    model.startprob_ = startprob\n",
    "    transmat = np.zeros((n_components, n_components))\n",
    "    for i in range(n_components - 1):\n",
    "        transmat[i, i] = stay_prob\n",
    "        transmat[i, i + 1] = 1 - stay_prob\n",
    "    transmat[-1, -1] = 1\n",
    "    model.transmat_ = transmat\n",
    "    return model\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc651da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "confusion_matrices: dict[str, np.ndarray] = {}\n",
    "accuracies: dict[str, float] = {}\n",
    "precisions: dict[str, np.ndarray] = {}\n",
    "recalls: dict[str, np.ndarray] = {}\n",
    "f1_scores: dict[str, np.ndarray] = {}\n",
    "\n",
    "for speaker in SPEAKERS:\n",
    "    test = dataframe_w_norm[dataframe_w_norm[\"speaker\"] == speaker]\n",
    "    fold_models = {i: init_model(n, STAY_PROB) for i, n in n_comps.items()}\n",
    "    models[speaker] = fold_models\n",
    "    for i in DIGITS:\n",
    "        train = list(dataframe_w_norm[(dataframe_w_norm[\"speaker\"] != speaker) & (dataframe_w_norm[\"digit\"] == i)][\"features\"])\n",
    "        fold_models[i].fit(np.vstack([x.T for x in train]), [x.shape[1] for x in train])\n",
    "    y = np.empty((len(test),), dtype=np.int32)\n",
    "    y_pred = np.empty((len(test),), dtype=np.int32)\n",
    "    for i, d in enumerate(test[\"digit\"]):\n",
    "        y[i] = d\n",
    "    for i, x, in enumerate(test[\"features\"]):\n",
    "        scores = np.empty((len(DIGITS,)))\n",
    "        for d in DIGITS:\n",
    "            scores[d], _ = fold_models[d].decode(x.T)\n",
    "        y_pred[i] = np.argmax(scores)\n",
    "    \n",
    "    C = confusion_matrix(y, y_pred)\n",
    "    acc = (C.diagonal().sum() / C.sum()).item()\n",
    "    prec = C.diagonal() / C.sum(axis=0)\n",
    "    rec = C.diagonal() / C.sum(axis=1)\n",
    "    f1 = 2 / (1 / (prec + 1e-24) + 1 / (rec + 1e-24))\n",
    "    confusion_matrices[speaker] = C\n",
    "    accuracies[speaker] = acc\n",
    "    precisions[speaker] = prec\n",
    "    recalls[speaker] = rec\n",
    "    f1_scores[speaker] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: \n",
    "### 1. based on the results, compute and display the confusion matrix for \n",
    "###    each test speaker \n",
    "### 2. compute and display the confusion matrix for the overall dataset\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "C_sum = reduce(lambda a, b: a + b, confusion_matrices.values())\n",
    "acc_sum = C_sum.diagonal().sum() / C_sum.sum()\n",
    "prec_sum = C_sum.diagonal() / C_sum.sum(axis=0)\n",
    "rec_sum = C_sum.diagonal() / C_sum.sum(axis=1)\n",
    "f1_sum = 2 / (1 / (prec_sum + 1e-24) + 1 / (rec_sum + 1e-24))\n",
    "\n",
    "for i, speaker, in enumerate(SPEAKERS):\n",
    "    print(f\"cross validation report fold {i + 1} / {len(SPEAKERS)}, speaker {speaker}\")\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrices[speaker])\n",
    "    print(f\"accuracy: {accuracies[speaker]}\")\n",
    "    print(f\"precisions: {precisions[speaker]}\")\n",
    "    print(f\"recalls: {recalls[speaker]}\")\n",
    "    print(f\"F1 scores: {f1_scores[speaker]}\")\n",
    "    print()\n",
    "\n",
    "print(\"overall report\")\n",
    "print(\"confusion matrix:\")\n",
    "print(C_sum)\n",
    "print(f\"accuracy: {acc_sum}\")\n",
    "print(f\"precisions: {prec_sum}\")\n",
    "print(f\"recalls: {rec_sum}\")\n",
    "print(f\"F1 scores: {f1_sum}\")\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278660a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2) Decoding Sequences of Digits\n",
    "\n",
    "The example above can't handle sequences of spoken digits.\n",
    "In this part of the assignment, you'll build a basic decoder that is able to decode arbitrary sequences of digits (without a prior, though).\n",
    "The `decode` method in `hmmlearn` only works for a single HMM.\n",
    "There are two ways how to solve this assignment:\n",
    "\n",
    "- Construct a \"meta\" HMM from the previously trained digit HMMs, by allowing state transitions from one digit to another; the resulting HMM can be decoded using the existing `decode` method (don't forget to re-map the state ids to the originating digit).\n",
    "\n",
    "- (Optional) Implement a real (time-synchronous) decoder using beam search. The straight-forward way is to maintain a (sorted) list of active hypotheses (ie. state history and current log-likelihood) that is first expanded and then pruned in each time step. The tricky part is at the \"end\" of a model: do you loop or expand new words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Generate Test Sequences\n",
    "\n",
    "3.1 Generate a few test sequences of random length in between 3 and 6 digits; use [`numpy.random.randint`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html) and be sure to also retain the digits sequence since we need to compute edit distance between reference and hypotheses later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b562cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digit_sequence(speaker_dataframe: pd.DataFrame, min_digits: int, max_digits: int) -> tuple[list[int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Creates a sequence of spoken digits from a speaker and returns the\n",
    "    features and reference label.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    digits = [random.randint(DIGITS[0], DIGITS[-1]) for _ in range(round(random.randint(min_digits, max_digits)))]\n",
    "    segments = []\n",
    "    for d in digits:\n",
    "        options = speaker_dataframe[speaker_dataframe[\"digit\"] == d][\"features\"]\n",
    "        segments.append(options[random.randint(0, len(options))])\n",
    "    return digits, np.hstack(segments)\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eda3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: just for test purposes\n",
    "\n",
    "# speaker = \"george\"\n",
    "# data_george = dataframe[dataframe[\"speaker\"] == speaker]\n",
    "# for i in range(20):\n",
    "#     data_seq, digits = create_digit_sequence(data_george)\n",
    "#     print(\"Digits: {}\".format(digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f453d4",
   "metadata": {},
   "source": [
    "### Create \"meta\" HMM\n",
    "\n",
    "4.1 Combine the previously trained HMMs to a single \"meta\" HMM, altering the transition probabilities to make a circular graph that allows each word to follow another.\n",
    "\n",
    "4.2 Implement a method that converts a state sequence relating to the meta HMM into a sequence of actual digits.\n",
    "\n",
    "4.3 Decode your test sequences and compute the [word error rate](https://en.wikipedia.org/wiki/Word_error_rate) (WER) with [JiWER](https://pypi.org/project/jiwer/) (install the package in your working environment).\n",
    "\n",
    "4.4 Compute an overall WER; ie. over the cross-validation.\n",
    "\n",
    "4.5 (Optional) Implement a basic time-synchronous beam search; how do the results compare to the above viterbi decoding in terms of accuracy and time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "def meta_hmm(models: list[hmm.GaussianHMM]) -> tuple[hmm.GaussianHMM, list[int]]:\n",
    "    c = sum(m.n_components for m in models)\n",
    "    feature_dim = models[0].means_.shape[1]\n",
    "    covars = np.empty((c, feature_dim, feature_dim))\n",
    "    means = np.empty((c, feature_dim))\n",
    "    start_probs = np.zeros(c)\n",
    "    transmat = np.zeros((c, c))\n",
    "    start_prob = 1 / len(models)\n",
    "    trans_prob = (1 - STAY_PROB) / len(models)\n",
    "    start_idcs = []\n",
    "    \n",
    "    i = 0\n",
    "    for m in models:\n",
    "        start_idcs.append(i)\n",
    "        j = i + m.n_components\n",
    "        covars[i:j, :, :] = m.covars_\n",
    "        means[i:j, :] = m.means_\n",
    "        start_probs[i] = start_prob\n",
    "        transmat[i:j-1, i:j] = m.transmat_[:-1, :]\n",
    "        transmat[j, j] = STAY_PROB\n",
    "        k = 0\n",
    "        for m_ in models:\n",
    "            transmat[j, k] = trans_prob\n",
    "            k += m_.n_components\n",
    "        i = j\n",
    "\n",
    "    result = hmm.GaussianHMM(c, models[0].covariance_type, params=\"\")\n",
    "    result.startprob_ = start_probs\n",
    "    result.transmat_ = transmat\n",
    "    result.covars_ = covars\n",
    "    result.means_ = means\n",
    "    return result, start_idcs\n",
    "\n",
    "class SequenceDecoder:\n",
    "    @property\n",
    "    def keys(self) -> list[int]:\n",
    "        return list(self.keys)\n",
    "\n",
    "    def __init__(self, models: dict[int, hmm.GaussianHMM]):\n",
    "        self.__keys = list(models.keys())        \n",
    "        self.__model, self.__start_idcs = meta_hmm([models[k] for k in self.keys])\n",
    "        self.__lookup = {self.__start_idcs[i]: self.__keys[i] for i in range(len(self.__keys))}\n",
    "\n",
    "    def decode(self, sequence: np.ndarray) -> list[int]:\n",
    "        result = []\n",
    "        _, states = self.__model.decode(sequence)\n",
    "        it = iter(states)\n",
    "        last = it.next()\n",
    "        result.append(self.__lookup[last])\n",
    "        for s in it:\n",
    "            if s != last and s in self.__lookup:\n",
    "                result.append(self.__lookup[s])\n",
    "            last = s\n",
    "        return result\n",
    "\n",
    "\n",
    "### END YOUR CODE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
