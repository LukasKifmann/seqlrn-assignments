{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) RNN as Language Model\n",
    "\n",
    "Similar to the n-gram language models in the previous tasks, imagine you have to write another thesis and just want to generate an interesting topic.\n",
    "In this assignment, you will train and use Recurrent Neural Networks as language models to generate new potential thesis topics.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the assignment on Recurrent Neural Networks, we'll (again) heavily use [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "Here, we'll rely on the RNN and Embedding modules already implemented by PyTorch.\n",
    "You can imagine the Embedding layer as a simple lookup table that stores embeddings of a fixed dictionary and size (quite similar to the Word2Vec parameters we've trained in assignment 2).\n",
    "Head over to the [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) and [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) modules to gain some understanding of their functionality.\n",
    "Code for processing data samples, batching, converting to tensors, etc. can get messy and hard to maintain. \n",
    "Therefore, you can use PyTorch's [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). \n",
    "Get familiar with the basics of data handling, as it will help you for upcoming assignments.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import TypedDict, Iterator, Optional, Callable\n",
    "import re\n",
    "from functools import reduce\n",
    "from math import log, exp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from nltk import lm\n",
    "from nltk.lm import preprocessing as prep\n",
    "from nltk.lm.api import LanguageModel\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import masked\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torcheval.metrics import Perplexity\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Create a PyTorch Dataset class which handles your tokenized data with respect to model inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Thesis:\n",
    "    registration_date: str\n",
    "    due_date: str\n",
    "    year_academic: int\n",
    "    type: str\n",
    "    degree: str\n",
    "    language: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "\n",
    "class _Thesis(TypedDict):\n",
    "    Anmeldedatum: str\n",
    "    Abgabedatum: str\n",
    "    JahrAkademisch: str\n",
    "    Art: str\n",
    "    Grad: str\n",
    "    Sprache: str\n",
    "    Titel: str\n",
    "    Abstract: str\n",
    "\n",
    "def to_thesis(thesis: _Thesis) -> Thesis:\n",
    "    return Thesis(\n",
    "        registration_date=thesis[\"Anmeldedatum\"],\n",
    "        due_date=thesis[\"Abgabedatum\"],\n",
    "        year_academic=int(thesis[\"JahrAkademisch\"]),\n",
    "        type=thesis[\"JahrAkademisch\"],\n",
    "        degree=thesis[\"Grad\"],\n",
    "        language=thesis[\"Sprache\"],\n",
    "        title=thesis[\"Titel\"],\n",
    "        abstract=thesis[\"Abstract\"]\n",
    "    )\n",
    "\n",
    "def load_theses_dataset(filepath) -> pd.DataFrame:\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    lists = {key: [] for key in Thesis.__dataclass_fields__.keys()}\n",
    "    with open(filepath, encoding=\"utf-8-sig\") as fp:\n",
    "        theses = map(to_thesis, csv.DictReader(fp.readlines(), delimiter=\";\")) # type: ignore\n",
    "        for thesis in theses:\n",
    "            for key in lists:\n",
    "                lists[key].append(thesis.__dict__[key])\n",
    "    return pd.DataFrame(lists)\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: Think about start and end of sentence tokens\n",
    "\n",
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    yield \"<s>\"\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^(\\w+)?([,\\.?!])?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is not None:\n",
    "                yield m.group(1).lower()\n",
    "            if m.group(2) is not None:\n",
    "                yield m.group(2)\n",
    "    yield \"</s>\"\n",
    "\n",
    "def preprocess(dataframe) -> list[list[str]]:\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    l = []\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe[\"language\"][i] == \"DE\":\n",
    "            l.append(list(tokenize(dataframe[\"title\"][i])))\n",
    "    return l\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "THESES_DATASET_PATH = \"../4-nnet/data/theses2022.csv\"\n",
    "\n",
    "dataframe = load_theses_dataset(THESES_DATASET_PATH)\n",
    "tokenized_data = preprocess(dataframe)\n",
    "vocabulary = {w for l in tokenized_data for w in l}\n",
    "idx2word = sorted(list(vocabulary))\n",
    "word2idx = {w: i for i, w in enumerate(idx2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 1.3 Implement the PyTorch theses dataset\n",
    "### Notice: It is possible to solve the task without this class.\n",
    "### Notice: However, with respect to DataLoaders it makes your life easier.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class ThesesDataset(Dataset):\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    @property\n",
    "    def voc_size(self) -> int:\n",
    "        return len(self.__word2idx)\n",
    "\n",
    "    def __init__(self, dataset: list[list[str]], word2idx: dict[str, int], dtype: torch.dtype = torch.float32):\n",
    "        self.__max_length = reduce(lambda acc, l: acc if acc > l else l, map(len, dataset), 0) - 1\n",
    "        self.__seq_idcs = []\n",
    "        self.__lengths = []\n",
    "        for i, l in enumerate(dataset):\n",
    "            for j in range(len(l) - 1):\n",
    "                self.__seq_idcs.append(i)\n",
    "                self.__lengths.append(j)\n",
    "        self.__data = dataset\n",
    "        self.__word2idx = word2idx\n",
    "        self.__dtype = dtype\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__seq_idcs)\n",
    "\n",
    "    def __getitem__(self, idx: slice | int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if isinstance(idx, int):\n",
    "            return self.__get_single(idx)\n",
    "        else:\n",
    "            return self.__get_multiple(idx)\n",
    "    \n",
    "    def __get_single(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        seq = self.__data[self.__seq_idcs[idx]]\n",
    "        length = self.__lengths[idx]\n",
    "        x = torch.full((self.__max_length,), -1, dtype=torch.int32)\n",
    "        for i in range(length):\n",
    "            x[i] = self.__word2idx[seq[i]]\n",
    "        y = torch.zeros(self.voc_size, dtype=self.dtype)\n",
    "        y[self.__word2idx[seq[length]]] = 1\n",
    "        return x, y\n",
    "    \n",
    "    def __get_multiple(self, idcs: slice) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i in range(idcs.start, idcs.stop, idcs.step):\n",
    "            x, y = self.__get_single(i)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return torch.vstack(xs), torch.stack(ys)\n",
    "    \n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement the RNN Language Model. Therefore, you can use the nn.Module and overwrite the forward function. For the embedding layer you can either use the embeddings learned from the previous word2vec assignment or train the `nn.Embedding` module and corresponding parameters from scratch.\n",
    "\n",
    "2.2 Implement the functionality to train your model with the train dataset.\n",
    "\n",
    "2.3 Implement the functionality to evaluate your model with the test dataset.\n",
    "\n",
    "2.4 Perform a train-test-split for your theses data, train the RNN Language Model and evaluate the loss & perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement the RNN Language Model (nn.Module)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class RNN_LM(nn.Module):\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.__device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, value: str | torch.device):\n",
    "        if isinstance(value, str):\n",
    "            value = torch.device(value)\n",
    "        self.__device = value\n",
    "        self.to(self.device)\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    def __init__(self, voc_size: int, embedding_dim: int, hidden_layer_sizes: list[int], device: torch.device, dtype: torch.dtype = torch.float32,  **kwargs):\n",
    "        super(RNN_LM, self).__init__(**kwargs)\n",
    "        self.__device = device\n",
    "        self.__dtype = dtype\n",
    "        self.__hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.embeddings = nn.Embedding(voc_size + 1, embedding_dim, voc_size, device=device, dtype=dtype)\n",
    "        self.hidden = nn.ModuleList()\n",
    "        prev_size = embedding_dim\n",
    "        for size in hidden_layer_sizes:\n",
    "            self.hidden.append(nn.Linear(prev_size + size, size, True, device, dtype))\n",
    "            prev_size = size\n",
    "        self.classification_head = nn.Linear(prev_size, voc_size, True, device, dtype)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = [torch.zeros((X.shape[0], s), device=self.device, dtype=self.dtype) for s in self.__hidden_layer_sizes]\n",
    "        word_embeddings = self.embeddings(X % self.embeddings.num_embeddings)\n",
    "        for i in range(X.shape[1]):\n",
    "            mask = X[:, i] >= 0\n",
    "            if not mask.any().item():\n",
    "                break\n",
    "            x = word_embeddings[:, i, :]\n",
    "            new_hidden_states = [self.__update_hidden(0, x, hidden_states[0], mask)]\n",
    "            for j in range(1, len(self.hidden)):\n",
    "                new_hidden_states.append(self.__update_hidden(j, new_hidden_states[-1], hidden_states[j], mask))\n",
    "            hidden_states = new_hidden_states\n",
    "        return self.classification_head(hidden_states[-1])\n",
    "    \n",
    "    def sample(self, word_idx: int, temperature: float, hidden_states: Optional[list[torch.Tensor]] = None) -> tuple[int, list[torch.Tensor]]:\n",
    "        if hidden_states is None:\n",
    "            hidden_states = [torch.zeros(s, device=self.device, dtype=self.dtype) for s in self.__hidden_layer_sizes]\n",
    "        x = self.embeddings(torch.tensor(word_idx, device = self.device))\n",
    "        new_hidden_states = [self.__update_hidden(0, x, hidden_states[0], )]\n",
    "        for i in range(1, len(self.hidden)):\n",
    "            new_hidden_states.append(self.__update_hidden(i, new_hidden_states[-1], hidden_states[i]))\n",
    "        probs = F.softmax(self.classification_head(new_hidden_states[-1]) / temperature, dim=0)\n",
    "        return int(torch.multinomial(probs, 1).item()), new_hidden_states\n",
    "    \n",
    "    def __update_hidden(self, i: int, x: torch.Tensor, prior: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        result = F.relu(self.hidden[i](torch.hstack([x, prior])))\n",
    "        if mask is not None:\n",
    "            return torch.where(mask.reshape(-1, 1).repeat(1, result.shape[1]), result, prior)\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "### Notice: If you want, you can also combine train and eval functionality\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader, loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], opt: Optimizer):\n",
    "    \"\"\"Trains the RNN-LM for one epoch.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    batch_count = len(loader)\n",
    "    running_loss = 0.0\n",
    "    for i, (X, Y) in enumerate(loader):\n",
    "        X = X.to(model.device)\n",
    "        Y = Y.to(model.device)\n",
    "        print(f\"\\r  training batch {i+1}/{batch_count}\", end=\"\")\n",
    "        opt.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, Y)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print()\n",
    "    print(f\"  average loss: {running_loss/batch_count}\")\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Implement the evaluation functionality\n",
    "### Notice: If you want, you can also combine train and eval\n",
    "\n",
    "def eval(model: nn.Module, loader: DataLoader, loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]):\n",
    "    \"\"\"Evaluates the optimized RNN-LM.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    batch_count = len(loader)\n",
    "    perplexity = Perplexity(device=model.device)\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y) in enumerate(loader):\n",
    "            X = X.to(model.device)\n",
    "            Y = Y.to(model.device)\n",
    "            print(f\"\\r  evaluating batch {i+1}/{batch_count}\", end=\"\")\n",
    "            logits = model(X)\n",
    "            running_loss += loss_fn(logits, Y).item()\n",
    "            perplexity.update(logits.reshape(X.shape[0], 1, -1), torch.argmax(Y, dim=1, keepdim=True))\n",
    "    print(f\"  average loss: {running_loss/batch_count}, perplexity: {perplexity.compute().item()}\")\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1/10...\n",
      "  training batch 14/14\n",
      "  average loss: 8.596871067355975\n",
      "training epoch 2/10...\n",
      "  training batch 14/14\n",
      "  average loss: 7.459982713435571\n",
      "training epoch 3/10...\n",
      "  training batch 14/14\n",
      "  average loss: 6.396625950697266\n",
      "training epoch 4/10...\n",
      "  training batch 14/14\n",
      "  average loss: 5.999752654831615\n",
      "training epoch 5/10...\n",
      "  training batch 14/14\n",
      "  average loss: 5.763152556584784\n",
      "training epoch 6/10...\n",
      "  training batch 14/14\n",
      "  average loss: 5.60170031270284\n",
      "training epoch 7/10...\n",
      "  training batch 14/14\n",
      "  average loss: 5.461297493586921\n",
      "training epoch 8/10...\n",
      "  training batch 14/14\n",
      "  average loss: 5.333537012139554\n",
      "training epoch 9/10...\n",
      "  training batch 14/14\n",
      "  average loss: 5.216041535890646\n",
      "training epoch 10/10...\n",
      "  training batch 14/14\n",
      "  average loss: 5.121400634452568\n",
      "evaluating model...\n",
      "  evaluating batch 4/4  average loss: 6.339836095925435, perplexity: 583.6861545097472\n"
     ]
    }
   ],
   "source": [
    "### TODO: 2.4 Initialize and train the RNN Language Model for X epochs\n",
    "\n",
    "# For split reproducibility\n",
    "# Optional: Use 5-fold cross validation\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "DEVICE = \"cuda\" # 'cpu', 'mps' or 'cuda'\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "HIDDEN_LAYER_SIZES = [64]\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "train_data, test_data = train_test_split(tokenized_data, test_size=TEST_RATIO, random_state=SEED)\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "train_dataset = ThesesDataset(train_data, word2idx, torch.float64)\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, True)\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "test_dataset = ThesesDataset(test_data, word2idx, torch.float64)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, True)\n",
    "\n",
    "# Your language model\n",
    "model = RNN_LM(len(vocabulary), EMBEDDING_DIM, HIDDEN_LAYER_SIZES, torch.device(DEVICE), torch.float64)\n",
    "\n",
    "# Your loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Your optimizer (optim.SGD should be okay)\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# TODO: Training for epoch i\n",
    "\n",
    "model.train()\n",
    "for i in range(EPOCHS):\n",
    "    print(f\"training epoch {i+1}/{EPOCHS}...\")\n",
    "    train(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "# TODO: Evaluation for epoch i\n",
    "\n",
    "model.eval()\n",
    "print(f\"evaluating model...\")\n",
    "eval(model, test_dataloader, criterion)\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Generate Titles\n",
    "\n",
    "3.1 Use the trained RNN Language Model to generate theses titles. How can you sample the next tokens?\n",
    "\n",
    "3.2 Compare your results with n-gram language models (e.g., n=4). Of course, you can use a library such as NLTK toolkit\n",
    "- What perplexity does a regular 4-gram have on the same split? \n",
    "- Compare the generated titles from the 4-gram and RNN-LM. Do you think the n-gram titles are better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konzeption und implementierung einer applikation zur optimierung von und prototypische implementierung eines für die entwicklung einer der datev eg und prototypische realisierung einer zur unterstützung der siemens ag mit hilfe\n",
      "entwicklung einer mobilen endgeräten des und implementierung eines für die dem und implementierung eines für die im rahmen der datev eg für die entwicklung eines systems zur verwaltung von und\n",
      "konzeption und implementierung eines für die realisierung eines konzepts zur unterstützung der datev eg in der datev eg und implementierung eines zur unterstützung der datev eg für die entwicklung eines\n",
      "konzeption und realisierung eines systems zur automatischen eines konzepts zur verwaltung von microsoft dynamics nav unter verwendung von beim und prototypische implementierung einer hochschule und implementierung einer zur unterstützung der\n",
      "analyse und implementierung eines für die konzeption und realisierung einer zur visualisierung von methoden der datev eg für die einsatz von reifegradmodellen für den analyse und entwicklung eines konzepts zur\n",
      "konzeption und realisierung einer mobilen endgeräten und prototypische implementierung einer für die data und implementierung einer software zur unterstützung von methoden der datev eg und realisierung eines für die einbindung\n",
      "konzeption und realisierung eines systems zur unterstützung der datev eg und implementierung eines systems zur verwaltung von und implementierung einer anwendung für das in der datev eg für die entwicklung\n",
      "automatische testen und implementierung eines konzepts zur unterstützung der datev eg und vergleich von mit im bereich der betrieblichen von und implementierung einer zur verbesserung der datev eg für die\n",
      "konzeption und realisierung eines verfahrens zur unterstützung der datev eg für das in der datev eg für die vergleich von business intelligence bei der datev eg mit hilfe von und\n",
      "konzeption und realisierung einer anwendung und prototypische implementierung einer im , entwicklung eines prototypen für die durch von und implementierung eines für die automatisierung der datev eg mit fokus auf\n"
     ]
    }
   ],
   "source": [
    "### TODO: 3.1 Generate titles with the trained RNN Language Model\n",
    "\n",
    "def generate(model: RNN_LM, word2idx: dict[str, int], idx2word: list[str], temperature: float, max_length: int) -> list[str]:\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    start = word2idx[\"<s>\"]\n",
    "    end = word2idx[\"</s>\"]\n",
    "    seq = [start]\n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            token, hidden = model.sample(seq[-1], temperature, hidden)\n",
    "            if token == end:\n",
    "                break\n",
    "            seq.append(token)\n",
    "    return list(map(lambda i: idx2word[seq[i]], range(1, len(seq))))\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "for i in range(10):\n",
    "    generated_title = generate(model, word2idx, idx2word, 0.4, 30)\n",
    "    print(\" \".join(generated_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "749babb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of n-gram model: 3532.0888655944823\n",
      "\n",
      "generated titles:\n",
      "\n",
      "vergleich fehlermanagementsystem trilum objekten getaeb betrieblicher fertigungszeit <UNK> geräteübergreifende prozessarchitekturen ansätze punktwolke bank bildklassifikation linuxserver kleinunternehmen dreier decoupling app healthcare gebrauchsanweisungen kunststoffspritzgussmaschinen verbrauchsdatenerfassung f abspielen austausch personenortung robert stochastischer nlp\n",
      "charakterisierung softwaregestützte kryptosystemen resultierenden eignungsvergleich ressourceneinsatzplanung internationalen gesamten verfahrens vpn datenvolumen vernetzung wissensdatenbank controllingreports darlehen prozessautomatisierung elektronischen autokonfigurator 6 portallösung spracherkenners jukebox klassifikator bestandsmanagements simulationen plugins maschinellem schuhen wiki energiewirtschaft\n",
      "bahnplanung versehenen ebilanz packages intranet kurven packproblems prognosemethoden raumfahrtunternehmens php4 firmendaten fehlermustern telekommunikationssystems testergebnissen sensornetzes isswas studenten serious i personalwirtschaft datenauswertungstools kernelsoftware realistischen zeichnen softwarequalität anlagensimulation relationalen studieninteressenten projektarbeit navigationsstrukturen\n",
      "problematik stores livebilddaten hochschulmanagement sle atemkorrelierte entscheidungsunterstützungssystems passenden datenauswertungstools linguistisch steuergeräte fahreignung fakultätsjahresbericht anwendern aufsichtsratswahlen vorgehensmodelle zentraleinkauf noten grafische einschließlich belieferung arbeitsabläufe buchhaltung fachwissenschaftlichen selbstkonfigurirenden einsatz mittelständischen verbalmorphologie fertigungsindustrie lieferungen\n",
      "vorabanmeldungen entscheidungshilfe modellaufbau blockierung messverfahren bedrohungsmodellierungen ziel akkreditierten obis 27001 phantom systemínitialisierung programmieren quantisierung versand klassenmodellen lehre ingsoft dialogkomponente softwarelösung projektmanagementsystem unternehmerischer detailgradsteuerung immobiliendaten mehrkernsysteme befähigung verlagswesen bis legacy autonome\n",
      "in arbeitsplatzes conversion bekanntwerden cloudgestützter completion möglicher rentenversicherung kundenspezifische funktionskonzept teil vorgehensmodell wegen abgesicherten paralleler nintex qualitätsverbesserung wiedererkennung robotik bayrischen spyware verteilten silbentrennung digital komplexen operationen gewinnung internetbasierender agierenden kleinunternehmen\n",
      "klassifikation dataraces darstellung softwarearchitektur anbetracht probabilistischer steuergeräte modellierter trends abfrage öffentliches automatisiertes programming navigationssysteme einflüsse som tätigen griechischen entwicklungen wirtschaftl versuche robotoprogrammierung familie reifegradmodels wirtschaftlichkeitsbetrachtung betriebsdatenerfassung konfigurationsoberfläche ethischen estimation headerdateien\n",
      "softwareenineering autorisierungstechnologien nutzweranalyse handschriftlicher formale zugriffskontrolle funktionskonzept befundungsarbeitsplatz menschen niedrigenergiehaus sendungsverfolgungssystems desktopbereitstellung unternehmensdaten interdisziplinären netzwerktopologie aufnahmen ankopplung verwenden soap filterlisten intranetportal unternehmensnetzwerken verkaufsterminals grundschutz kürzesten auswerteapplikationen distanzlehre lokalisierung vielen speisen\n",
      "audioerkennung lösungsansätzen detailgradsteuerung softwaregestütztes sensomotorischen datevnet fachlich proof formularauswertung ameisenalgorithmen verbrauchssparende flächenlichtquellen aufzeigen dokumentenmanagementsystems akkreditierten langzeitstabile werkzeuge population flächenlichtquellen verlagswesen effects johnson und exportierung weboberflächen elementfunktion objektstrukturen stadtweites points audiobearbeitung\n",
      "workflowbasierten neuen gamification datenschutzgrundverordnung ce glint webbasierte redaktionswerkezugs ausgearbeiteten arm applikationsdeployments covid automobilen ring warehouse dynamics bildsequenzen raumfahrtunternehmens marketingplan netzterminierung synchroner anforderungsanalyse steuergerät smartcard authentifizierung natürlicher remote pages fachrichtung behälterproblems\n"
     ]
    }
   ],
   "source": [
    "### TODO: 3.2 Generate titles with the trained n-gram language model\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def build_n_gram_model(n: int, data: list[list[str]]) -> LanguageModel:\n",
    "\n",
    "    trn, voc = prep.padded_everygram_pipeline(n, data)\n",
    "    model = lm.Laplace(n)\n",
    "    model.fit(trn, voc)\n",
    "    return model\n",
    "\n",
    "def sample_next_token(prev: list[str], n_gram_model: LanguageModel) -> Optional[str]:\n",
    "    \n",
    "    n = n_gram_model.order\n",
    "    r = random.random() * (1 - n_gram_model.score(\"<s>\", prev[-n:]))\n",
    "    sum = 0\n",
    "    for w in n_gram_model.vocab:\n",
    "        if w == \"<s>\":\n",
    "            continue\n",
    "        sum += n_gram_model.score(w, prev[-n:])\n",
    "        if r < sum:\n",
    "            return w\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_with_n_gram_model(n_gram_model: LanguageModel, title_length: int)-> list[str]:\n",
    "    \n",
    "    n = n_gram_model.order\n",
    "    l = [\"<s>\"]\n",
    "    for _ in range(title_length):\n",
    "        next = sample_next_token(l, n_gram_model)\n",
    "        if next is None or next == \"</s>\":\n",
    "            break\n",
    "        l.append(next)\n",
    "    return list(l[i] for i in range(1, len(l)))\n",
    "\n",
    "def perplexity(model: LanguageModel, data: list[list[str]]) -> float:\n",
    "    sum_log_prob = 0\n",
    "    count = 0\n",
    "    for seq in data:\n",
    "        for i in range(2, len(seq)):\n",
    "            sum_log_prob += log(model.score(seq[i], seq[i-model.order:i]))\n",
    "            count += 1\n",
    "    return exp(-sum_log_prob/count)\n",
    "\n",
    "\n",
    "n_gram_model = build_n_gram_model(4, train_data)\n",
    "\n",
    "n_gram_perplexity = perplexity(n_gram_model, test_data)\n",
    "print(f\"perplexity of n-gram model: {n_gram_perplexity}\")\n",
    "\n",
    "print(\"\\ngenerated titles:\\n\")\n",
    "for i in range(10):\n",
    "    generated_title = generate_with_n_gram_model(n_gram_model, 30)\n",
    "    print(\" \".join(generated_title))\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
