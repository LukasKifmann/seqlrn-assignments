{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) RNN as Language Model\n",
    "\n",
    "Similar to the n-gram language models in the previous tasks, imagine you have to write another thesis and just want to generate an interesting topic.\n",
    "In this assignment, you will train and use Recurrent Neural Networks as language models to generate new potential thesis topics.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the assignment on Recurrent Neural Networks, we'll (again) heavily use [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "Here, we'll rely on the RNN and Embedding modules already implemented by PyTorch.\n",
    "You can imagine the Embedding layer as a simple lookup table that stores embeddings of a fixed dictionary and size (quite similar to the Word2Vec parameters we've trained in assignment 2).\n",
    "Head over to the [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) and [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) modules to gain some understanding of their functionality.\n",
    "Code for processing data samples, batching, converting to tensors, etc. can get messy and hard to maintain. \n",
    "Therefore, you can use PyTorch's [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). \n",
    "Get familiar with the basics of data handling, as it will help you for upcoming assignments.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import TypedDict, Iterator, Optional, Callable\n",
    "import re\n",
    "from functools import reduce\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import masked\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torcheval.metrics import Perplexity\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Create a PyTorch Dataset class which handles your tokenized data with respect to model inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Thesis:\n",
    "    registration_date: str\n",
    "    due_date: str\n",
    "    year_academic: int\n",
    "    type: str\n",
    "    degree: str\n",
    "    language: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "\n",
    "class _Thesis(TypedDict):\n",
    "    Anmeldedatum: str\n",
    "    Abgabedatum: str\n",
    "    JahrAkademisch: str\n",
    "    Art: str\n",
    "    Grad: str\n",
    "    Sprache: str\n",
    "    Titel: str\n",
    "    Abstract: str\n",
    "\n",
    "def to_thesis(thesis: _Thesis) -> Thesis:\n",
    "    return Thesis(\n",
    "        registration_date=thesis[\"Anmeldedatum\"],\n",
    "        due_date=thesis[\"Abgabedatum\"],\n",
    "        year_academic=int(thesis[\"JahrAkademisch\"]),\n",
    "        type=thesis[\"JahrAkademisch\"],\n",
    "        degree=thesis[\"Grad\"],\n",
    "        language=thesis[\"Sprache\"],\n",
    "        title=thesis[\"Titel\"],\n",
    "        abstract=thesis[\"Abstract\"]\n",
    "    )\n",
    "\n",
    "def load_theses_dataset(filepath) -> pd.DataFrame:\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    lists = {key: [] for key in Thesis.__dataclass_fields__.keys()}\n",
    "    with open(filepath, encoding=\"utf-8-sig\") as fp:\n",
    "        theses = map(to_thesis, csv.DictReader(fp.readlines(), delimiter=\";\")) # type: ignore\n",
    "        for thesis in theses:\n",
    "            for key in lists:\n",
    "                lists[key].append(thesis.__dict__[key])\n",
    "    return pd.DataFrame(lists)\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: Think about start and end of sentence tokens\n",
    "\n",
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    yield \"<s>\"\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^(\\w+)?([,\\.?!])?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is not None:\n",
    "                yield m.group(1).lower()\n",
    "            if m.group(2) is not None:\n",
    "                yield m.group(2)\n",
    "    yield \"</s>\"\n",
    "\n",
    "def preprocess(dataframe) -> list[list[str]]:\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    l = []\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe[\"language\"][i] == \"DE\":\n",
    "            l.append(list(tokenize(dataframe[\"abstract\"][i])))\n",
    "    return l\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "THESES_DATASET_PATH = \"../4-nnet/data/theses2022.csv\"\n",
    "\n",
    "dataframe = load_theses_dataset(THESES_DATASET_PATH)\n",
    "tokenized_data = preprocess(dataframe)\n",
    "vocabulary = {w for l in tokenized_data for w in l}\n",
    "idx2word = sorted(list(vocabulary))\n",
    "word2idx = {w: i for i, w in enumerate(idx2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 1.3 Implement the PyTorch theses dataset\n",
    "### Notice: It is possible to solve the task without this class.\n",
    "### Notice: However, with respect to DataLoaders it makes your life easier.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class ThesesDataset(Dataset):\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    @property\n",
    "    def voc_size(self) -> int:\n",
    "        return len(self.__word2idx)\n",
    "\n",
    "    def __init__(self, dataset: list[list[str]], word2idx: dict[str, int], dtype: torch.dtype = torch.float16):\n",
    "        self.__max_length = reduce(lambda acc, l: acc if acc > l else l, map(len, dataset), 0) - 1\n",
    "        self.__seq_idcs = []\n",
    "        self.__lengths = []\n",
    "        for i, l in enumerate(dataset):\n",
    "            for j in range(len(l) - 1):\n",
    "                self.__seq_idcs.append(i)\n",
    "                self.__lengths.append(j)\n",
    "        self.__data = dataset\n",
    "        self.__word2idx = word2idx\n",
    "        self.__dtype = dtype\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__seq_idcs)\n",
    "\n",
    "    def __getitem__(self, idx: slice | int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if isinstance(idx, int):\n",
    "            return self.__get_single(idx)\n",
    "        else:\n",
    "            return self.__get_multiple(idx)\n",
    "    \n",
    "    def __get_single(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        seq = self.__data[self.__seq_idcs[idx]]\n",
    "        length = self.__lengths[idx]\n",
    "        _x = torch.zeros((length , self.voc_size), dtype=self.dtype)\n",
    "        for i in range(length):\n",
    "            _x[i, self.__word2idx[seq[i]]] = 1\n",
    "        x = torch.vstack([_x, torch.full((self.__max_length - length, self.voc_size), torch.nan, dtype=self.dtype)])\n",
    "        y = torch.zeros(self.voc_size, dtype=self.dtype)\n",
    "        y[self.__word2idx[seq[length]]] = 1\n",
    "        return x, y\n",
    "    \n",
    "    def __get_multiple(self, idcs: slice) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i in range(idcs.start, idcs.stop, idcs.step):\n",
    "            x, y = self.__get_single(i)\n",
    "            xs.append(x.reshape(1, x.shape[0], x.shape[1]))\n",
    "            ys.append(y)\n",
    "        return torch.vstack(xs), torch.stack(ys)\n",
    "    \n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement the RNN Language Model. Therefore, you can use the nn.Module and overwrite the forward function. For the embedding layer you can either use the embeddings learned from the previous word2vec assignment or train the `nn.Embedding` module and corresponding parameters from scratch.\n",
    "\n",
    "2.2 Implement the functionality to train your model with the train dataset.\n",
    "\n",
    "2.3 Implement the functionality to evaluate your model with the test dataset.\n",
    "\n",
    "2.4 Perform a train-test-split for your theses data, train the RNN Language Model and evaluate the loss & perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement the RNN Language Model (nn.Module)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class RNN_LM(nn.Module):\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.__device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, value: str | torch.device):\n",
    "        if isinstance(value, str):\n",
    "            value = torch.device(value)\n",
    "        self.__device = value\n",
    "        self.to(self.device)\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    def __init__(self, voc_size: int, embedding_dim: int, hidden_layer_sizes: list[int], device: torch.device, dtype: torch.dtype = torch.float16,  **kwargs):\n",
    "        super(RNN_LM, self).__init__(**kwargs)\n",
    "        self.__device = device\n",
    "        self.__dtype = dtype\n",
    "        self.__hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.embeddings = nn.Linear(voc_size, embedding_dim, False, device, dtype)\n",
    "        self.hidden: list[nn.Linear] = []\n",
    "        prev_size = embedding_dim\n",
    "        for size in hidden_layer_sizes:\n",
    "            self.hidden.append(nn.Linear(prev_size + size, size, True, device, dtype))\n",
    "            prev_size = size\n",
    "        self.classification_head = nn.Linear(prev_size, voc_size, True, device, dtype)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = [torch.zeros((X.shape[0], s), device=self.device, dtype=self.dtype) for s in self.__hidden_layer_sizes]\n",
    "        word_embeddings = self.embeddings(X)\n",
    "        for i in range(X.shape[1]):\n",
    "            x = word_embeddings[:, i, :]\n",
    "            mask = (~x[:, 0:1].isnan())\n",
    "            print(mask)\n",
    "            if not mask.any().item():\n",
    "                break\n",
    "            new_hidden_states = [self.__update_hidden(0, x, hidden_states[0], mask)]\n",
    "            for j in range(1, len(self.hidden)):\n",
    "                new_hidden_states.append(self.__update_hidden(j, new_hidden_states[-1], hidden_states[j], mask))\n",
    "            hidden_states = new_hidden_states\n",
    "        return self.classification_head(hidden_states[-1])\n",
    "    \n",
    "    def __update_hidden(self, i: int, x: torch.Tensor, prior: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        result = F.relu(self.hidden[i](torch.hstack([x, prior])))\n",
    "        mask.any\n",
    "        return torch.where(mask.repeat(1, result.shape[1]), result, prior)\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "### Notice: If you want, you can also combine train and eval functionality\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader, loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], opt: Optimizer):\n",
    "    \"\"\"Trains the RNN-LM for one epoch.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    batch_count = len(loader)\n",
    "    running_loss = 0\n",
    "    for i, (X, Y) in enumerate(loader):\n",
    "        X = X.to(model.device)\n",
    "        Y = Y.to(model.device)\n",
    "        print(f\"\\r  training batch {i+1}/{batch_count}\", end=\"\")\n",
    "        opt.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, Y)\n",
    "        print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print()\n",
    "    print(f\"  average loss: {running_loss/batch_count}\")\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Implement the evaluation functionality\n",
    "### Notice: If you want, you can also combine train and eval\n",
    "\n",
    "def eval(model: nn.Module, loader: DataLoader, loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]):\n",
    "    \"\"\"Evaluates the optimized RNN-LM.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    batch_count = len(loader)\n",
    "    perplexity = Perplexity()\n",
    "    running_perp = 0\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y) in loader:\n",
    "            X = X.to(model.device)\n",
    "            Y = Y.to(model.device)\n",
    "            print(f\"\\r  evaluating batch {i+1}/{batch_count}\")\n",
    "            logits = model(X)\n",
    "            running_loss += loss_fn(logits, Y).item()\n",
    "            perplexity.update(X, torch.argmax(Y, dim=1))\n",
    "            running_perp += perplexity.compute().item()\n",
    "    print()\n",
    "    print(f\"  average loss: {running_loss/batch_count}, average perplexity: {running_perp/batch_count}\")\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1/100...\n",
      "  training batch 1/92329tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[True]], device='cuda:0')\n",
      "tensor([[False]], device='cuda:0')\n",
      "9.515625\n",
      "  training batch 2/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 3/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 4/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 5/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 6/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 7/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 8/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 9/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 10/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 11/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 12/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 13/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 14/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 15/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 16/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 17/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 18/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 19/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 20/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 21/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 22/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 23/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 24/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 25/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 26/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 27/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 28/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 29/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 30/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 31/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 32/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 33/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 34/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 35/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 36/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 37/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 38/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 39/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 40/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 41/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 42/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 43/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 44/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 45/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 46/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 47/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 48/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 49/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 50/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 51/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 52/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 53/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 54/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 55/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 56/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 57/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 58/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 59/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 60/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 61/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 62/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 63/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 64/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 65/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 66/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 67/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 68/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 69/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 70/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 71/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 72/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 73/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 74/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 75/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 76/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 77/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 78/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 79/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 80/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 81/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 82/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 83/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 84/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 85/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 86/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 87/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 88/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 89/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 90/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 91/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 92/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 93/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 94/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 95/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 96/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 97/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 98/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 99/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 100/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 101/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 102/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 103/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 104/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 105/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 106/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 107/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 108/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 109/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 110/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 111/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 112/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 113/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 114/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 115/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 116/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 117/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 118/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 119/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 120/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 121/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 122/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 123/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 124/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 125/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 126/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 127/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 128/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 129/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 130/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 131/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 132/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 133/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 134/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 135/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 136/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 137/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 138/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 139/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 140/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 141/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 142/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 143/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 144/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 145/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 146/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 147/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 148/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 149/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 150/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 151/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 152/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 153/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 154/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 155/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 156/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 157/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 158/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 159/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 160/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 161/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 162/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 163/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 164/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 165/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 166/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 167/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 168/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 169/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 170/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 171/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 172/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 173/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 174/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 175/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 176/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 177/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 178/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 179/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 180/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 181/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 182/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 183/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 184/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 185/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 186/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 187/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 188/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 189/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 190/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 191/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 192/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 193/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 194/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 195/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 196/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 197/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 198/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 199/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 200/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 201/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 202/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 203/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 204/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 205/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 206/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 207/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 208/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 209/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 210/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 211/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 212/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 213/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 214/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 215/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 216/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 217/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 218/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 219/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 220/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 221/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 222/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 223/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 224/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 225/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 226/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 227/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 228/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 229/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 230/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 231/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 232/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 233/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 234/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 235/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 236/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 237/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 238/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 239/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 240/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 241/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 242/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 243/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 244/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 245/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 246/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 247/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 248/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 249/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 250/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 251/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 252/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 253/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 254/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 255/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 256/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 257/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 258/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 259/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 260/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 261/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 262/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 263/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 264/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 265/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 266/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 267/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 268/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 269/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 270/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 271/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 272/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 273/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 274/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 275/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 276/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 277/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 278/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 279/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 280/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 281/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 282/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 283/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 284/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 285/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 286/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 287/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 288/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 289/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 290/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 291/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 292/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 293/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 294/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 295/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 296/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 297/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 298/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 299/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 300/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 301/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 302/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 303/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 304/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 305/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 306/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 307/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 308/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 309/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 310/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 311/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 312/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 313/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 314/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 315/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 316/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 317/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 318/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 319/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 320/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 321/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 322/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 323/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 324/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 325/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 326/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 327/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 328/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 329/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 330/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 331/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 332/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 333/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 334/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 335/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 336/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 337/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 338/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 339/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 340/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 341/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 342/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 343/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 344/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 345/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 346/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 347/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 348/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 349/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 350/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 351/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 352/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 353/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 354/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 355/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 356/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 357/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 358/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 359/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 360/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 361/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 362/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 363/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 364/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 365/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 366/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 367/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 368/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 369/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 370/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 371/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 372/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 373/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 374/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 375/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 376/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 377/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 378/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 379/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 380/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 381/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 382/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 383/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 384/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 385/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 386/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 387/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 388/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 389/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 390/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 391/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 392/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 393/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 394/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 395/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 396/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 397/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 398/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 399/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 400/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 401/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 402/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 403/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 404/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 405/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 406/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 407/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 408/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 409/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 410/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 411/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 412/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 413/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 414/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 415/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 416/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 417/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 418/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 419/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 420/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 421/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 422/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 423/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 424/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 425/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 426/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 427/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 428/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 429/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 430/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 431/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 432/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 433/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 434/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 435/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 436/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 437/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 438/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 439/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 440/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 441/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 442/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 443/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 444/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 445/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 446/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 447/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 448/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 449/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 450/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 451/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 452/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 453/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 454/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 455/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 456/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 457/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 458/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 459/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 460/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 461/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 462/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 463/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 464/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 465/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 466/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 467/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 468/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 469/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 470/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 471/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 472/92329tensor([[False]], device='cuda:0')\n",
      "nan\n",
      "  training batch 473/92329tensor([[False]], device='cuda:0')\n",
      "nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# TODO: Evaluation for epoch i\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[115], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, loss_fn, opt)\u001b[0m\n\u001b[1;32m      8\u001b[0m batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[1;32m      9\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (X, Y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m     11\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m     Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/GITHUB/seqlrn/assignments/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:147\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[collate_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMapping\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/abc.py:117\u001b[0m, in \u001b[0;36mABCMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Register a virtual subclass of an ABC.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Returns the subclass, to allow usage as a class decorator.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _abc_register(\u001b[38;5;28mcls\u001b[39m, subclass)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _abc_instancecheck(\u001b[38;5;28mcls\u001b[39m, instance)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TODO: 2.4 Initialize and train the RNN Language Model for X epochs\n",
    "\n",
    "# For split reproducibility\n",
    "# Optional: Use 5-fold cross validation\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "DEVICE = \"cuda\" # 'cpu', 'mps' or 'cuda'\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "HIDDEN_LAYER_SIZES = [256, 64, 64]\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "train_data, test_data = train_test_split(tokenized_data, test_size=TEST_RATIO, random_state=SEED)\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "train_dataset = ThesesDataset(train_data, word2idx)\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, True)\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "test_dataset = ThesesDataset(test_data, word2idx)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, True)\n",
    "\n",
    "# Your language model\n",
    "model = RNN_LM(len(vocabulary), EMBEDDING_DIM, HIDDEN_LAYER_SIZES, torch.device(DEVICE))\n",
    "\n",
    "# Your loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Your optimizer (optim.SGD should be okay)\n",
    "optimizer = Adam(model.parameters(), 0.0001)\n",
    "\n",
    "\n",
    "# TODO: Training for epoch i\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(f\"training epoch {i+1}/{EPOCHS}...\")\n",
    "    train(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "# TODO: Evaluation for epoch i\n",
    "\n",
    "print(f\"evaluating model...\")\n",
    "eval(model, test_dataloader, criterion)\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Generate Titles\n",
    "\n",
    "3.1 Use the trained RNN Language Model to generate theses titles. How can you sample the next tokens?\n",
    "\n",
    "3.2 Compare your results with n-gram language models (e.g., n=4). Of course, you can use a library such as NLTK toolkit\n",
    "- What perplexity does a regular 4-gram have on the same split? \n",
    "- Compare the generated titles from the 4-gram and RNN-LM. Do you think the n-gram titles are better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m### END YOUR CODE\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     generated_title \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(generated_title))\n",
      "Cell \u001b[0;32mIn[64], line 6\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(arguments)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(arguments):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m### YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TODO: 3.1 Generate titles with the trained RNN Language Model\n",
    "\n",
    "def generate(arguments):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "for i in range(10):\n",
    "    generated_title = generate(None)\n",
    "    print(\" \".join(generated_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749babb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.2 Generate titles with the trained n-gram language model\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
