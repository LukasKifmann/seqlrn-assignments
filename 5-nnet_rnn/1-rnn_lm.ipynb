{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) RNN as Language Model\n",
    "\n",
    "Similar to the n-gram language models in the previous tasks, imagine you have to write another thesis and just want to generate an interesting topic.\n",
    "In this assignment, you will train and use Recurrent Neural Networks as language models to generate new potential thesis topics.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the assignment on Recurrent Neural Networks, we'll (again) heavily use [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "Here, we'll rely on the RNN and Embedding modules already implemented by PyTorch.\n",
    "You can imagine the Embedding layer as a simple lookup table that stores embeddings of a fixed dictionary and size (quite similar to the Word2Vec parameters we've trained in assignment 2).\n",
    "Head over to the [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) and [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) modules to gain some understanding of their functionality.\n",
    "Code for processing data samples, batching, converting to tensors, etc. can get messy and hard to maintain. \n",
    "Therefore, you can use PyTorch's [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). \n",
    "Get familiar with the basics of data handling, as it will help you for upcoming assignments.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import TypedDict, Iterator, Optional\n",
    "import re\n",
    "from functools import reduce\n",
    "from math import floor, ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Create a PyTorch Dataset class which handles your tokenized data with respect to model inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Thesis:\n",
    "    registration_date: str\n",
    "    due_date: str\n",
    "    year_academic: int\n",
    "    type: str\n",
    "    degree: str\n",
    "    language: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "\n",
    "class _Thesis(TypedDict):\n",
    "    Anmeldedatum: str\n",
    "    Abgabedatum: str\n",
    "    JahrAkademisch: str\n",
    "    Art: str\n",
    "    Grad: str\n",
    "    Sprache: str\n",
    "    Titel: str\n",
    "    Abstract: str\n",
    "\n",
    "def to_thesis(thesis: _Thesis) -> Thesis:\n",
    "    return Thesis(\n",
    "        registration_date=thesis[\"Anmeldedatum\"],\n",
    "        due_date=thesis[\"Abgabedatum\"],\n",
    "        year_academic=int(thesis[\"JahrAkademisch\"]),\n",
    "        type=thesis[\"JahrAkademisch\"],\n",
    "        degree=thesis[\"Grad\"],\n",
    "        language=thesis[\"Sprache\"],\n",
    "        title=thesis[\"Titel\"],\n",
    "        abstract=thesis[\"Abstract\"]\n",
    "    )\n",
    "\n",
    "def load_theses_dataset(filepath) -> pd.DataFrame:\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    lists = {key: [] for key in Thesis.__dataclass_fields__.keys()}\n",
    "    with open(filepath, encoding=\"utf-8-sig\") as fp:\n",
    "        theses = map(to_thesis, csv.DictReader(fp.readlines(), delimiter=\";\")) # type: ignore\n",
    "        for thesis in theses:\n",
    "            for key in lists:\n",
    "                lists[key].append(thesis.__dict__[key])\n",
    "    return pd.DataFrame(lists)\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: Think about start and end of sentence tokens\n",
    "\n",
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    yield \"<s>\"\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^(\\w+)?([,\\.?!])?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is not None:\n",
    "                yield m.group(1).lower()\n",
    "            if m.group(2) is not None:\n",
    "                yield m.group(2)\n",
    "    yield \"</s>\"\n",
    "\n",
    "def preprocess(dataframe) -> list[list[str]]:\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    l = []\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe[\"language\"][i] == \"DE\":\n",
    "            l.append(list(tokenize(dataframe[\"abstract\"][i])))\n",
    "    return l\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "THESES_DATASET_PATH = \"../4-nnet/data/theses2022.csv\"\n",
    "\n",
    "dataframe = load_theses_dataset(THESES_DATASET_PATH)\n",
    "tokenized_data = preprocess(dataframe)\n",
    "vocabulary = {w for l in tokenized_data for w in l}\n",
    "idx2word = sorted(list(vocabulary))\n",
    "word2idx = {w: i for i, w in enumerate(idx2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (4031134988.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 57\u001b[0;36m\u001b[0m\n\u001b[0;31m    if stop is None\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "### TODO: 1.3 Implement the PyTorch theses dataset\n",
    "### Notice: It is possible to solve the task without this class.\n",
    "### Notice: However, with respect to DataLoaders it makes your life easier.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class ThesesDataset(Dataset):\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    @property\n",
    "    def voc_size(self) -> int:\n",
    "        return len(self.__word2idx)\n",
    "\n",
    "    def __init__(self, dataset: list[list[str]], word2idx: list[str], dtype: torch.dtype = torch.float16):\n",
    "        self.__max_length = reduce(lambda acc, l: acc if acc > l else l, map(len, dataset), 0) - 1\n",
    "        self.__seq_idcs = []\n",
    "        self.__lengths = []\n",
    "        for i, l in enumerate(dataset):\n",
    "            for j in range(len(l) - 1):\n",
    "                self.__seq_idcs.append(i)\n",
    "                self.__lengths.append(j)\n",
    "        self.__data = dataset\n",
    "        self.__word2idx = word2idx\n",
    "        self.__dtype = dtype\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__seq_idcs)\n",
    "\n",
    "    def __getitem__(self, idx: slice | int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if isinstance(idx, int):\n",
    "            return self.__get_single(idx)\n",
    "        else:\n",
    "            return self.__get_multiple(idx)\n",
    "    \n",
    "    def __get_single(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        seq = self.__data[self.__seq_idcs[idx]]\n",
    "        length = self.__lengths[idx]\n",
    "        _x = torch.zeros((self.__max_length , self.voc_size), dtype=self.dtype)\n",
    "        for i in range(length):\n",
    "            _x[i, self.__word2idx[seq[i]]] = 1\n",
    "        x = masked.masked_tensor(_x, torch.vstack([\n",
    "            torch.full((length, self.voc_size), True),\n",
    "            torch.full((self.__max_length - length, self.voc_size), False)\n",
    "        ]), True)\n",
    "        y = torch.zeros(self.voc_size, dtype=self.dtype)\n",
    "        y[self.__word2idx[seq[length]]] = 1\n",
    "        return x, y\n",
    "    \n",
    "    def __get_multiple(self, idcs: slice) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i in range(idcs.start, idcs.stop, idcs.step):\n",
    "            x, y = self.__get_single(i)\n",
    "            xs.append(x.reshape(1, x.shape[0], x.shape[1]))\n",
    "            ys.append(y)\n",
    "        return torch.vstack(xs), torch.stack(ys)\n",
    "    \n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement the RNN Language Model. Therefore, you can use the nn.Module and overwrite the forward function. For the embedding layer you can either use the embeddings learned from the previous word2vec assignment or train the `nn.Embedding` module and corresponding parameters from scratch.\n",
    "\n",
    "2.2 Implement the functionality to train your model with the train dataset.\n",
    "\n",
    "2.3 Implement the functionality to evaluate your model with the test dataset.\n",
    "\n",
    "2.4 Perform a train-test-split for your theses data, train the RNN Language Model and evaluate the loss & perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement the RNN Language Model (nn.Module)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class RNN_LM(nn.Module):\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.__device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, value: str | torch.device):\n",
    "        if isinstance(value, str):\n",
    "            value = torch.device(value)\n",
    "        self.__device = value\n",
    "        self.to(self.device)\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    def __init__(self, voc_size: int, embedding_dim: int, hidden_layer_sizes: list[int], device: torch.device, dtype: torch.dtype = torch.float16,  **kwargs):\n",
    "        super(RNN_LM, self).__init__(**kwargs)\n",
    "        self.__device = device\n",
    "        self.__dtype = dtype\n",
    "        self.__hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.embeddings = nn.Linear(voc_size, embedding_dim, False, device, dtype)\n",
    "        self.hidden: list[nn.Linear] = []\n",
    "        prev_size = embedding_dim\n",
    "        for size in hidden_layer_sizes:\n",
    "            self.hidden.append(nn.Linear(prev_size + size, size, True, device, dtype))\n",
    "            prev_size = size\n",
    "        self.classification_head = nn.Linear(prev_size, voc_size, True, device, dtype)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = [torch.zeros((X.shape[0], s), device=self.device, dtype=self.dtype) for s in self.__hidden_layer_sizes]\n",
    "        word_embeddings = self.embeddings(X)\n",
    "        for i in range(X.shape[1]):\n",
    "            new_hidden_states = [self.hidden[0](torch.hstack([word_embeddings[:, i, :], hidden_states[0]]))]\n",
    "            for j in range(1, len(self.hidden)):\n",
    "                new_hidden_states.append(self.hidden[i](torch.hstack([new_hidden_states[-1], hidden_states[j]])))\n",
    "            hidden_states = new_hidden_states\n",
    "        return self.classification_head(hidden_states[-1])\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "### Notice: If you want, you can also combine train and eval functionality\n",
    "\n",
    "def train(arguments):\n",
    "    \"\"\"Trains the RNN-LM for one epoch.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2c016",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Implement the evaluation functionality\n",
    "### Notice: If you want, you can also combine train and eval\n",
    "\n",
    "def eval(arguments):\n",
    "    \"\"\"Evaluates the optimized RNN-LM.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.4 Initialize and train the RNN Language Model for X epochs\n",
    "\n",
    "# For split reproducibility\n",
    "# Optional: Use 5-fold cross validation\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "DEVICE = \"cpu\" # 'cpu', 'mps' or 'cuda'\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "train_dataset = None\n",
    "train_dataloader = None\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "test_dataset = None\n",
    "test_dataloader = None\n",
    "\n",
    "# Your language model\n",
    "model = None\n",
    "\n",
    "# Your loss function\n",
    "criterion = None\n",
    "\n",
    "# Your optimizer (optim.SGD should be okay)\n",
    "optimizer = None\n",
    "\n",
    "\n",
    "# TODO: Training for epoch i\n",
    "\n",
    "# TODO: Evaluation for epoch i\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Generate Titles\n",
    "\n",
    "3.1 Use the trained RNN Language Model to generate theses titles. How can you sample the next tokens?\n",
    "\n",
    "3.2 Compare your results with n-gram language models (e.g., n=4). Of course, you can use a library such as NLTK toolkit\n",
    "- What perplexity does a regular 4-gram have on the same split? \n",
    "- Compare the generated titles from the 4-gram and RNN-LM. Do you think the n-gram titles are better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.1 Generate titles with the trained RNN Language Model\n",
    "\n",
    "def generate(arguments):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "for i in range(10):\n",
    "    generated_title = generate(None)\n",
    "    print(\" \".join(generated_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749babb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.2 Generate titles with the trained n-gram language model\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
