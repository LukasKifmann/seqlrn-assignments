{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2) RNN for Classification\n",
    "\n",
    "The theses dataset also contains types (diploma, bachelor, master) and categories (internal/external) for each thesis. \n",
    "In this part, we want to classify whether the thesis is bachelor or master; and if it's internal or external. \n",
    "Since PyTorch provides most things sort-of out of the box, we want you to compare the following Recurrent Neural Network variation: \n",
    "[RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), [GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html), [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html), and Bidirectional-[LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) by using the `bidirectional` flag.\n",
    "The basic setup as well as some code and steps can be reused from your solution for the language modeling task.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the assignment on Recurrent Neural Networks, we'll (again) heavily use [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "Here, we'll rely on the RNN and Embedding modules already implemented by PyTorch.\n",
    "You can imagine the Embedding layer as a simple lookup table that stores embeddings of a fixed dictionary and size (quite similar to the Word2Vec parameters we've trained in assignment 2).\n",
    "Head over to the [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) and [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) modules to gain some understanding of their functionality.\n",
    "Code for processing data samples, batching, converting to tensors, etc. can get messy and hard to maintain. \n",
    "Therefore, you can use PyTorch's [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). \n",
    "Get familiar with the basics of data handling, as it will help you for upcoming assignments.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import TypedDict, Iterator, Optional, TypeVar, Generic, Callable\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from functools import reduce\n",
    "from abc import abstractmethod, ABC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "from torch.optim import Optimizer, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Filter out all diploma theses; they might be too easy to spot because they only cover \"old\" topics.\n",
    "\n",
    "1.4 Create a PyTorch Dataset class which handles your tokenized data with respect to input and (class) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Thesis:\n",
    "    registration_date: str\n",
    "    due_date: str\n",
    "    year_academic: int\n",
    "    type: str\n",
    "    degree: str\n",
    "    language: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "\n",
    "class _Thesis(TypedDict):\n",
    "    Anmeldedatum: str\n",
    "    Abgabedatum: str\n",
    "    JahrAkademisch: str\n",
    "    Art: str\n",
    "    Grad: str\n",
    "    Sprache: str\n",
    "    Titel: str\n",
    "    Abstract: str\n",
    "\n",
    "def to_thesis(thesis: _Thesis) -> Thesis:\n",
    "    return Thesis(\n",
    "        registration_date=thesis[\"Anmeldedatum\"],\n",
    "        due_date=thesis[\"Abgabedatum\"],\n",
    "        year_academic=int(thesis[\"JahrAkademisch\"]),\n",
    "        type=thesis[\"JahrAkademisch\"],\n",
    "        degree=thesis[\"Grad\"],\n",
    "        language=thesis[\"Sprache\"],\n",
    "        title=thesis[\"Titel\"],\n",
    "        abstract=thesis[\"Abstract\"]\n",
    "    )\n",
    "\n",
    "def load_theses_dataset(filepath) -> pd.DataFrame:\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    lists = {key: [] for key in Thesis.__dataclass_fields__.keys()}\n",
    "    with open(filepath, encoding=\"utf-8-sig\") as fp:\n",
    "        theses = map(to_thesis, csv.DictReader(fp.readlines(), delimiter=\";\")) # type: ignore\n",
    "        for thesis in theses:\n",
    "            for key in lists:\n",
    "                lists[key].append(thesis.__dict__[key])\n",
    "    return pd.DataFrame(lists)\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: Think about start and end of sentence tokens\n",
    "\n",
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    yield \"<s>\"\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^(\\w+)?([,\\.?!])?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is not None:\n",
    "                yield m.group(1).lower()\n",
    "            if m.group(2) is not None:\n",
    "                yield m.group(2)\n",
    "    yield \"</s>\"\n",
    "\n",
    "def preprocess(dataframe) -> tuple[list[list[str]], list[str]]:\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    seqs = []\n",
    "    degrees = []\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe[\"language\"][i] == \"DE\":\n",
    "            seqs.append(list(tokenize(dataframe[\"title\"][i])))\n",
    "            degrees.append(dataframe[\"degree\"][i])\n",
    "    return seqs, degrees\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "THESES_DATASET_PATH = \"../4-nnet/data/theses2022.csv\"\n",
    "\n",
    "dataframe = load_theses_dataset(THESES_DATASET_PATH)\n",
    "tokenized_data, degrees = preprocess(dataframe)\n",
    "vocabulary = {w for l in tokenized_data for w in l}\n",
    "idx2word = sorted(list(vocabulary))\n",
    "word2idx = {w: i for i, w in enumerate(idx2word)}\n",
    "unique_degrees = sorted(list(set(degrees)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 1.3 Implement the PyTorch theses dataset\n",
    "### Notice: It is possible to solve the task without this class.\n",
    "### Notice: However, with respect to DataLoaders it makes your life easier.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class ThesesDataset(Dataset):\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    @property\n",
    "    def voc_size(self) -> int:\n",
    "        return len(self.__word2idx)\n",
    "    \n",
    "    @property\n",
    "    def class_count(self) -> int:\n",
    "        return len(self.__unique_degrees)\n",
    "\n",
    "    def __init__(self, sequences: list[list[str]], degrees:list[str], word2idx: dict[str, int], unique_degrees: list[str], dtype: torch.dtype = torch.float32):\n",
    "        self.__sequences = sequences\n",
    "        self.__degrees = degrees\n",
    "        self.__word2idx = word2idx\n",
    "        self.__unique_degrees = unique_degrees\n",
    "        self.__dtype = dtype\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__sequences)\n",
    "\n",
    "    def __getitem__(self, idx: slice | int) -> tuple[torch.Tensor | PackedSequence, torch.Tensor]:\n",
    "        if isinstance(idx, int):\n",
    "            return self.__get_single(idx)\n",
    "        else:\n",
    "            return self.__get_multiple(idx)\n",
    "    \n",
    "    def __get_single(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        seq = self.__sequences[idx]\n",
    "        x = torch.tensor([self.__word2idx[i] for i in seq], dtype=torch.int32)\n",
    "        y = torch.zeros(self.class_count, dtype=self.dtype)\n",
    "        y[self.__unique_degrees.index(self.__degrees[idx])] = 1\n",
    "        return x, y\n",
    "    \n",
    "    def __get_multiple(self, idcs: slice) -> tuple[PackedSequence, torch.Tensor]:\n",
    "        return ThesesDataset.collate([self.__get_single(i) for i in range(idcs.start, idcs.stop, idcs.step)])\n",
    "    \n",
    "    def loader(self, batch_size: int) -> DataLoader:\n",
    "        return DataLoader(self, batch_size, True, collate_fn=ThesesDataset.collate)    \n",
    "    \n",
    "    @staticmethod\n",
    "    def collate(tups: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[PackedSequence, torch.Tensor]:\n",
    "        tups.sort(key=lambda tup: tup[0].shape[0], reverse=True)\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for tup in tups:\n",
    "            xs.append(tup[0])\n",
    "            ys.append(tup[1])\n",
    "        return pack_sequence(xs), torch.vstack(ys)\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement the RNN for Classification. Therefore, you can use the nn.Module and overwrite the forward function.\n",
    "\n",
    "2.2 Train and evaluate your models with 5-fold cross-validation. As in RNN-LM, you can either learn the embeddings from scratch or reuse the ones from word2vec.\n",
    "\n",
    "2.3 Assemble a table: Recall/Precision/F1 measure for each of the mentioned RNN variants (RNN, GRU, LSTM). Which one works best?\n",
    "\n",
    "2.4 Bonus: Apply your best classifier to the remaining diploma theses; are those on average more bachelor or master? :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement the RNN classifier (nn.Module)\n",
    "### Notice: Think about padding for batch sizes > 1\n",
    "### Notice: 'torch.nn.utils.rnn' provides functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "class RNNClassifierBase(nn.Module, ABC):\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.__device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, value: str | torch.device):\n",
    "        if isinstance(value, str):\n",
    "            value = torch.device(value)\n",
    "        self.__device = value\n",
    "        self.to(self.device)\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return self.__dtype\n",
    "    \n",
    "    def __init__(self, voc_size: int, embedding_dim: int, device: torch.device, dtype: torch.dtype, **kwargs):\n",
    "        super(RNNClassifierBase, self).__init__(**kwargs)        \n",
    "        self.__device = device\n",
    "        self.__dtype = dtype\n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_dim, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "class RNN_Classifier(RNNClassifierBase):    \n",
    "    def __init__(self, voc_size: int, class_count, embedding_dim: int, hidden_layer_size: int, hidden_layer_count: int, device: torch.device, dtype: torch.dtype = torch.float32,  **kwargs):\n",
    "        super(RNN_Classifier, self).__init__(voc_size, embedding_dim, device, dtype, **kwargs)\n",
    "        self.__hidden_layer_size = hidden_layer_size\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.hidden.append(nn.Linear(embedding_dim + hidden_layer_size, hidden_layer_size, True, device, dtype))\n",
    "        for _ in range(hidden_layer_count - 1):\n",
    "            self.hidden.append(nn.Linear(2 * hidden_layer_size, hidden_layer_size, True, device, dtype))\n",
    "        self.classification_head = nn.Linear(hidden_layer_size, class_count, True, device, dtype)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor | PackedSequence) -> torch.Tensor:\n",
    "        if isinstance(X, PackedSequence):\n",
    "            hidden_states = [torch.zeros((int(X.batch_sizes[0].item()), self.__hidden_layer_size), device=self.device, dtype=self.dtype) for _ in range(len(self.hidden))] \n",
    "            word_embeddings = self.embeddings(X.data)\n",
    "            start = 0\n",
    "            for batch_size in X.batch_sizes:\n",
    "                stop = start + batch_size\n",
    "                x = word_embeddings[start:stop, :]\n",
    "                new_hidden_states = [self.__update_hidden(0, x, hidden_states[0])]\n",
    "                for i in range(1, len(self.hidden)):\n",
    "                    new_hidden_states.append(self.__update_hidden(i, new_hidden_states[-1][:batch_size, :], hidden_states[i]))\n",
    "                hidden_states = new_hidden_states\n",
    "                start = stop\n",
    "            return self.classification_head(hidden_states[-1])\n",
    "        else:\n",
    "            hidden_states = [torch.zeros(self.__hidden_layer_size, device=self.device, dtype=self.dtype) for _ in range(len(self.hidden))]\n",
    "            word_embeddings = self.embeddings(X)\n",
    "            for i in range(X.shape[0]):\n",
    "                new_hidden_states = [self.__update_hidden(0, word_embeddings[i, :], hidden_states[0])]\n",
    "                for i in range(1, len(self.hidden)):\n",
    "                    new_hidden_states.append(self.__update_hidden(i, new_hidden_states[-1], hidden_states[i]))\n",
    "                hidden_states = new_hidden_states\n",
    "            return self.classification_head(hidden_states[-1])\n",
    "    \n",
    "    def __update_hidden(self, i: int, x: torch.Tensor, prior: torch.Tensor) -> torch.Tensor:\n",
    "        if len(prior.shape) == 2:\n",
    "            return torch.vstack([\n",
    "                F.relu(self.hidden[i](torch.hstack([x, prior[0:x.shape[0], :]]))),\n",
    "                prior[x.shape[0]:, :]\n",
    "            ])\n",
    "        else:\n",
    "            return F.relu(self.hidden[i](torch.hstack([x, prior])))\n",
    "\n",
    "def _get_features_for_classification(X: torch.Tensor | PackedSequence, bidirectional: bool) -> torch.Tensor:\n",
    "    if isinstance(X, PackedSequence):\n",
    "        feature_list = []\n",
    "        total = 0\n",
    "        i = X.batch_sizes.shape[0] - 1\n",
    "        stop = X.data.shape[0]\n",
    "        if bidirectional:\n",
    "            features_per_dir = X.data.shape[1]//2\n",
    "            while total < X.batch_sizes[0]:\n",
    "                start = stop - X.batch_sizes[i] + total\n",
    "                count = stop - start\n",
    "                if count != 0:\n",
    "                    total += count\n",
    "                    feature_list.append(X.data[start:stop, :features_per_dir])\n",
    "                stop -= X.batch_sizes[i]\n",
    "                i -= 1\n",
    "            return torch.hstack([\n",
    "                torch.vstack(feature_list),\n",
    "                X.data[:X.batch_sizes[0], features_per_dir:]\n",
    "            ])\n",
    "        else:\n",
    "            while total < X.batch_sizes[0]:\n",
    "                start = stop - X.batch_sizes[i] + total\n",
    "                count = stop - start\n",
    "                if count != 0:\n",
    "                    total += count\n",
    "                    feature_list.append(X.data[start:stop, :])\n",
    "                stop -= X.batch_sizes[i]\n",
    "                i -= 1\n",
    "            return torch.vstack(feature_list)\n",
    "    else:\n",
    "        if bidirectional:\n",
    "            features_per_dir = X.data.shape[1]//2\n",
    "            return torch.hstack([X.data[-1, :features_per_dir], X.data[0, features_per_dir:]])\n",
    "        else:\n",
    "            return X.data[-1, :]\n",
    "\n",
    "class LSTMClassifier(RNNClassifierBase):    \n",
    "    @property\n",
    "    def bidirectional(self) -> bool:\n",
    "        return self.__bidirectional\n",
    "    \n",
    "    def __init__(self, voc_size: int, class_count, embedding_dim: int, hidden_layer_size: int, hidden_layer_count: int, bidirectional: bool, device: torch.device, dtype: torch.dtype = torch.float32,  **kwargs):\n",
    "        super(LSTMClassifier, self).__init__(voc_size, embedding_dim, device, dtype, **kwargs)\n",
    "        self.__bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_layer_size, hidden_layer_count, bidirectional=bidirectional, device=device, dtype=dtype)\n",
    "        self.classificaiton_head = nn.Linear((2 if self.bidirectional else 1) * hidden_layer_size, class_count, True, device, dtype)\n",
    "\n",
    "    def forward(self, X: torch.Tensor | PackedSequence) -> torch.Tensor:\n",
    "        if isinstance(X, PackedSequence):\n",
    "            word_embeddings = self.embeddings(X.data)\n",
    "            lstm_out, _ = self.lstm(PackedSequence(word_embeddings, X.batch_sizes, None, None))\n",
    "        else:\n",
    "            word_embeddings = self.embeddings(X.data)\n",
    "            lstm_out, _ = self.lstm(word_embeddings)\n",
    "        return self.classificaiton_head(_get_features_for_classification(lstm_out, self.bidirectional))\n",
    "        \n",
    "class GRUClassifier(RNNClassifierBase):    \n",
    "    @property\n",
    "    def bidirectional(self) -> bool:\n",
    "        return self.__bidirectional\n",
    "\n",
    "    def __init__(self, voc_size: int, class_count, embedding_dim: int, hidden_layer_size: int, hidden_layer_count: int, bidirectional: bool, device: torch.device, dtype: torch.dtype = torch.float32,  **kwargs):\n",
    "        super(GRUClassifier, self).__init__(voc_size, embedding_dim, device, dtype, **kwargs)\n",
    "        self.__bidirectional = bidirectional\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_layer_size, hidden_layer_count, bidirectional=bidirectional, device=device, dtype=dtype)\n",
    "        self.classification_head = nn.Linear((2 if self.bidirectional else 1) * hidden_layer_size, class_count, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, X: torch.Tensor | PackedSequence) -> torch.Tensor:\n",
    "        if isinstance(X, PackedSequence):\n",
    "            word_embeddings = self.embeddings(X.data)\n",
    "            gru_out, _ = self.gru(PackedSequence(word_embeddings, X.batch_sizes, None, None))\n",
    "        else:\n",
    "            word_embeddings = self.embeddings(X)\n",
    "            gru_out, _ = self.gru(word_embeddings)\n",
    "        return self.classification_head(_get_features_for_classification(gru_out, self.bidirectional))\n",
    "        \n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "### Notice: If you want, you can also combine train and eval functionality\n",
    "\n",
    "def train(model: RNNClassifierBase, loader: DataLoader, loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], opt: Optimizer):\n",
    "    \"\"\"Trains the RNN-Classifier for one epoch.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    batch_count = len(loader)\n",
    "    running_loss = 0.0\n",
    "    for i, (X, Y) in enumerate(loader):\n",
    "        X = X.to(model.device)\n",
    "        Y = Y.to(model.device)\n",
    "        print(f\"\\r  training batch {i+1}/{batch_count}\", end=\"\")\n",
    "        opt.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, Y)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print()\n",
    "    print(f\"  average loss: {running_loss/batch_count}\")\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the evaluation functionality\n",
    "### Notice: If you want, you can also combine train and eval\n",
    "\n",
    "def eval(model: nn.Module, loader: DataLoader, loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]) -> float:\n",
    "    \"\"\"Evaluates the optimized RNN-Classifier.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    batch_count = len(loader)\n",
    "    running_loss = 0.0\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y) in enumerate(loader):\n",
    "            X = X.to(model.device)\n",
    "            Y = Y.to(model.device)\n",
    "            print(f\"\\r  evaluating batch {i+1}/{batch_count}\", end=\"\")\n",
    "            logits = model(X)\n",
    "            running_loss += loss_fn(logits, Y).item()\n",
    "            for y in torch.argmax(logits, 1).tolist():\n",
    "                Y_pred.append(y)\n",
    "            for y in torch.argmax(Y, 1).tolist():\n",
    "                Y_true.append(y)\n",
    "    C = confusion_matrix(Y_true, Y_pred)\n",
    "    acc = (C.diagonal().sum() / C.sum())\n",
    "    prec = (C.diagonal() / C.sum(0) + 1e-128)\n",
    "    rec = (C.diagonal() / C.sum(1) + 1e-128)\n",
    "    f1 = 2 / ((1 / prec) + (1 / rec))\n",
    "    print(f\"  average loss: {running_loss/batch_count}\")\n",
    "    print(f\"  accuracy:         {acc.item()}\")\n",
    "    print(f\"  precision scores: {prec.tolist()}\")\n",
    "    print(f\"  recall scores:    {rec.tolist()}\")\n",
    "    print(f\"  F1 scores:        {f1.tolist()}\")\n",
    "    return np.mean(f1).item()\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "# Fold 1 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2602261837039675\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2474031490939004\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24686498727117265\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24558061787060328\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2441821907247816\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24627713433333806\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24379871147019522\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24206108919211797\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24054197541304997\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23919914875711715\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23579312222344534\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23369149012225016\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22723433588232314\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2203186558825629\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21427431063992636\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2040607567344393\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1962267266852515\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1848894783428737\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17450284319264547\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16364649363926478\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3337044268846512\n",
      "  accuracy:         0.6056338028169014\n",
      "  precision scores: [0.7643312101910829, 0.16071428571428573]\n",
      "  recall scores:    [0.718562874251497, 0.1956521739130435]\n",
      "  F1 scores:        [0.7407407407407407, 0.17647058823529413]\n",
      "new best model found!\n",
      "\n",
      "##########\n",
      "# Fold 2 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24709609150886536\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24605153926781245\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2461145669221878\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2458497349705015\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24314884415694646\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2434955026422228\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2402926300253187\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2379582247563771\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.235717596752303\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23061116465500422\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22603100538253784\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2177483652319227\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2097190831388746\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.201704312648092\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19312816006796701\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18213239525045669\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17260820737906865\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16312939992972783\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15414966004235403\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1430046898978097\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.33325573801994324\n",
      "  accuracy:         0.5821596244131455\n",
      "  precision scores: [0.7846715328467153, 0.21710526315789475]\n",
      "  recall scores:    [0.6437125748502994, 0.358695652173913]\n",
      "  F1 scores:        [0.7072368421052632, 0.27049180327868855]\n",
      "new best model found!\n",
      "\n",
      "##########\n",
      "# Fold 3 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.248167912874903\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2457054363829749\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24707440606185369\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24527448628629958\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24414638536317007\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24316317481654032\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24130619210856302\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24039816004889353\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23736254658017839\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2350527857031141\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.230611828821046\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22365381249359675\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2167060375213623\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2096982662166868\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20227292392935073\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1944437175989151\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18411104806831904\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17439452239445277\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1669845368180956\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1588937214442662\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.32640647888183594\n",
      "  accuracy:         0.6094117647058823\n",
      "  precision scores: [0.7755775577557755, 0.19672131147540983]\n",
      "  recall scores:    [0.7057057057057057, 0.2608695652173913]\n",
      "  F1 scores:        [0.7389937106918238, 0.22429906542056077]\n",
      "\n",
      "##########\n",
      "# Fold 4 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24807188979216985\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24720614297049387\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24499923842293875\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2445427498647145\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24361609348229\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24339544773101807\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24097565880843572\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23863826479230607\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23683862388134003\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23395470636231558\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22676963252680643\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2201423900468009\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2121128205742155\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2042648047208786\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19425939236368453\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18429775748934066\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17284704106194632\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1582286528178624\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14752245587962015\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13480996446950094\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3948832154273987\n",
      "  accuracy:         0.6211764705882353\n",
      "  precision scores: [0.7866666666666666, 0.224]\n",
      "  recall scores:    [0.7087087087087087, 0.30434782608695654]\n",
      "  F1 scores:        [0.7456556082148499, 0.25806451612903225]\n",
      "new best model found!\n",
      "\n",
      "##########\n",
      "# Fold 5 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2464855079139982\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24886193658624375\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2462381933416639\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24556401797703334\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24421297013759613\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.24210404923983983\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23930314183235168\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23735457445893968\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23279797179358347\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22884327811854227\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2236910398517336\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21503098734787532\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20643075661999838\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1976460473878043\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19037601351737976\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17768071378980363\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16700226281370437\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1586727350950241\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1441508595432554\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1332740666610854\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.32681192457675934\n",
      "  accuracy:         0.5849056603773585\n",
      "  precision scores: [0.7940074906367042, 0.22929936305732485]\n",
      "  recall scores:    [0.6366366366366366, 0.3956043956043956]\n",
      "  F1 scores:        [0.7066666666666667, 0.2903225806451613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TODO: 2.2 Initialize and train the RNN-Classifier for X epochs\n",
    "\n",
    "best_model_f1 = -1.0\n",
    "best_model_class = \"\"\n",
    "best_model_fold = -1\n",
    "best_model = None\n",
    "best_model_bidirectional = False\n",
    "\n",
    "# For split reproducibility\n",
    "# Use 5-fold cross validation\n",
    "SEED = 42\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "bachelor_indices = []\n",
    "master_indices = []\n",
    "diploma_indices = []\n",
    "for i, d in enumerate(degrees):\n",
    "    if d == \"Bachelor\":\n",
    "        bachelor_indices.append(i)\n",
    "    elif d == \"Master\":\n",
    "        master_indices.append(i)\n",
    "    elif d == \"Diplom\":\n",
    "        diploma_indices.append(i)\n",
    "\n",
    "fold_idcs_train = [[] for _ in range(NUM_FOLDS)]#\n",
    "fold_idcs_test = [[] for _ in range(NUM_FOLDS)]\n",
    "\n",
    "k_fold = KFold(5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for l in [bachelor_indices, master_indices]:\n",
    "    for i, (train_idcs, test_idcs) in enumerate(k_fold.split(l)):\n",
    "        for j in train_idcs:\n",
    "            fold_idcs_train[i].append(l[j])\n",
    "        for j in test_idcs:\n",
    "            fold_idcs_test[i].append(l[j])\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "DEVICE = \"cuda:0\" # 'cpu', 'mps' or 'cuda'\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "HIDDEN_LAYER_SIZE = 32\n",
    "HIDDEN_LAYER_COUNT = 1\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "for i in range(NUM_FOLDS):\n",
    "\n",
    "    # Use batch_size=1 if you want to avoid padding handling\n",
    "    train_dataset = ThesesDataset(\n",
    "        [tokenized_data[j] for j in fold_idcs_train[i]],\n",
    "        [degrees[j] for j in fold_idcs_train[i]],\n",
    "        word2idx,\n",
    "        [\"Bachelor\", \"Master\"]\n",
    "    )\n",
    "    train_dataloader = train_dataset.loader(BATCH_SIZE)\n",
    "\n",
    "    # Use batch_size=1 if you want to avoid padding handling\n",
    "    test_dataset = ThesesDataset(\n",
    "        [tokenized_data[j] for j in fold_idcs_test[i]],\n",
    "        [degrees[j] for j in fold_idcs_test[i]],\n",
    "        word2idx,\n",
    "        [\"Bachelor\", \"Master\"]\n",
    "    )\n",
    "    test_dataloader = test_dataset.loader(BATCH_SIZE)\n",
    "\n",
    "    # Your language model\n",
    "    model = RNN_Classifier(len(word2idx), 2, EMBEDDING_DIM, HIDDEN_LAYER_SIZE, HIDDEN_LAYER_COUNT, torch.device(DEVICE))\n",
    "\n",
    "    # Your loss function\n",
    "    criterion = nn.CrossEntropyLoss(torch.tensor([0.25,0.75], device=model.device))\n",
    "\n",
    "    # Your optimizer (optim.SGD should be okay)\n",
    "    optimizer = Adam(model.parameters())\n",
    "\n",
    "    print(\"##########\")\n",
    "    print(f\"# Fold {i+1} #\")\n",
    "    print(\"##########\\n\")\n",
    "\n",
    "\n",
    "    # TODO: Training for epoch i\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        print(f\"training epoch {e+1}/{EPOCHS}...\")\n",
    "        train(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "\n",
    "    # TODO: Evaluation for epoch i\n",
    "\n",
    "    print(\"evaluating model...\")\n",
    "    f1 = eval(model, test_dataloader, criterion)\n",
    "    if f1 > best_model_f1:\n",
    "        print(\"new best model found!\")\n",
    "        best_model_f1 = f1\n",
    "        best_model_fold = i\n",
    "        best_model = model\n",
    "        best_model_class = best_model.__class__.__name__\n",
    "        \n",
    "    print()\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "7a178fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "# Fold 1 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.228196063211986\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22709385199206217\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22598325780459813\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22532422627721513\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2237403861113957\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22256567861352647\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21928690373897552\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2169070690870285\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2114428792681013\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20591068054948533\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19689242328916276\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1897202879190445\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1770175269671849\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16487481338637217\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14904211461544037\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13344003473009383\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11627176297562462\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10132879870278495\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08500839450529643\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.06978295424154826\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3812141865491867\n",
      "  accuracy:         0.6314553990610329\n",
      "  precision scores: [0.7940199335548173, 0.24]\n",
      "  recall scores:    [0.7155688622754491, 0.32608695652173914]\n",
      "  F1 scores:        [0.752755905511811, 0.2764976958525346]\n",
      "new best model found!\n",
      "\n",
      "##########\n",
      "# Fold 2 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2289043175322669\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22687907942703792\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22775866304125106\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22637798743588583\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.224392831325531\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22294439588274276\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22085068481309073\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21788510041577475\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21176548089299882\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20310326984950475\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19387042309556687\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17944075805800302\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1674463791506631\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15196935619626725\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1368789098092488\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1207020314676421\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10467063848461423\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08803509282214302\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07220706450087684\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.05886804738215038\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.46748581528663635\n",
      "  accuracy:         0.5657276995305164\n",
      "  precision scores: [0.7670250896057348, 0.1836734693877551]\n",
      "  recall scores:    [0.6407185628742516, 0.29347826086956524]\n",
      "  F1 scores:        [0.6982055464926592, 0.22594142259414227]\n",
      "\n",
      "##########\n",
      "# Fold 3 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22889727992670877\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22787949229989732\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2268765973193305\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22544656693935394\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22542975417205266\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2238439747265407\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22200935653277806\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21928209705012186\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2136454220329012\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20843311079910823\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20074790503297532\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19063928723335266\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1816100116286959\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16891287480081832\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15584707473005568\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14195664652756282\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12834446345056807\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11168178170919418\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09584727138280869\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07869901827403478\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3910094201564789\n",
      "  accuracy:         0.6094117647058823\n",
      "  precision scores: [0.7774086378737541, 0.20161290322580644]\n",
      "  recall scores:    [0.7027027027027027, 0.2717391304347826]\n",
      "  F1 scores:        [0.7381703470031545, 0.23148148148148145]\n",
      "\n",
      "##########\n",
      "# Fold 4 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22757548093795776\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22658332543713705\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22624461778572627\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22366492663111007\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2224844970873424\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22094871955258505\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21740348849977767\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2117131416286741\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20778852701187134\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19924510376793997\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1898461409977504\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17909632623195648\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16522176350866044\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1516210287809372\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1363007596560887\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12069254368543625\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10476271914584297\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09196573602301734\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.0748092776962689\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.062322456389665604\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.40675032138824463\n",
      "  accuracy:         0.6235294117647059\n",
      "  precision scores: [0.8035087719298246, 0.2571428571428571]\n",
      "  recall scores:    [0.6876876876876877, 0.391304347826087]\n",
      "  F1 scores:        [0.7411003236245955, 0.3103448275862069]\n",
      "new best model found!\n",
      "\n",
      "##########\n",
      "# Fold 5 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22859829451356614\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22646233439445496\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22589673101902008\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22492052614688873\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22302874071257456\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22013417099203383\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2176140695810318\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21338929661682673\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20837350615433284\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20152599258082254\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1924792868750436\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1848809484924589\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17221444632325852\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1596842301743371\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14848057287079947\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1325484131063734\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11785063786166054\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10147368695054736\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08658444136381149\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07312011931623731\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3968043476343155\n",
      "  accuracy:         0.5778301886792453\n",
      "  precision scores: [0.7851851851851852, 0.21428571428571427]\n",
      "  recall scores:    [0.6366366366366366, 0.3626373626373626]\n",
      "  F1 scores:        [0.7031509121061359, 0.26938775510204077]\n",
      "\n",
      "##########\n",
      "# Fold 1 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.228671418769019\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.226609040583883\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22622510577951158\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22332837539059774\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22050590174538748\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21821947395801544\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21482014443193162\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20847501712185995\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20058399864605495\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1903899950640542\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1794331478221076\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16458460688591003\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14932589020047868\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13249305848564422\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11572587277208056\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09901968815496989\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08450080773660115\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07137200981378555\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.057134444160120826\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.0445670203438827\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.40883825719356537\n",
      "  accuracy:         0.6291079812206573\n",
      "  precision scores: [0.7894736842105263, 0.22950819672131148]\n",
      "  recall scores:    [0.718562874251497, 0.30434782608695654]\n",
      "  F1 scores:        [0.7523510971786834, 0.26168224299065423]\n",
      "\n",
      "##########\n",
      "# Fold 2 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22983360929148539\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22870431627546037\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2277702135699136\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22528742253780365\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.223820584160941\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2236468004328864\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22105961825166429\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21636122252259934\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2111188301018306\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20538872693266189\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19536347687244415\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18440317256110056\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16933488845825195\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15377319497721537\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13852640560695104\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12372505558388573\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10492652867521558\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09061067125626973\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07336104501570974\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.060182021132537296\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3541409522294998\n",
      "  accuracy:         0.6056338028169014\n",
      "  precision scores: [0.7862068965517242, 0.22058823529411764]\n",
      "  recall scores:    [0.6826347305389222, 0.32608695652173914]\n",
      "  F1 scores:        [0.7307692307692308, 0.2631578947368421]\n",
      "\n",
      "##########\n",
      "# Fold 3 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2316544737134661\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2266822968210493\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22717886311667307\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22462110008512223\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22347709110804967\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22073406406811305\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2167976200580597\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2134933705840792\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2075673065015248\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2015896247965949\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19365827313491277\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17988052112715586\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16820235550403595\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15282660722732544\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1348706324185644\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11853944084474019\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09839750613485064\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08157556397574288\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.06488974285977227\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.04994472063013485\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.4870869219303131\n",
      "  accuracy:         0.6658823529411765\n",
      "  precision scores: [0.7975077881619937, 0.25961538461538464]\n",
      "  recall scores:    [0.7687687687687688, 0.29347826086956524]\n",
      "  F1 scores:        [0.7828746177370031, 0.2755102040816327]\n",
      "new best model found!\n",
      "\n",
      "##########\n",
      "# Fold 4 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.228643604687282\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22684119641780853\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.225078461425645\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22300367270197188\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22067269470010484\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21738831060273306\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21392786077090672\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20898068589823587\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2009230319942747\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1914874847446169\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18024813490254538\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16787189032350266\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15681095847061702\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13883171336991446\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12260072146143232\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10618390994412559\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08924892970493861\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07482305062668664\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.06255409653697695\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.04950140682714326\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.4529818892478943\n",
      "  accuracy:         0.6282352941176471\n",
      "  precision scores: [0.7777777777777778, 0.2]\n",
      "  recall scores:    [0.7357357357357357, 0.2391304347826087]\n",
      "  F1 scores:        [0.7561728395061729, 0.21782178217821782]\n",
      "\n",
      "##########\n",
      "# Fold 5 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2276083699294499\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2270426026412419\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22675434180668422\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22385705155985697\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22283834431852614\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21918888602937972\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2170648170369012\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.210416955607278\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2051522433757782\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19688870012760162\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1868170542376382\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1772347092628479\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16288630664348602\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1489736565521785\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13356246799230576\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11654736633811678\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10001564983810697\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.0841512041432517\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.0673977934888431\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.05380963321243014\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.45760945975780487\n",
      "  accuracy:         0.6202830188679245\n",
      "  precision scores: [0.7866666666666666, 0.21774193548387097]\n",
      "  recall scores:    [0.7087087087087087, 0.2967032967032967]\n",
      "  F1 scores:        [0.7456556082148499, 0.25116279069767444]\n",
      "\n",
      "##########\n",
      "# Fold 1 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22918069149766648\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22619851997920445\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22413933915751322\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22164421209267207\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2195428822721754\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21632533414023264\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21375427288668497\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21115830540657043\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20630801575524466\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20150140992232732\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19529600654329574\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1890468363251005\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17977200235639298\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17167380239282334\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16267565744263784\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14945795493466513\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13775961526802608\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12318537597145353\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1112653421504157\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09637566868747983\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3468729108572006\n",
      "  accuracy:         0.6220657276995305\n",
      "  precision scores: [0.7952218430034129, 0.24060150375939848]\n",
      "  recall scores:    [0.6976047904191617, 0.34782608695652173]\n",
      "  F1 scores:        [0.7432216905901117, 0.28444444444444444]\n",
      "\n",
      "##########\n",
      "# Fold 2 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23266158146517618\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22749350113528116\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22637194182191575\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2251927320446287\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22159946603434427\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2203599682876042\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21671727086816514\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21363723703793117\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20965289643832616\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2047959715127945\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19810123954500472\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18942661157676152\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1807871929236821\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16869628642286574\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15665629718984878\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14348954600947245\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1285652560847146\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11467477892126356\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09975228245769228\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08598456531763077\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.37998226284980774\n",
      "  accuracy:         0.6220657276995305\n",
      "  precision scores: [0.7873754152823921, 0.224]\n",
      "  recall scores:    [0.7095808383233533, 0.30434782608695654]\n",
      "  F1 scores:        [0.7464566929133859, 0.25806451612903225]\n",
      "\n",
      "##########\n",
      "# Fold 3 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22853917734963552\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2274953510080065\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22623793355056218\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22420148977211543\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2232868948153087\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22016760919775283\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21909148352486746\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21707055824143545\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21229572807039535\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20837504310267313\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2027386554649898\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19698651986462729\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18915908890111105\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1816367847578866\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17253472123827254\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15863906698567526\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14616249288831437\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12875788978167943\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11248331942728587\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09812345675059728\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3484804481267929\n",
      "  accuracy:         0.6329411764705882\n",
      "  precision scores: [0.7920792079207921, 0.23770491803278687]\n",
      "  recall scores:    [0.7207207207207207, 0.31521739130434784]\n",
      "  F1 scores:        [0.7547169811320755, 0.2710280373831776]\n",
      "\n",
      "##########\n",
      "# Fold 4 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2302047120673316\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22674381307193212\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22577598903860366\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2254199023757662\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22281867691448756\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22123623107160842\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2176508264882224\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21429283916950226\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20989632393632615\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2057599425315857\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19916678965091705\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19053075356142862\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1818802718605314\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17137277339185988\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16118779991354262\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14858701825141907\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1371116808482579\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12126953367676054\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1065040145601545\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09214142603533608\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3735896646976471\n",
      "  accuracy:         0.5764705882352941\n",
      "  precision scores: [0.776173285198556, 0.20270270270270271]\n",
      "  recall scores:    [0.6456456456456456, 0.32608695652173914]\n",
      "  F1 scores:        [0.7049180327868854, 0.25000000000000006]\n",
      "\n",
      "##########\n",
      "# Fold 5 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23203960912568228\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22768949610846384\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22491508509431565\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22289198637008667\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22233899576323374\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21999715268611908\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21668274913515365\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21409282088279724\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20957469088690622\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2046480221407754\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19872169196605682\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19114552864006587\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18435796669551305\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17483295926025935\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1643255821296147\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15283191416944777\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13780511702810014\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12221506025109972\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10839808625834328\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09327226238591331\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.351744145154953\n",
      "  accuracy:         0.5872641509433962\n",
      "  precision scores: [0.7821428571428571, 0.20833333333333334]\n",
      "  recall scores:    [0.6576576576576577, 0.32967032967032966]\n",
      "  F1 scores:        [0.7145187601957587, 0.25531914893617025]\n",
      "\n",
      "##########\n",
      "# Fold 1 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22740181429045542\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22755814024380275\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22403964826038905\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22171329600470407\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22091555382524217\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21725102620465414\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2141898742743901\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2096766893352781\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20517476115907943\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19922201335430145\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1911270980324064\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18418108778340475\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17465366210256303\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16283990442752838\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1502166156257902\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13599825969764165\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12109698355197906\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10444512005363192\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09031656810215541\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07681870779820851\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.31691908836364746\n",
      "  accuracy:         0.568075117370892\n",
      "  precision scores: [0.8024193548387096, 0.24157303370786518]\n",
      "  recall scores:    [0.5958083832335329, 0.4673913043478261]\n",
      "  F1 scores:        [0.683848797250859, 0.3185185185185185]\n",
      "\n",
      "##########\n",
      "# Fold 2 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2296523771115712\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22680209151336125\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2247784627335412\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22215293773583003\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22147945421082632\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2174511445420129\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21497092928205216\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21149903535842896\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2053459414413997\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19862494511263712\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19061246940067836\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1810231591973986\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17309051326342992\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1598461355481829\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14547742903232574\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1304625336612974\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11666487689529147\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10174692635025297\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08645727911165782\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07238479490791049\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3543143570423126\n",
      "  accuracy:         0.6408450704225352\n",
      "  precision scores: [0.7854889589905363, 0.22018348623853212]\n",
      "  recall scores:    [0.7455089820359282, 0.2608695652173913]\n",
      "  F1 scores:        [0.7649769585253458, 0.23880597014925373]\n",
      "\n",
      "##########\n",
      "# Fold 3 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22846603393554688\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22290701951299394\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22077858235154832\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21947007519858225\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21451168826648168\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2135720636163439\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2079268800360816\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2030243149825505\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19745208535875594\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1905458356652941\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1833070686885289\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1748323142528534\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16503075190952846\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1536626453910555\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.13951608219317027\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12594867391245707\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11131544411182404\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09655340335198812\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08329675133739199\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.06940191664866038\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.3503356873989105\n",
      "  accuracy:         0.611764705882353\n",
      "  precision scores: [0.7937062937062938, 0.23741007194244604]\n",
      "  recall scores:    [0.6816816816816816, 0.358695652173913]\n",
      "  F1 scores:        [0.7334410339256866, 0.2857142857142857]\n",
      "\n",
      "##########\n",
      "# Fold 4 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22735224877085006\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2244881966284343\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2221574251140867\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21985118516853877\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21772007005555288\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21485278010368347\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21275848576000758\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2074805817433766\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20062582620552608\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1977192610502243\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18851677009037562\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17978051091943467\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.17052503568785532\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15891866173063005\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14619301472391402\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1327235283596175\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11875813454389572\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10688724155936923\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09011638590267726\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07698906532355718\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.36490195989608765\n",
      "  accuracy:         0.6305882352941177\n",
      "  precision scores: [0.7933333333333333, 0.24]\n",
      "  recall scores:    [0.7147147147147147, 0.32608695652173914]\n",
      "  F1 scores:        [0.7519747235387046, 0.2764976958525346]\n",
      "\n",
      "##########\n",
      "# Fold 5 #\n",
      "##########\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23219058981963567\n",
      "training epoch 2/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22782086900302342\n",
      "training epoch 3/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.224426092846053\n",
      "training epoch 4/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22184561405863082\n",
      "training epoch 5/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.22029210839952743\n",
      "training epoch 6/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21764647960662842\n",
      "training epoch 7/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.21407925869737351\n",
      "training epoch 8/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2103277231965746\n",
      "training epoch 9/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20624759367534093\n",
      "training epoch 10/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.20016920140811376\n",
      "training epoch 11/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1946079432964325\n",
      "training epoch 12/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.18513031942503794\n",
      "training epoch 13/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.175749129482678\n",
      "training epoch 14/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16424929244177683\n",
      "training epoch 15/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15412195452622005\n",
      "training epoch 16/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14065658833299363\n",
      "training epoch 17/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12510812069688523\n",
      "training epoch 18/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10900512231247765\n",
      "training epoch 19/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09436629073960441\n",
      "training epoch 20/20...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08015654129641396\n",
      "evaluating model...\n",
      "  evaluating batch 2/2  average loss: 0.33749470114707947\n",
      "  accuracy:         0.6179245283018868\n",
      "  precision scores: [0.7938144329896907, 0.23308270676691728]\n",
      "  recall scores:    [0.6936936936936937, 0.34065934065934067]\n",
      "  F1 scores:        [0.7403846153846154, 0.27678571428571425]\n",
      "\n",
      "\n",
      "BEST MODEL:\n",
      "  type:          LSTMClassifier\n",
      "  bidirectional: True\n",
      "  fold:          2\n",
      "  average F1:    0.5291924109093179\n"
     ]
    }
   ],
   "source": [
    "### TODO: 2.3 Compare the results of various RNN variants (classification metrics)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "MODEL_FACTORIES = [\n",
    "    lambda: LSTMClassifier(len(word2idx), 2, EMBEDDING_DIM, HIDDEN_LAYER_SIZE, HIDDEN_LAYER_COUNT, False, device=torch.device(DEVICE)),\n",
    "    lambda: LSTMClassifier(len(word2idx), 2, EMBEDDING_DIM, HIDDEN_LAYER_SIZE, HIDDEN_LAYER_COUNT, True, device=torch.device(DEVICE)),\n",
    "    lambda: GRUClassifier(len(word2idx), 2, EMBEDDING_DIM, HIDDEN_LAYER_SIZE, HIDDEN_LAYER_COUNT, False, device=torch.device(DEVICE)),\n",
    "    lambda: GRUClassifier(len(word2idx), 2, EMBEDDING_DIM, HIDDEN_LAYER_SIZE, HIDDEN_LAYER_COUNT, True, device=torch.device(DEVICE))\n",
    "]\n",
    "\n",
    "for model_factory in MODEL_FACTORIES:\n",
    "    for i in range(NUM_FOLDS):\n",
    "        train_dataset = ThesesDataset(\n",
    "            [tokenized_data[j] for j in fold_idcs_train[i]],\n",
    "            [degrees[j] for j in fold_idcs_train[i]],\n",
    "            word2idx,\n",
    "            [\"Bachelor\", \"Master\"]\n",
    "        )\n",
    "        train_dataloader = train_dataset.loader(BATCH_SIZE)\n",
    "\n",
    "        test_dataset = ThesesDataset(\n",
    "            [tokenized_data[j] for j in fold_idcs_test[i]],\n",
    "            [degrees[j] for j in fold_idcs_test[i]],\n",
    "            word2idx,\n",
    "            [\"Bachelor\", \"Master\"]\n",
    "        )\n",
    "        test_dataloader = test_dataset.loader(BATCH_SIZE)\n",
    "\n",
    "        model = model_factory()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(torch.tensor([0.2,0.8], device=model.device))\n",
    "\n",
    "        optimizer = Adam(model.parameters())\n",
    "\n",
    "        print(\"##########\")\n",
    "        print(f\"# Fold {i+1} #\")\n",
    "        print(\"##########\\n\")\n",
    "\n",
    "        for e in range(EPOCHS):\n",
    "            print(f\"training epoch {e+1}/{EPOCHS}...\")\n",
    "            train(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "        print(\"evaluating model...\")\n",
    "        f1 = eval(model, test_dataloader, criterion)\n",
    "        if f1 > best_model_f1:\n",
    "            print(\"new best model found!\")\n",
    "            best_model_f1 = f1\n",
    "            best_model_fold = i\n",
    "            best_model = model\n",
    "            best_model_class = best_model.__class__.__name__\n",
    "            best_model_bidirectional = model.bidirectional\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(\"\\nBEST MODEL:\")\n",
    "print(f\"  type:          {best_model_class}\")\n",
    "print(f\"  bidirectional: {best_model_bidirectional}\")\n",
    "print(f\"  fold:          {best_model_fold}\")\n",
    "print(f\"  average F1:    {best_model_f1}\")\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "db9b733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632 diploma theses classified as bachelors thesis, 224 diploma theses classified as masters thesis\n"
     ]
    }
   ],
   "source": [
    "### TODO: 2.4 (Optional) Apply your best classifier to the diploma theses\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "diploma_dataset = ThesesDataset([tokenized_data[i] for i in diploma_indices], [degrees[i] for i in diploma_indices], word2idx, [\"Diplom\"])\n",
    "diploma_dataloader = diploma_dataset.loader(2048)\n",
    "with torch.no_grad():\n",
    "    results = torch.argmax(torch.vstack([best_model(x.to(best_model.device)) for (x, _) in diploma_dataloader]), 1).tolist()\n",
    "bachelor_count = results.count(0)\n",
    "master_count = results.count(1)\n",
    "print(f\"{bachelor_count} diploma theses classified as bachelors thesis, {master_count} diploma theses classified as masters thesis\")\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
