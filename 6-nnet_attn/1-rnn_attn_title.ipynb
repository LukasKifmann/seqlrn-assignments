{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 6: Attention (please!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Thesis Title Classification\n",
    "\n",
    "In this assignment, we'll again rely on the theses dataset and want to classify whether a thesis is bachelor or master.\n",
    "Update your B.Sc. / M.Sc. thesis title classification model from the previous assignment and integrate the attention mechanism.\n",
    "Therefore, implement the `dot product attention` and check how it affects the training and performance for this task.\n",
    "In case you want to start fresh, we provide some boiler plate code of a base RNN classification model as well as ready-to-go data loading.\n",
    "The basic setup as well as some code and steps can be reused from your solution for the RNN tasks.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sklearn_metrics\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Iterator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Filter out all diploma theses; they might be too easy to spot because they only cover \"old\" topics.\n",
    "\n",
    "1.4 Create a PyTorch Dataset class which handles your tokenized data with respect to input and (class) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theses_dataset(filepath):\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return pd.read_csv(filepath, header=0, sep=\";\")\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    yield \"<s>\"\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^(\\w+)?([,\\.?!])?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is not None:\n",
    "                yield m.group(1).lower()\n",
    "            if m.group(2) is not None:\n",
    "                yield m.group(2)\n",
    "    yield \"</s>\"\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    def _preprocss_fn(text):\n",
    "        remove_digits = str.maketrans(string.digits, ' '*len(string.digits))\n",
    "        remove_pun = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        text = text.translate(remove_digits)\n",
    "        text = text.translate(remove_pun)\n",
    "        text = re.sub(' {2,}', ' ', text)\n",
    "        return text.lower()\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # Remove punctuation, digits and lowercase titles\n",
    "    dataframe[\"Titel\"] = dataframe[\"Titel\"].apply(lambda s: _preprocss_fn(s))\n",
    "\n",
    "    # Filter out empty and short titles\n",
    "    dataframe = dataframe[dataframe[\"Titel\"].str.len() > 4]\n",
    "\n",
    "    # Reset index of dataframe\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # Simple tokenization of titles\n",
    "    dataframe[\"tokenized\"] = [list(tokenize(title)) for title in dataframe[\"Titel\"].values]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num theses (overall): 2982\n",
      "Num theses (w/o diplom): 2126\n",
      "Num theses (diplom): 856\n",
      "\n",
      "Grad\n",
      "Bachelor    1667\n",
      "Diplom       856\n",
      "Master       459\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "dataframe_all = load_theses_dataset(\"data/theses2022.csv\")\n",
    "dataframe_all = dataframe_all[dataframe_all[\"Sprache\"] == \"DE\"]\n",
    "dataframe_all = preprocess(dataframe_all)\n",
    "\n",
    "# Convert labels to integer\n",
    "LABEL2IDX = {\"Bachelor\": 0, \"Master\": 1, \"Diplom\": 2}\n",
    "dataframe_all[\"label\"] = dataframe_all[\"Grad\"].apply(lambda l: LABEL2IDX[l])\n",
    "\n",
    "# Filter out `Diplom`\n",
    "dataframe_diplom = dataframe_all[dataframe_all[\"Grad\"] == \"Diplom\"]\n",
    "dataframe = dataframe_all[dataframe_all[\"Grad\"] != \"Diplom\"]\n",
    "\n",
    "# Check number of samples and label distribution\n",
    "print(f\"Num theses (overall): {len(dataframe_all)}\")\n",
    "print(f\"Num theses (w/o diplom): {len(dataframe)}\")\n",
    "print(f\"Num theses (diplom): {len(dataframe_diplom)}\")\n",
    "print()\n",
    "print(dataframe_all[\"Grad\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2b4a9bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7951\n"
     ]
    }
   ],
   "source": [
    "### Notice: Think about padding tokens for batch sizes > 1\n",
    "\n",
    "vocab = set()\n",
    "vocab.add(\"<pad>\")\n",
    "\n",
    "# For a more realistic application, we have to deal with unknown tokens\n",
    "# that were not present in the training corpus. However, for the sake\n",
    "# of clarity, we add all possible tokens from our dataset.\n",
    "# vocab.add(\"<unk>\")\n",
    "\n",
    "# Prepare vocabulary\n",
    "for s in dataframe_all.tokenized:\n",
    "    vocab.update(s)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(sorted(vocab))}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(sorted(vocab))}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch dataset for our thesis classification task\n",
    "\n",
    "class ThesisClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, labels, word2idx):\n",
    "        self.data, self.labels = [], []\n",
    "        for tokens, label in zip(dataset, labels):\n",
    "            # Create inputs; map tokens to ids\n",
    "            self.data.append(torch.stack([\n",
    "                torch.tensor(word2idx[w], dtype=torch.long) for w in tokens\n",
    "            ]))\n",
    "\n",
    "            # Create labels; already an integer\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns one input and label sample\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement the dot product attention mechanism and integrate it into your RNN classification model.\n",
    "\n",
    "2.2 Train and evaluate your models with a train-test-split (or optional 5-fold cross-validation).\n",
    "\n",
    "2.3 Assemble a table: Recall/Precision/F1 measure for RNN classification with and without attention. Do your results improve w.r.t. your old model?\n",
    "\n",
    "2.4 Can you find certain words that receive high attention weights regarding the decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement RNN classifier (nn.Module)\n",
    "### Notice: Think about padding for batch sizes > 1\n",
    "### Notice: 'torch.nn.utils.rnn' provides functionality\n",
    "### Notice: Here you can integrate the attention mechanism\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, return_weights: bool = False) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]: \n",
    "    scores = torch.where(Q.isnan(), 0, Q) @ torch.where(K.isnan(), 0, Q).mT / sqrt(Q.shape[1])\n",
    "    score_mask = ~K[:, :, 0:1].mT.isnan().repeat(1, scores.shape[1], 1)\n",
    "    weight_mask = ~Q[:, :, 0:1].isnan().repeat(1, 1, scores.shape[2])\n",
    "    weights = torch.where(\n",
    "        weight_mask,\n",
    "        F.softmax(torch.where(score_mask, scores, -torch.inf), 2),\n",
    "        0\n",
    "    )\n",
    "    result = weights @ torch.where(V.isnan(), 0, V)\n",
    "    if return_weights:\n",
    "        return result, weights\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence, PackedSequence\n",
    "\n",
    "class GRU_Classifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, num_classes,\n",
    "                 with_attention=False):\n",
    "        super(GRU_Classifier, self).__init__()\n",
    "        self.with_attention = with_attention\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_embeddings, \n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=False,\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, X, lengths, hidden=None):\n",
    "        embeddings = self.embedding(X)\n",
    "\n",
    "        # Packed squence helps avoid unneccsary computation\n",
    "        packed_seq = pack_padded_sequence(embeddings, lengths)\n",
    "\n",
    "        outputs, hidden_states = self.rnn(packed_seq , hidden)\n",
    "\n",
    "        # If tuple (h_n, c_n) containts cell state c_n then select h_n\n",
    "        if isinstance(hidden_states, tuple):\n",
    "            hidden = hidden_states[0]\n",
    "        else:\n",
    "            hidden = hidden_states\n",
    "\n",
    "        if self.with_attention:\n",
    "            H = self.get_attention_input(outputs)\n",
    "            attention_out, weights = scaled_dot_product_attention(H, H, H, True)\n",
    "            clf_input = attention_out[:, 0, :]\n",
    "        else:\n",
    "            clf_input, weights = hidden, None\n",
    "\n",
    "        # Apply classifier with hidden states\n",
    "        logits = self.fc(clf_input.squeeze(0))\n",
    "        if len(logits.shape) == 1:\n",
    "            logits = logits.reshape(1, -1)\n",
    "\n",
    "        return logits, hidden_states, weights\n",
    "    \n",
    "    def get_attention_input(self, X: PackedSequence) -> torch.Tensor:\n",
    "        result = torch.full((int(X.batch_sizes[0]), X.batch_sizes.shape[0], X.data.shape[1]), torch.nan, dtype=X.data.dtype, device=X.data.device)\n",
    "        start = 0\n",
    "        for i, size in enumerate(X.batch_sizes):\n",
    "            stop = start + size\n",
    "            result[:size, i, :] = X.data[start:stop, :]\n",
    "            start = stop\n",
    "        return result\n",
    "\n",
    "\n",
    "class SequencePadder():\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        labels = [x[1] for x in sorted_batch]\n",
    "        padded = pad_sequence(sequences, padding_value=self.symbol)\n",
    "        lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "        return padded, torch.LongTensor(labels), lengths\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def train(model: nn.Module, dataloader: DataLoader, criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optimizer: optim.Optimizer, device: torch.device):\n",
    "    batch_count = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    for i, (X, Y, lengths) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        print(f\"\\r  training batch {i+1}/{batch_count}\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        logits, _, _ = model(X, lengths)\n",
    "        loss = criterion(logits, Y)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    print(f\"  average loss: {running_loss/batch_count}\")\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the evaluation functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def eval(model: nn.Module, dataloader: DataLoader, criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], device: torch.device, return_attn_dict=False):\n",
    "    batch_count = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    attention_scores = []\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y, lengths) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            print(f\"\\r  evaluating batch {i+1}/{batch_count}\", end=\"\")\n",
    "            logits, _, weights = model(X, lengths)\n",
    "            running_loss += criterion(logits, Y).item()\n",
    "            for y in torch.argmax(logits, 1).tolist():\n",
    "                Y_pred.append(y)\n",
    "            for y in Y.tolist():\n",
    "                Y_true.append(y)\n",
    "            if return_attn_dict: attention_scores.append(weights.to(\"cpu\"))\n",
    "    C = sklearn_metrics.confusion_matrix(Y_true, Y_pred)\n",
    "    acc = (C.diagonal().sum() / C.sum())\n",
    "    prec = (C.diagonal() / (C.sum(0) + 1e-128))\n",
    "    rec = (C.diagonal() / (C.sum(1) + 1e-128))\n",
    "    f1 = 2 / ((1 / (prec + 1e-128)) + (1 / (rec + 1e-128)) + 1e-128)\n",
    "    print(f\"  average loss:     {running_loss/batch_count}\")\n",
    "    print(f\"  accuracy:         {acc.item()}\")\n",
    "    print(f\"  precision scores: {prec.tolist()}\")\n",
    "    print(f\"  recall scores:    {rec.tolist()}\")\n",
    "    print(f\"  F1 scores:        {f1.tolist()}\")\n",
    "    if return_attn_dict:\n",
    "        return attention_scores\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "# Model without attention #\n",
      "###########################\n",
      "\n",
      "training epoch 1/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.7023503524916512\n",
      "training epoch 2/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6915952392986843\n",
      "training epoch 3/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6864392927714756\n",
      "training epoch 4/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.682222740990775\n",
      "training epoch 5/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6764605896813529\n",
      "training epoch 6/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.669379711151123\n",
      "training epoch 7/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6606558987072536\n",
      "training epoch 8/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6493268864495414\n",
      "training epoch 9/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.633919222014291\n",
      "training epoch 10/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6135083947862897\n",
      "training epoch 11/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.587559883083616\n",
      "training epoch 12/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.5553031180586133\n",
      "training epoch 13/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.5163071709019798\n",
      "training epoch 14/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.4718383082321712\n",
      "training epoch 15/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.42432373975004467\n",
      "training epoch 16/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.3762269360678537\n",
      "training epoch 17/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.3275801379765783\n",
      "training epoch 18/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.27915743738412857\n",
      "training epoch 19/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.23314547751631057\n",
      "training epoch 20/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.1908851164792265\n",
      "training epoch 21/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.15288234395640238\n",
      "training epoch 22/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.11966035887598991\n",
      "training epoch 23/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.09185672657830375\n",
      "training epoch 24/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.06974776514938899\n",
      "training epoch 25/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.052942781443042417\n",
      "training epoch 26/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.04070254149181502\n",
      "training epoch 27/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.031492418609559536\n",
      "training epoch 28/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.024438729310142144\n",
      "training epoch 29/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.01940078813848751\n",
      "training epoch 30/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.01542789482378534\n",
      "evaluating model...\n",
      "  evaluating batch 3/3  average loss:     2.204897324244181\n",
      "  accuracy:         0.6428571428571429\n",
      "  precision scores: [0.7965260545905707, 0.16279069767441862]\n",
      "  recall scores:    [0.7482517482517482, 0.20388349514563106]\n",
      "  F1 scores:        [0.7716346153846153, 0.1810344827586207]\n",
      "\n",
      "########################\n",
      "# Model with attention #\n",
      "########################\n",
      "\n",
      "training epoch 1/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6975874134472438\n",
      "training epoch 2/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6898829851831708\n",
      "training epoch 3/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6841988137790135\n",
      "training epoch 4/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6788548231124878\n",
      "training epoch 5/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6731083478246417\n",
      "training epoch 6/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6664502620697021\n",
      "training epoch 7/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6584498115948269\n",
      "training epoch 8/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6486243435314724\n",
      "training epoch 9/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6364246436527797\n",
      "training epoch 10/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.6211656161717006\n",
      "training epoch 11/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.602045612675803\n",
      "training epoch 12/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.5783285541193826\n",
      "training epoch 13/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.5498184987476894\n",
      "training epoch 14/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.5170982948371342\n",
      "training epoch 15/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.4814418043409075\n",
      "training epoch 16/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.4440109005996159\n",
      "training epoch 17/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.40487087198666166\n",
      "training epoch 18/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.36529976555279325\n",
      "training epoch 19/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.32716383252825054\n",
      "training epoch 20/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2906076003398214\n",
      "training epoch 21/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.25553806658302036\n",
      "training epoch 22/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.2225959300994873\n",
      "training epoch 23/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.19248086107628687\n",
      "training epoch 24/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.16582502265061652\n",
      "training epoch 25/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.14257502502628736\n",
      "training epoch 26/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.12161825490849358\n",
      "training epoch 27/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.10372871745909963\n",
      "training epoch 28/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.08818705220307622\n",
      "training epoch 29/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.07400387392512389\n",
      "training epoch 30/30...\n",
      "  training batch 7/7\n",
      "  average loss: 0.062171896387423785\n",
      "evaluating model...\n",
      "  evaluating batch 3/3  average loss:     2.1062244971593223\n",
      "  accuracy:         0.631578947368421\n",
      "  precision scores: [0.8140161725067385, 0.2111801242236025]\n",
      "  recall scores:    [0.703962703962704, 0.3300970873786408]\n",
      "  F1 scores:        [0.7549999999999999, 0.25757575757575757]\n"
     ]
    }
   ],
   "source": [
    "### TODO: 2.3 Initialize and train the RNN Classification Model for X epochs + Evaluation\n",
    "\n",
    "# Training parameters\n",
    "SEED = 42\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "DEVICE = \"cuda:0\" # 'cpu', 'mps' or 'cuda'\n",
    "LABEL_COL = \"label\"\n",
    "PAD_IDX = word2idx[\"<pad>\"]\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_LAYER_SIZE = 32\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "data_samples = dataframe[\"tokenized\"].values\n",
    "data_labels = dataframe[LABEL_COL].values\n",
    "train_idcs, test_idcs = train_test_split(list(range(len(data_samples))), random_state=SEED)\n",
    "dataset_train = ThesisClassificationDataset([data_samples[i] for i in train_idcs], [data_labels[i] for i in train_idcs], word2idx=word2idx)\n",
    "dataset_test = ThesisClassificationDataset([data_samples[i] for i in test_idcs], [data_labels[i] for i in test_idcs], word2idx=word2idx)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, collate_fn=SequencePadder(PAD_IDX))\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, collate_fn=SequencePadder(PAD_IDX))\n",
    "\n",
    "print(\"###########################\")\n",
    "print(\"# Model without attention #\")\n",
    "print(\"###########################\\n\")\n",
    "\n",
    "model_without_attention = GRU_Classifier(vocab_size, EMBEDDING_DIM, HIDDEN_LAYER_SIZE, 2).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(torch.tensor([0.2, 0.8])).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model_without_attention.parameters(), LEARNING_RATE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"training epoch {e+1}/{EPOCHS}...\")\n",
    "    train(model_without_attention, dataloader_train, criterion, optimizer, torch.device(DEVICE))\n",
    "\n",
    "print(\"evaluating model...\")\n",
    "eval(model_without_attention, dataloader_test, criterion, torch.device(DEVICE))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"########################\")\n",
    "print(\"# Model with attention #\")\n",
    "print(\"########################\\n\")\n",
    "\n",
    "model_with_attention = GRU_Classifier(vocab_size, EMBEDDING_DIM, HIDDEN_LAYER_SIZE, 2, True).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(torch.tensor([0.2, 0.8])).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model_with_attention.parameters(), LEARNING_RATE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"training epoch {e+1}/{EPOCHS}...\")\n",
    "    train(model_with_attention, dataloader_train, criterion, optimizer, torch.device(DEVICE))\n",
    "\n",
    "print(\"evaluating model...\")\n",
    "attention_scores: list[torch.Tensor] = eval(model_with_attention, dataloader_test, criterion, torch.device(DEVICE), True) # type: ignore\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2f719873",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.4 Visualize the attention weights\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
