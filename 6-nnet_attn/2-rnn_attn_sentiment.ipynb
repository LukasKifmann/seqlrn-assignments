{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 6: Attention (please!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2) Sentiment Analysis\n",
    "\n",
    "In this task, we'll use the kaggle Rotten Tomatoes Dataset for this exercise: [Source and Download instructions](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data).\n",
    "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset.\n",
    "The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order.\n",
    "Each sentence has been parsed into many phrases (chunks) using the Stanford parser.\n",
    "Each phrase has a `PhraseId`, each sentence a `SentenceId`.\n",
    "Phrases that are repeated (such as short/common words) are only included once in the data.\n",
    "\n",
    "### Data\n",
    "\n",
    "Rotten Tomatoes Dataset: `train.tsv` contains the phrases and their associated sentiment labels.\n",
    "We have additionally provided a `SentenceId` so that you can track which phrases belong to a single sentence.\n",
    "`test.tsv` contains just phrases; use your model to assign a sentiment label to each phrase.\n",
    "\n",
    "The sentiment labels are:\n",
    "\n",
    "* 0 - negative\n",
    "* 1 - somewhat negative\n",
    "* 2 - neutral\n",
    "* 3 - somewhat positive\n",
    "* 4 - positive\n",
    "\n",
    "### GloVe Word Embeddings\n",
    "\n",
    "Use GloVe word embeddings for your `nn.Embedding` layer, there is a number of pretrained models for English available in the `torchtext` module.\n",
    "You are free to  use any kind of attention and architecture you like.\n",
    "Just remember that the basic form for attention based networks is always and encoder / Decoder architecture.\n",
    "Use `torchtext.vocab.GloVe` to get started quickly with the word embeddings.\n",
    "\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sklearn_metrics\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from math import sqrt\n",
    "from typing import Iterator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import PackedSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 As always: conduct some data preprocessing.\n",
    "\n",
    "1.2 Download and prepare the GloVe word embeddings. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Create a PyTorch Dataset class which handles your tokenized data with respect to input and (class) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_dataset(filepath):\n",
    "    \"\"\"Loads all phrase instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return pd.read_csv(filepath, header=0, sep='\\t')\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    yield \"<s>\"\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^(\\w+)?([,\\.?!])?$\", s)\n",
    "        if m is not None:\n",
    "            if m.group(1) is not None:\n",
    "                yield m.group(1).lower()\n",
    "            if m.group(2) is not None:\n",
    "                yield m.group(2)\n",
    "    yield \"</s>\"\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    def _preprocss_fn(text):\n",
    "        remove_pun = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        remove_digits = str.maketrans(string.digits, ' '*len(string.digits))\n",
    "        text = text.translate(remove_digits)\n",
    "        text = text.translate(remove_pun)\n",
    "        text = re.sub(' {2,}', ' ', text)\n",
    "        return text.lower()\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # Remove punctuation, digits and lowercase phrases\n",
    "    dataframe[\"Phrase\"] = dataframe[\"Phrase\"].apply(lambda s: _preprocss_fn(s))\n",
    "\n",
    "    # Filter out empty phrases\n",
    "    dataframe = dataframe[dataframe[\"Phrase\"].str.len() > 1]\n",
    "\n",
    "    # Reset index of dataframe\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # Simple tokenization of phrases\n",
    "    dataframe[\"tokenized\"] = [list(tokenize(title)) for title in dataframe[\"Phrase\"].values]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train dataset: 155880\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "train_dataframe = load_sentiment_dataset(\"data/rotten_tomatoes_train.tsv\")\n",
    "train_dataframe = preprocess(train_dataframe)\n",
    "\n",
    "# Map for formatting labels\n",
    "IDX2SENTIMENT = {0: \"negative\", 1: \"somewhat negative\", 2: \"neutral\",\n",
    "                 3: \"somewhat positive\", 4: \"positive\"}\n",
    "\n",
    "# Test labels not available and submission to Kaggle required\n",
    "# test_dataframe = load_sentiment_dataset(\"data/rotten_tomatoes_test.tsv\")\n",
    "# test_dataframe = preprocess(test_dataframe)\n",
    "\n",
    "print(f\"Num train dataset: {len(train_dataframe)}\")\n",
    "# print(f\"Num test dataset: {len(test_dataframe)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dc6c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download the pre-trained (english) GloVe embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Prepare glove embeddings\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "\n",
    "def append_special(glove, special, vec=None):\n",
    "    glove.itos.append(special)\n",
    "    glove.stoi[special] = glove.itos.index(special)\n",
    "    if vec is None:\n",
    "        vec = torch.zeros(1, glove.vectors.size(1))\n",
    "    glove.vectors = torch.cat((glove.vectors, vec))\n",
    "    return glove\n",
    "\n",
    "glove = GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "# We need to add some special tokens\n",
    "glove = append_special(glove, UNK_TOKEN)\n",
    "glove = append_special(glove, PAD_TOKEN)\n",
    "glove = append_special(glove, START_TOKEN)\n",
    "glove = append_special(glove, END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RottenTomatoesDataset(Dataset):\n",
    "    def __init__(self, dataset, labels, glove, unk=\"<unk>\"):\n",
    "        self.data, self.labels = [], []\n",
    "        for tokens, label in zip(dataset, labels):\n",
    "            # Create inputs; map tokens to ids\n",
    "            self.data.append(torch.stack([\n",
    "                torch.tensor(glove.stoi.get(w, glove.stoi.get(unk)), dtype=torch.long) for w in tokens\n",
    "            ]))\n",
    "\n",
    "            # Create labels; already an integer\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns one input and label sample\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement and reuse your RNN-based classifciation models for the sentiment classification task. \n",
    "\n",
    "2.2 Train and evaluate your models by performing a train-test-split on the `train.tsv` file.\n",
    "\n",
    "2.3 Check and compare your classification results with some publicly available baselines (there are plenty of on the internet).\n",
    "\n",
    "2.4 Visualize the attention weights for the words and pick some nice samples for each sentiment category!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement RNN classifier (nn.Module)\n",
    "### Notice: Think about padding for batch sizes > 1\n",
    "### Notice: 'torch.nn.utils.rnn' provides functionality\n",
    "### Notice: Here you can integrate the attention mechanism\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, return_weights: bool = False) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]: \n",
    "    scores = torch.where(Q.isnan(), 0, Q) @ torch.where(K.isnan(), 0, Q).mT / sqrt(Q.shape[1])\n",
    "    score_mask = ~K[:, :, 0:1].mT.isnan().repeat(1, scores.shape[1], 1)\n",
    "    weight_mask = ~Q[:, :, 0:1].isnan().repeat(1, 1, scores.shape[2])\n",
    "    weights = torch.where(\n",
    "        weight_mask,\n",
    "        F.softmax(torch.where(score_mask, scores, -torch.inf), 2),\n",
    "        0\n",
    "    )\n",
    "    result = weights @ torch.where(V.isnan(), 0, V)\n",
    "    if return_weights:\n",
    "        return result, weights\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "class GRU_Classifier(nn.Module):\n",
    "    def __init__(self, glove: GloVe, hidden_dim: int, num_classes: int, with_attention: bool = False):\n",
    "        super(GRU_Classifier, self).__init__()\n",
    "        self.with_attention = with_attention\n",
    "\n",
    "        # TODO: add glove embeddings\n",
    "        self.embedding = nn.Embedding(len(glove), glove.dim, _weight=glove.vectors) # type: ignore\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=glove.dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=False,\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, X, lengths, hidden=None):\n",
    "        embeddings = self.embedding(X)\n",
    "\n",
    "        # Packed squence helps avoid unneccsary computation\n",
    "        padded_seq = pack_padded_sequence(embeddings, lengths)\n",
    "\n",
    "        outputs, hidden_states = self.rnn(padded_seq, hidden)\n",
    "\n",
    "        # If tuple (h_n, c_n) containts cell state c_n then select h_n\n",
    "        if isinstance(hidden_states, tuple):\n",
    "            hidden = hidden_states[0]\n",
    "        else:\n",
    "            hidden = hidden_states\n",
    "\n",
    "        if self.with_attention:\n",
    "            H = self.get_attention_input(outputs)\n",
    "            attention_out, weights = scaled_dot_product_attention(H, H, H, True)\n",
    "            clf_input = attention_out[:, 0, :]\n",
    "        else:\n",
    "            clf_input, weights = hidden, None\n",
    "\n",
    "        # Apply classifier with hidden states\n",
    "        logits = self.fc(clf_input.squeeze(0))\n",
    "\n",
    "        return logits, hidden_states, weights\n",
    "    \n",
    "    def get_attention_input(self, X: PackedSequence) -> torch.Tensor:\n",
    "        result = torch.full((int(X.batch_sizes[0]), X.batch_sizes.shape[0], X.data.shape[1]), torch.nan, dtype=X.data.dtype, device=X.data.device)\n",
    "        start = 0\n",
    "        for i, size in enumerate(X.batch_sizes):\n",
    "            stop = start + size\n",
    "            result[:size, i, :] = X.data[start:stop, :]\n",
    "            start = stop\n",
    "        return result\n",
    "\n",
    "\n",
    "class SequencePadder():\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        labels = [x[1] for x in sorted_batch]\n",
    "        padded = pad_sequence(sequences, padding_value=self.symbol)\n",
    "        lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "        return padded, torch.LongTensor(labels), lengths\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    batch_count = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    for i, (X, Y, lengths) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        print(f\"\\r  training batch {i+1}/{batch_count}\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        logits, _, _ = model(X, lengths)\n",
    "        loss = criterion(logits, Y)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    print(f\"  average loss: {running_loss/batch_count}\")\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the evaluation functionality\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def eval(model, dataloader, criterion, device, return_attn_dict=False):\n",
    "    batch_count = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    attention_scores = []\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y, lengths) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            print(f\"\\r  evaluating batch {i+1}/{batch_count}\", end=\"\")\n",
    "            logits, _, weights = model(X, lengths)\n",
    "            running_loss += criterion(logits, Y).item()\n",
    "            for y in torch.argmax(logits, 1).tolist():\n",
    "                Y_pred.append(y)\n",
    "            for y in Y.tolist():\n",
    "                Y_true.append(y)\n",
    "            if return_attn_dict: attention_scores.append(weights.to(\"cpu\"))\n",
    "    C = sklearn_metrics.confusion_matrix(Y_true, Y_pred)\n",
    "    acc = (C.diagonal().sum() / C.sum())\n",
    "    prec = (C.diagonal() / (C.sum(0) + 1e-128))\n",
    "    rec = (C.diagonal() / (C.sum(1) + 1e-128))\n",
    "    f1 = 2 / ((1 / (prec + 1e-128)) + (1 / (rec + 1e-128)) + 1e-128)\n",
    "    print(f\"  average loss:     {running_loss/batch_count}\")\n",
    "    print(f\"  accuracy:         {acc.item()}\")\n",
    "    print(f\"  precision scores: {prec.tolist()}\")\n",
    "    print(f\"  recall scores:    {rec.tolist()}\")\n",
    "    print(f\"  F1 scores:        {f1.tolist()}\")\n",
    "    if return_attn_dict:\n",
    "        return attention_scores\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "# Model without attention #\n",
      "###########################\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 115/115\n",
      "  average loss: 1.104146601324496\n",
      "training epoch 2/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.9041635741358218\n",
      "training epoch 3/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.8292977136114369\n",
      "training epoch 4/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.7772068593813025\n",
      "training epoch 5/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.7393067696820135\n",
      "training epoch 6/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.7092604352080304\n",
      "training epoch 7/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6838595779045769\n",
      "training epoch 8/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6622593892657238\n",
      "training epoch 9/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.643453453934711\n",
      "training epoch 10/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6268586464550184\n",
      "training epoch 11/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6147339051184447\n",
      "training epoch 12/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6007806951585023\n",
      "training epoch 13/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5864780138368192\n",
      "training epoch 14/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5661739528179168\n",
      "training epoch 15/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5496914767700692\n",
      "training epoch 16/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5347332135490749\n",
      "training epoch 17/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5208043772241343\n",
      "training epoch 18/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5096045415038648\n",
      "training epoch 19/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.4973379856866339\n",
      "training epoch 20/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.4823629962361377\n",
      "evaluating model...\n",
      "  evaluating batch 39/39  average loss:     1.0625033623132951\n",
      "  accuracy:         0.658403900436233\n",
      "  precision scores: [0.49507389162561577, 0.5385471750626217, 0.7467625214244906, 0.5762289981331674, 0.5697445972495089]\n",
      "  recall scores:    [0.33095499451152577, 0.5647985989492119, 0.7890632860448737, 0.5683073523996564, 0.38309114927344784]\n",
      "  F1 scores:        [0.39671052631578946, 0.5513605926770195, 0.767330365442004, 0.5722407613397603, 0.4581358609794629]\n",
      "\n",
      "########################\n",
      "# Model with attention #\n",
      "########################\n",
      "\n",
      "training epoch 1/20...\n",
      "  training batch 115/115\n",
      "  average loss: 1.1338283347046894\n",
      "training epoch 2/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.9240404051283132\n",
      "training epoch 3/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.8434094330538874\n",
      "training epoch 4/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.7882080995518228\n",
      "training epoch 5/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.7469517816667971\n",
      "training epoch 6/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.7139860464178998\n",
      "training epoch 7/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6874865936196368\n",
      "training epoch 8/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6670158163360927\n",
      "training epoch 9/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6520298848981443\n",
      "training epoch 10/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6391740174397178\n",
      "training epoch 11/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6258467111898505\n",
      "training epoch 12/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6115193753138832\n",
      "training epoch 13/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.6009217897187109\n",
      "training epoch 14/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5933244549709817\n",
      "training epoch 15/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5800562184789907\n",
      "training epoch 16/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5661341667175293\n",
      "training epoch 17/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5536012396864269\n",
      "training epoch 18/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5446244265722192\n",
      "training epoch 19/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5372403398804043\n",
      "training epoch 20/20...\n",
      "  training batch 115/115\n",
      "  average loss: 0.5348809922518938\n",
      "evaluating model...\n",
      "  evaluating batch 39/39  average loss:     0.9596278239519168\n",
      "  accuracy:         0.6572235052604568\n",
      "  precision scores: [0.5021720243266724, 0.5386370116909039, 0.7523459417626004, 0.5699004057543342, 0.534034034034034]\n",
      "  recall scores:    [0.3172338090010977, 0.5513718622300058, 0.7824730858235235, 0.5689210752424205, 0.4698370761778952]\n",
      "  F1 scores:        [0.38883282879246556, 0.5449300447136881, 0.7671138291576248, 0.5694103194103195, 0.4998828765518856]\n"
     ]
    }
   ],
   "source": [
    "### TODO: 2.3 Initialize and train the RNN Classification Model for X epochs + Evaluation\n",
    "\n",
    "# Training parameters\n",
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "DEVICE = \"cuda:0\" # 'cpu', 'mps' or 'cuda'\n",
    "LABEL_COL = \"Sentiment\"\n",
    "PAD_IDX = glove.stoi[PAD_TOKEN] # type: ignore\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "### Notice: Data loading example\n",
    "data_samples = train_dataframe[\"tokenized\"].values\n",
    "data_labels = train_dataframe[LABEL_COL].values\n",
    "train_idcs, test_idcs = train_test_split(list(range(len(data_samples))), random_state=SEED)\n",
    "dataset_train = RottenTomatoesDataset([data_samples[i] for i in train_idcs], [data_labels[i] for i in train_idcs], glove)\n",
    "dataset_test = RottenTomatoesDataset([data_samples[i] for i in test_idcs], [data_labels[i] for i in test_idcs], glove)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, collate_fn=SequencePadder(PAD_IDX))\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, collate_fn=SequencePadder(PAD_IDX))\n",
    "\n",
    "print(\"###########################\")\n",
    "print(\"# Model without attention #\")\n",
    "print(\"###########################\\n\")\n",
    "\n",
    "model_without_attention = GRU_Classifier(glove, HIDDEN_LAYER_SIZE, len(IDX2SENTIMENT)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model_without_attention.parameters(), LEARNING_RATE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"training epoch {e+1}/{EPOCHS}...\")\n",
    "    train(model_without_attention, dataloader_train, criterion, optimizer, torch.device(DEVICE))\n",
    "\n",
    "print(\"evaluating model...\")\n",
    "eval(model_without_attention, dataloader_test, criterion, torch.device(DEVICE))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"########################\")\n",
    "print(\"# Model with attention #\")\n",
    "print(\"########################\\n\")\n",
    "\n",
    "model_with_attention = GRU_Classifier(glove, HIDDEN_LAYER_SIZE, len(IDX2SENTIMENT), True).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model_with_attention.parameters(), LEARNING_RATE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"training epoch {e+1}/{EPOCHS}...\")\n",
    "    train(model_with_attention, dataloader_train, criterion, optimizer, torch.device(DEVICE))\n",
    "\n",
    "print(\"evaluating model...\")\n",
    "attention_scores: list[torch.Tensor] = eval(model_with_attention, dataloader_test, criterion, torch.device(DEVICE), True) # type: ignore\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2696138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.4 Visualize the attention weights\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
