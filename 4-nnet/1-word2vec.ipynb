{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Skip-grams\n",
    "\n",
    "Tomas Mikolov's [original paper](https://arxiv.org/abs/1301.3781) for word2vec is not very specific on how to actually compute the embedding matrices.\n",
    "Xin Ron provides a much more detailed [walk-through](https://arxiv.org/pdf/1411.2738.pdf) of the math, I recommend you go through it before you continue with this assignment.\n",
    "Now, while the original implementation was in C and estimates the matrices directly, in this assignment, we want to use PyTorch (and autograd) to train the matrices.\n",
    "There are plenty of example implementations and blog posts out there that show how to do it, I particularly recommend [Mateusz Bednarski's](https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb) version. Familiarize yourself with skip-grams and how to train them using pytorch.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the upcoming assignments on Neural Networks, we'll be heavily using [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "If you're not already familiar with PyTorch, now's the time to get started with it.\n",
    "Head over to the [Basics](https://pytorch.org/tutorials/beginner/basics/intro.html) and gain some understanding about the essentials.\n",
    "Before starting this assignment, make sure you've got PyTorch installed in your working environment. \n",
    "It's a quick setup, and you'll find all the instructions you need on the PyTorch website.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "from typing import TypedDict, Iterator, Iterable, Optional\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from math import floor, ceil\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the initialization of the matrices and to map tokens to indices.\n",
    "\n",
    "1.3 Generate the training pairs with center word and context word. Which window size do you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"data/theses2022.csv\"\n",
    "\n",
    "@dataclass\n",
    "class Thesis:\n",
    "    registration_date: str\n",
    "    due_date: str\n",
    "    year_academic: int\n",
    "    type: str\n",
    "    degree: str\n",
    "    language: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "\n",
    "class _Thesis(TypedDict):\n",
    "    Anmeldedatum: str\n",
    "    Abgabedatum: str\n",
    "    JahrAkademisch: str\n",
    "    Art: str\n",
    "    Grad: str\n",
    "    Sprache: str\n",
    "    Titel: str\n",
    "    Abstract: str\n",
    "\n",
    "def to_thesis(thesis: _Thesis) -> Thesis:\n",
    "    return Thesis(\n",
    "        registration_date=thesis[\"Anmeldedatum\"],\n",
    "        due_date=thesis[\"Abgabedatum\"],\n",
    "        year_academic=int(thesis[\"JahrAkademisch\"]),\n",
    "        type=thesis[\"JahrAkademisch\"],\n",
    "        degree=thesis[\"Grad\"],\n",
    "        language=thesis[\"Sprache\"],\n",
    "        title=thesis[\"Titel\"],\n",
    "        abstract=thesis[\"Abstract\"]\n",
    "    )\n",
    "\n",
    "def load_theses_dataset(filepath) -> pd.DataFrame:\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    lists = {key: [] for key in Thesis.__dataclass_fields__.keys()}\n",
    "    with open(filepath, encoding=\"utf-8-sig\") as fp:\n",
    "        theses = map(to_thesis, csv.DictReader(fp.readlines(), delimiter=\";\")) # type: ignore\n",
    "        for thesis in theses:\n",
    "            for key in lists:\n",
    "                lists[key].append(thesis.__dict__[key])\n",
    "    return pd.DataFrame(lists)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "dataset = load_theses_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> Iterator[str]:\n",
    "    for s in text.split():\n",
    "        m = re.match(r\"^([@#]?\\w+)[,\\.?!]?$\", s)\n",
    "        if m is not None:\n",
    "            yield m.group(1)\n",
    "\n",
    "def preprocess(dataframe: pd.DataFrame) -> list[list[str]]:\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    l = []\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe[\"language\"][i] == \"DE\":\n",
    "            l.append(list(tokenize(dataframe[\"title\"][i])))\n",
    "            l.append(list(tokenize(dataframe[\"abstract\"][i])))\n",
    "    return l\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acde311",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_SAMPLING_RATIO = 3\n",
    "\n",
    "def word_frequencies(word2idx: dict[str, int], data: Iterable[str]) -> np.ndarray:\n",
    "    counts = np.zeros(len(word2idx), np.int32)\n",
    "    total = 0\n",
    "    for w in data:\n",
    "        counts[word2idx[w]] += 1\n",
    "        total += 1\n",
    "    return counts / total\n",
    "\n",
    "def draw_idx(word_frequencies: np.ndarray) -> int:\n",
    "    return np.random.choice(word_frequencies.shape[0], p=word_frequencies)\n",
    "\n",
    "def create_training_pairs(data: list[list[str]], word2idx: dict[str, int], window_size: int) -> list[tuple[int, list[int], list[int]]]:\n",
    "    \"\"\"Creates training pairs based on skip-grams for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    data = list(filter(lambda l: len(l) > 2 * window_size, data))\n",
    "    freqs = word_frequencies(word2idx, (w for l in data for w in l))\n",
    "    negative_count = NEGATIVE_SAMPLING_RATIO * window_size\n",
    "    result = []\n",
    "    for l in data:\n",
    "        for i in range(window_size, len(l) - window_size):\n",
    "            center = word2idx[l[i]]\n",
    "            positive = []\n",
    "            for j in range(i - window_size, i):\n",
    "                positive.append(word2idx[l[j]])\n",
    "            for j in range(i + 1, i + window_size + 1):\n",
    "                positive.append(word2idx[l[j]])\n",
    "            pos_set = set(positive)\n",
    "            neg_set = set()\n",
    "            while len(neg_set) < negative_count:\n",
    "                neg = draw_idx(freqs)\n",
    "                if not neg in pos_set:\n",
    "                    neg_set.add(neg)\n",
    "            result.append((center, positive, list(neg_set)))\n",
    "    return result\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = load_theses_dataset(DATASET_PATH)\n",
    "tokenized_data = preprocess(dataframe)\n",
    "vocabulary = {w for l in tokenized_data for w in l}\n",
    "word2idx = {w: i for i, w in enumerate(vocabulary)}\n",
    "idx2word = list(vocabulary)\n",
    "training_pairs = create_training_pairs(tokenized_data, word2idx, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Analyze\n",
    "\n",
    "2.1 Implement and train the word2vec model with your training data.\n",
    "\n",
    "2.2 Implement a method to find the top-k similar words for a given word (token).\n",
    "\n",
    "2.3 Analyze: What are the most similar words to \"Konzeption\", \"Cloud\" and \"virtuelle\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.1 Implement and train the word2vec model.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    x_in: torch.Tensor\n",
    "    x_out: torch.Tensor\n",
    "    p: torch.nn.Parameter\n",
    "    o: torch.nn.Parameter\n",
    "    y: torch.Tensor\n",
    "    p_idcs: list[int]\n",
    "    o_idcs: list[int]\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.x_in\n",
    "        del self.x_out\n",
    "        del self.p\n",
    "        del self.o\n",
    "        del self.y\n",
    "        del self.p_idcs\n",
    "        del self.o_idcs\n",
    "        \n",
    "\n",
    "class BatchAllocator(Iterable[Batch]):\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        return self.__batch_size\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.__device\n",
    "\n",
    "    def __init__(self, batch_size: int, data_list: list[tuple[int, list[int], list[int]]], device: torch.device):\n",
    "        self.__batch_size = batch_size\n",
    "        self.__data_list = data_list\n",
    "        self.__device = device\n",
    "        self.__samples_per_input = len(data_list[0][1]) + len(data_list[0][2])\n",
    "        self.P = torch.tensor(0)\n",
    "        self.O = torch.tensor(0)\n",
    "\n",
    "    def __iter__(self) -> Iterator[Batch]:\n",
    "        for idcs in self.__idcs():\n",
    "            yield self.__allocate_batch(idcs)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return ceil(len(self.__data_list) / self.batch_size)    \n",
    "\n",
    "    def __allocate_batch(self, idcs: tuple[int, int]) -> Batch:\n",
    "        i, j = idcs\n",
    "        size = j - i\n",
    "        p_idx_set = set()\n",
    "        o_idx_set = set()\n",
    "        for center, positive, negative in (self.__data_list[k] for k in range(i, j)):\n",
    "            p_idx_set.add(center)\n",
    "            for l in positive:\n",
    "                o_idx_set.add(l)\n",
    "            for l in negative:\n",
    "                o_idx_set.add(l)\n",
    "        p_idcs = list(p_idx_set)\n",
    "        o_idcs = list(o_idx_set)\n",
    "        x_in = torch.zeros((size, 1, len(p_idcs)), dtype=torch.float32, device=self.device)\n",
    "        p = torch.empty((1, len(p_idcs), self.P.shape[1]), dtype=torch.float32, device=self.device)\n",
    "        o = torch.empty((1, self.O.shape[1], len(o_idcs)), dtype=torch.float32, device=self.device)\n",
    "        x_out = torch.zeros((size, len(o_idcs), self.__samples_per_input), dtype=torch.float32, device=self.device)\n",
    "        y = torch.zeros((size, self.__samples_per_input), dtype=torch.float32, device=self.device)\n",
    "        for k in range(size):\n",
    "            center, positive, negative = self.__data_list[i + k]\n",
    "            x_in[k, 0, p_idcs.index(center)] = 1\n",
    "            for l in range(len(positive)):\n",
    "                x_out[k, o_idcs.index(positive[l]), l] = 1\n",
    "                y[k, l] = 1\n",
    "            for l in range(len(negative)):\n",
    "                x_out[k, o_idcs.index(negative[l]), l + len(positive)] = 1\n",
    "        for row, p_idx in enumerate(p_idcs):\n",
    "            p[0, row, :] = self.P[p_idx, :]\n",
    "        for col, o_idx in enumerate(o_idcs):\n",
    "            o[0, :, col] = self.O[o_idx, :]\n",
    "        return Batch(x_in, x_out, torch.nn.Parameter(p), torch.nn.Parameter(o), y, p_idcs, o_idcs) # type: ignore\n",
    "\n",
    "    def __idcs(self) -> Iterator[tuple[int, int]]:\n",
    "        i = 0\n",
    "        for _ in range(floor(len(self.__data_list) / self.batch_size)):\n",
    "            j = i + self.batch_size\n",
    "            yield i, j\n",
    "            i = j\n",
    "        if i < len(self.__data_list):\n",
    "            yield i, len(self.__data_list)\n",
    "\n",
    "def word2vec(voc_size: int, data_list: list[tuple[int, list[int], list[int]]], dim: int, batch_size: int, epochs: int, lr: float, device: Optional[torch.device] = None) -> torch.Tensor:\n",
    "    if device == None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    P = torch.rand((voc_size, dim))\n",
    "    O = torch.rand((voc_size, dim))\n",
    "    allocator = BatchAllocator(batch_size, data_list, device)\n",
    "    allocator.P = P\n",
    "    allocator.O = O\n",
    "    batch_count = len(allocator)\n",
    "    for e in range(1, epochs + 1):\n",
    "        print(f\"training epoch {e}/{epochs}...\")\n",
    "        running_loss = 0\n",
    "        for k, batch in enumerate(allocator):\n",
    "            print(f\"\\r    batch {k + 1}/{batch_count}\", end=\"\")\n",
    "            opt = torch.optim.SGD([batch.p, batch.o], lr)\n",
    "            opt.zero_grad()\n",
    "            logits = (batch.x_in @ batch.p @ batch.o @ batch.x_out).flatten(end_dim=1)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, batch.y)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                for i, j in enumerate(batch.p_idcs):\n",
    "                    P[j, :] = batch.p[0, i, :]\n",
    "                for i, j in enumerate(batch.o_idcs):\n",
    "                    O[j, :] = batch.o[0, :, i]\n",
    "            del opt\n",
    "            del loss\n",
    "            del logits\n",
    "            del batch\n",
    "        print()\n",
    "        print(f\"    average loss: {running_loss / batch_count}\")\n",
    "    return P + O\n",
    "\n",
    "M = word2vec(len(idx2word), training_pairs, 300, 512, 100, 0.15)     \n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement a method to find the top-k similar words.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def nearest_neighbours(query: str, word2idx: dict[str, int], V: list[str], M: torch.Tensor, k: int):\n",
    "    query_idx = word2idx[query]\n",
    "    scores = M[query_idx, :] @ M.T\n",
    "    top_idcs = [-1 for _ in range(k)]\n",
    "    for i in range(scores.shape[0]):\n",
    "        if not i == query_idx:\n",
    "            for j in range(k):\n",
    "                if top_idcs[j] == -1 or scores[top_idcs[j]] < scores[i]:\n",
    "                    top_idcs.insert(j, i)\n",
    "                    del top_idcs[-1]\n",
    "                    break\n",
    "    count = k\n",
    "    while top_idcs[k-1] == -1:\n",
    "        count -= 1\n",
    "    result = []\n",
    "    for j in range(count):\n",
    "        i = top_idcs[j]\n",
    "        result.append((idx2word[i], scores[i].item()))\n",
    "    return result\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Find the most similar words for \"Konzeption\", \"Cloud\" and \"virtuelle\".\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print(nearest_neighbours(\"Konzeption\", word2idx, idx2word, M, 5))\n",
    "print(nearest_neighbours(\"Cloud\", word2idx, idx2word, M, 5))\n",
    "print(nearest_neighbours(\"virtuelle\", word2idx, idx2word, M, 5))\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Play with the Embeddings\n",
    "\n",
    "3.1 Use the computed embeddings: Can you identify the most similar theses for some examples?\n",
    "\n",
    "3.2 Visualize the embeddings for a subset of theses using [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). You can use [Scikit-Learn](https://scikit-learn.org/stable/) and [Matplotlib](https://matplotlib.org) or [Seaborn](https://seaborn.pydata.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.1 Compute the embeddings for the theses and transform with TSNE.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def thesis_embedding(thesis: Thesis, word2idx: dict[str, int], M: torch.Tensor):\n",
    "    total = torch.zeros(M.shape[1], dtype=M.dtype)\n",
    "    for i in map(lambda t: word2idx[t], tokenize(thesis.title)):\n",
    "        total += M[i, :]\n",
    "    for i in map(lambda t: word2idx[t], tokenize(thesis.abstract)):\n",
    "        total += M[i, :]\n",
    "    return total.norm()\n",
    "\n",
    "embeddings = []\n",
    "for d in dataframe.to_dict(\"records\"):\n",
    "    embeddings.append(thesis_embedding(Thesis(**d))) # type: ignore\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.2 Visualize the samples in the 2D space.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
